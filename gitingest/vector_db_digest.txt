Directory structure:
└── gpt-embeddings/
    ├── LICENSE
    ├── chat_utils.py
    ├── check_utf8.py
    ├── database_utils.py
    ├── env_vars_template.sh
    ├── get_pdf_text.py
    ├── main.py
    ├── mysecrets_template.py
    ├── requirements.txt
    ├── run_vecdb_interface_server.sh
    ├── .gitignore
    ├── chatgpt-retrieval-plugin/
    │   ├── README.md
    │   ├── Dockerfile
    │   ├── LICENSE
    │   ├── Makefile
    │   ├── poetry.lock
    │   ├── pyproject.toml
    │   ├── .dockerignore
    │   ├── .gitignore
    │   ├── datastore/
    │   │   ├── __init__.py
    │   │   ├── datastore.py
    │   │   ├── factory.py
    │   │   └── providers/
    │   │       ├── __init__.py
    │   │       ├── analyticdb_datastore.py
    │   │       ├── azuresearch_datastore.py
    │   │       ├── chroma_datastore.py
    │   │       ├── elasticsearch_datastore.py
    │   │       ├── llama_datastore.py
    │   │       ├── milvus_datastore.py
    │   │       ├── pgvector_datastore.py
    │   │       ├── pinecone_datastore.py
    │   │       ├── postgres_datastore.py
    │   │       ├── qdrant_datastore.py
    │   │       ├── redis_datastore.py
    │   │       ├── supabase_datastore.py
    │   │       ├── weaviate_datastore.py
    │   │       └── zilliz_datastore.py
    │   ├── docs/
    │   │   ├── deployment/
    │   │   │   ├── flyio.md
    │   │   │   ├── heroku.md
    │   │   │   ├── other-options.md
    │   │   │   ├── removing-unused-dependencies.md
    │   │   │   └── render.md
    │   │   └── providers/
    │   │       ├── analyticdb/
    │   │       │   └── setup.md
    │   │       ├── azuresearch/
    │   │       │   └── setup.md
    │   │       ├── chroma/
    │   │       │   └── setup.md
    │   │       ├── elasticsearch/
    │   │       │   └── setup.md
    │   │       ├── llama/
    │   │       │   └── setup.md
    │   │       ├── milvus/
    │   │       │   └── setup.md
    │   │       ├── pinecone/
    │   │       │   └── setup.md
    │   │       ├── postgres/
    │   │       │   └── setup.md
    │   │       ├── qdrant/
    │   │       │   └── setup.md
    │   │       ├── redis/
    │   │       │   └── setup.md
    │   │       ├── supabase/
    │   │       │   └── setup.md
    │   │       ├── weaviate/
    │   │       │   └── setup.md
    │   │       └── zilliz/
    │   │           └── setup.md
    │   ├── examples/
    │   │   ├── authentication-methods/
    │   │   │   ├── no-auth/
    │   │   │   │   ├── ai-plugin.json
    │   │   │   │   └── main.py
    │   │   │   ├── oauth/
    │   │   │   │   └── ai-plugin.json
    │   │   │   ├── service-http/
    │   │   │   │   └── ai-plugin.json
    │   │   │   └── user-http/
    │   │   │       └── ai-plugin.json
    │   │   ├── docker/
    │   │   │   ├── elasticsearch/
    │   │   │   │   ├── README.md
    │   │   │   │   └── docker-compose.yaml
    │   │   │   ├── milvus/
    │   │   │   │   └── docker-compose.yaml
    │   │   │   ├── qdrant/
    │   │   │   │   ├── README.md
    │   │   │   │   ├── docker-compose.yaml
    │   │   │   │   ├── documents.json
    │   │   │   │   └── queries.json
    │   │   │   └── redis/
    │   │   │       └── docker-compose.yml
    │   │   ├── memory/
    │   │   │   ├── README.md
    │   │   │   ├── ai-plugin.json
    │   │   │   ├── main.py
    │   │   │   └── openapi.yaml
    │   │   └── providers/
    │   │       ├── elasticsearch/
    │   │       │   └── search.ipynb
    │   │       ├── pinecone/
    │   │       │   └── semantic-search.ipynb
    │   │       ├── redis/
    │   │       │   └── semantic-search-and-filter.ipynb
    │   │       └── supabase/
    │   │           ├── config.toml
    │   │           ├── seed.sql
    │   │           ├── .gitignore
    │   │           └── migrations/
    │   │               └── 20230414142107_init_pg_vector.sql
    │   ├── local_server/
    │   │   ├── ai-plugin.json
    │   │   ├── main.py
    │   │   └── openapi.yaml
    │   ├── models/
    │   │   ├── api.py
    │   │   └── models.py
    │   ├── scripts/
    │   │   ├── process_json/
    │   │   │   ├── README.md
    │   │   │   ├── example.json
    │   │   │   └── process_json.py
    │   │   ├── process_jsonl/
    │   │   │   ├── README.md
    │   │   │   ├── example.jsonl
    │   │   │   └── process_jsonl.py
    │   │   └── process_zip/
    │   │       ├── README.md
    │   │       ├── example.zip
    │   │       └── process_zip.py
    │   ├── server/
    │   │   └── main.py
    │   ├── services/
    │   │   ├── chunks.py
    │   │   ├── date.py
    │   │   ├── extract_metadata.py
    │   │   ├── file.py
    │   │   ├── openai.py
    │   │   └── pii_detection.py
    │   ├── tests/
    │   │   ├── __init__.py
    │   │   └── datastore/
    │   │       └── providers/
    │   │           ├── analyticdb/
    │   │           │   └── test_analyticdb_datastore.py
    │   │           ├── azuresearch/
    │   │           │   └── test_azuresearch_datastore.py
    │   │           ├── chroma/
    │   │           │   └── test_chroma_datastore.py
    │   │           ├── elasticsearch/
    │   │           │   └── test_elasticsearch_datastore.py
    │   │           ├── llama/
    │   │           │   └── test_llama_datastore.py
    │   │           ├── milvus/
    │   │           │   └── test_milvus_datastore.py
    │   │           ├── postgres/
    │   │           │   └── test_postgres_datastore.py
    │   │           ├── qdrant/
    │   │           │   └── test_qdrant_datastore.py
    │   │           ├── redis/
    │   │           │   └── test_redis_datastore.py
    │   │           ├── supabase/
    │   │           │   └── test_supabase_datastore.py
    │   │           ├── weaviate/
    │   │           │   ├── docker-compose.yml
    │   │           │   └── test_weaviate_datastore.py
    │   │           └── zilliz/
    │   │               └── test_zilliz_datastore.py
    │   ├── .github/
    │   │   └── pull_request_template.md
    │   └── .well-known/
    │       ├── ai-plugin.json
    │       └── openapi.yaml
    ├── databankDir/
    └── destFilesDir/
        ├── extracted_pages_0.txt
        ├── extracted_pages_1.txt
        ├── extracted_pages_10.txt
        ├── extracted_pages_100.txt
        ├── extracted_pages_101.txt
        ├── extracted_pages_102.txt
        ├── extracted_pages_103.txt
        ├── extracted_pages_104.txt
        ├── extracted_pages_11.txt
        ├── extracted_pages_12.txt
        ├── extracted_pages_13.txt
        ├── extracted_pages_14.txt
        ├── extracted_pages_15.txt
        ├── extracted_pages_16.txt
        ├── extracted_pages_17.txt
        ├── extracted_pages_18.txt
        ├── extracted_pages_19.txt
        ├── extracted_pages_2.txt
        ├── extracted_pages_20.txt
        ├── extracted_pages_21.txt
        ├── extracted_pages_22.txt
        ├── extracted_pages_23.txt
        ├── extracted_pages_24.txt
        ├── extracted_pages_25.txt
        ├── extracted_pages_26.txt
        ├── extracted_pages_27.txt
        ├── extracted_pages_28.txt
        ├── extracted_pages_29.txt
        ├── extracted_pages_3.txt
        ├── extracted_pages_30.txt
        ├── extracted_pages_31.txt
        ├── extracted_pages_32.txt
        ├── extracted_pages_33.txt
        ├── extracted_pages_34.txt
        ├── extracted_pages_35.txt
        ├── extracted_pages_36.txt
        ├── extracted_pages_37.txt
        ├── extracted_pages_38.txt
        ├── extracted_pages_39.txt
        ├── extracted_pages_4.txt
        ├── extracted_pages_40.txt
        ├── extracted_pages_41.txt
        ├── extracted_pages_42.txt
        ├── extracted_pages_43.txt
        ├── extracted_pages_44.txt
        ├── extracted_pages_45.txt
        ├── extracted_pages_46.txt
        ├── extracted_pages_47.txt
        ├── extracted_pages_48.txt
        ├── extracted_pages_49.txt
        ├── extracted_pages_5.txt
        ├── extracted_pages_50.txt
        ├── extracted_pages_51.txt
        ├── extracted_pages_52.txt
        ├── extracted_pages_53.txt
        ├── extracted_pages_54.txt
        ├── extracted_pages_55.txt
        ├── extracted_pages_56.txt
        ├── extracted_pages_57.txt
        ├── extracted_pages_58.txt
        ├── extracted_pages_59.txt
        ├── extracted_pages_6.txt
        ├── extracted_pages_60.txt
        ├── extracted_pages_61.txt
        ├── extracted_pages_62.txt
        ├── extracted_pages_63.txt
        ├── extracted_pages_64.txt
        ├── extracted_pages_65.txt
        ├── extracted_pages_66.txt
        ├── extracted_pages_67.txt
        ├── extracted_pages_68.txt
        ├── extracted_pages_69.txt
        ├── extracted_pages_7.txt
        ├── extracted_pages_70.txt
        ├── extracted_pages_71.txt
        ├── extracted_pages_72.txt
        ├── extracted_pages_73.txt
        ├── extracted_pages_74.txt
        ├── extracted_pages_75.txt
        ├── extracted_pages_76.txt
        ├── extracted_pages_77.txt
        ├── extracted_pages_78.txt
        ├── extracted_pages_79.txt
        ├── extracted_pages_8.txt
        ├── extracted_pages_80.txt
        ├── extracted_pages_81.txt
        ├── extracted_pages_82.txt
        ├── extracted_pages_83.txt
        ├── extracted_pages_84.txt
        ├── extracted_pages_85.txt
        ├── extracted_pages_86.txt
        ├── extracted_pages_87.txt
        ├── extracted_pages_88.txt
        ├── extracted_pages_89.txt
        ├── extracted_pages_9.txt
        ├── extracted_pages_90.txt
        ├── extracted_pages_91.txt
        ├── extracted_pages_92.txt
        ├── extracted_pages_93.txt
        ├── extracted_pages_94.txt
        ├── extracted_pages_95.txt
        ├── extracted_pages_96.txt
        ├── extracted_pages_97.txt
        ├── extracted_pages_98.txt
        └── extracted_pages_99.txt

================================================
File: vectordb/gpt-embeddings/LICENSE
================================================
MIT License

Copyright (c) 2023 Sergio Solorzano

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
File: vectordb/gpt-embeddings/chat_utils.py
================================================
#!/usr/bin/env python3
from typing import Any, List, Dict
import openai
import requests
from mysecrets import DATABASE_INTERFACE_BEARER_TOKEN
from mysecrets import OPENAI_API_KEY
import logging

def query_database(query_prompt: str) -> Dict[str, Any]:
    num_chunks = input("Enter Number Embeddings To Retrieve: ")
    """
    Query vector database to retrieve chunk with user's input questions.
    """
    url = "http://0.0.0.0:8000/query"
    headers = {
        "Content-Type": "application/json",
        "accept": "application/json",
        "Authorization": f"Bearer {DATABASE_INTERFACE_BEARER_TOKEN}",
    }
    data = {"queries": [{"query": query_prompt, "top_k": num_chunks}]}
    
    response = requests.post(url, json=data, headers=headers)

    if response.status_code == 200:
        result = response.json()
        # process the result
        print(); print("-" * 40); print()
        print(f"\033[43mTop {num_chunks} chunks from query_prompt ",query_prompt," has response with chunks:\033[0m")
        for query_result in result['results']:
            for embedding in query_result['results']:
                page_index = embedding['text'].rfind("Page")
                print(); print(f"Embedding Id:",embedding['id'],"Score:",embedding['score'],"Doc Page:",embedding['text'][page_index:],"Doc text:",embedding['text'][:50],"...")
        return result
    else:
        raise ValueError(f"Error getting chunks for query_prompt {query_prompt}: {response.status_code} : {response.content}")


def apply_prompt_template(question: str) -> str:
    """
        A helper function that applies additional template on user's question.
        Prompt engineering could be done here to improve the result. Here I will just use a minimal example.
    """
    prompt = f"""
        By considering above input from me, answer the question: {question}
    """
    return prompt

def call_chatgpt_api(user_question: str, chunks: List[str]) -> Dict[str, Any]:
    """
    Call chatgpt api with user's question and retrieved chunks.
    """
    # Send a request to the GPT-3 API
    messages = list(
        map(lambda chunk: {
            "role": "user",
            "content": chunk
        }, chunks))
    #print("List of chunks:",messages)
    question = apply_prompt_template(user_question)
    messages.append({"role": "user", "content": question})
    print(); print("-" * 40); print()
    print(f"\033[43mSending Request to GPT\033[0m")
    response = openai.ChatCompletion.create(
        #engine="selfgengpt35t0301", #azure openai
        model = "gpt-3.5-turbo-0613",
        messages=messages,
        max_tokens=1024,
        temperature=0.7,  # High temperature leads to a more creative response.
    )
    print(); print("-" * 40); print()
    print(f"\033[43mRequest Sent with Embeddings\033[0m")
    return response


def ask(user_question: str) -> Dict[str, Any]:
    """
    Handle user's questions.
    """
    # Get chunks from database.
    chunks_response = query_database(user_question)
    chunks = []
    for result in chunks_response["results"]:
        for inner_result in result["results"]:
            chunks.append(inner_result["text"])
    
    logging.info("User's questions: %s", user_question)
    logging.info("Retrieved chunks: %s", chunks)
    
    response = call_chatgpt_api(user_question, chunks)
    logging.info("Response: %s", response)
    
    return response["choices"][0]["message"]["content"]


================================================
File: vectordb/gpt-embeddings/check_utf8.py
================================================
#!/usr/bin/env python3
import chardet

def is_utf8(text):
  """Returns True if the text is encoded in UTF-8, False otherwise."""

  detector = chardet.UniversalDetector()
  detector.feed(text)
  result = detector.result

  if result['encoding'] == 'utf-8':
    return True
  else:
    return False

def main():
  with open('myfile.txt', 'rb') as f:
    text = f.read()

  if is_utf8(text):
    print('All the text in the file is encoded in UTF-8.')
  else:
    print('Not all the text in the file is encoded in UTF-8.')

if __name__ == '__main__':
  main()



================================================
File: vectordb/gpt-embeddings/database_utils.py
================================================
#!/usr/bin/env python3

import argparse
import sys
from typing import Any, Dict
import requests
import os
from mysecrets import *

import pinecone 

SEARCH_TOP_K = 3
destination_files_dir="destFilesDir"

#cli help
parser = argparse.ArgumentParser(
                    prog='VectorDB Utils',
                    description=f'Vector DB API to post file from {destination_files_dir}, post prompt, get embeddings')

args = parser.parse_args()
print()

for c,arg in enumerate(sys.argv):
	if c==1:
		source_fname = arg
		print("Text SOURCE file: " + source_fname)

def upsert_file(directory: str):
    """
    Upload all files under a directory to the vector database.
    """
    url = "http://0.0.0.0:8000/upsert-file"
    headers = {"Authorization": "Bearer " + DATABASE_INTERFACE_BEARER_TOKEN}
    files = []
    for filename in os.listdir(directory):
        if os.path.isfile(os.path.join(directory, filename)):
            file_path = os.path.join(directory, filename)
            with open(file_path, "rb") as f:
                file_content = f.read()
                files.append(("file", (filename, file_content, "text/plain")))
            response = requests.post(url,
                                     headers=headers,
                                     files=files,
                                     timeout=600)
            if response.status_code == 200:
                print(filename + " uploaded successfully to vector db.")
            else:
                print(
                    f"Error: {response.status_code} {response.content} for uploading "
                    + filename)

def upsert(id: str, content: str):
    """
    Upload one piece of text to the database.
    """
    url = "http://0.0.0.0:8000/upsert"
    headers = {
        "accept": "application/json",
        "Content-Type": "application/json",
        "Authorization": "Bearer " + DATABASE_INTERFACE_BEARER_TOKEN,
    }

    data = {
        "documents": [{
            "id": id,
            "text": content,
        }]
    }
    response = requests.post(url, json=data, headers=headers, timeout=600)

    if response.status_code == 200:
        print("uploaded successfully.")
    else:
        print(f"Error: {response.status_code} {response.content}")

def query_database(query_prompt: str) -> Dict[str, Any]:
    """
    Query vector database to retrieve chunk with user's input question.
    """
    url = "http://0.0.0.0:8000/query"
    headers = {
        "Content-Type": "application/json",
        "accept": "application/json",
        "Authorization": f"Bearer {DATABASE_INTERFACE_BEARER_TOKEN}",
    }
    data = {"queries": [{"query": query_prompt, "top_k": SEARCH_TOP_K}]}

    response = requests.post(url, json=data, headers=headers, timeout=600)

    if response.status_code == 200:
        result = response.json()
        # process the result
        return result
    else:
        raise ValueError(f"Error: {response.status_code} : {response.content}")

def delete_all_vecs():  
    pinecone.init(api_key=PINECONE_API_KEY, environment='gcp-starter') 
    index = pinecone.Index('gpt-with-embeddings') 
    #query = index.query(queries=[], include_ids=True, top_k=1000)  # Set a sufficient top_k value
    res = index.query(vector=[0 for _ in range(1536)], top_k=10000)
    #print("HERE:",res)
    # Retrieve all keys from the query results
    for match in res['matches']:
         print(match.id)
         delete_response = index.delete(ids=[match.id])
         print("Delete response:",delete_response)

if __name__ == "__main__":
    print("\033[43mUpload Page Chunks.\033[0m")
    upsert_file(destination_files_dir)
    #delete_all_vecs()


================================================
File: vectordb/gpt-embeddings/env_vars_template.sh
================================================
#!/bin/bash
#Azure Openai settings:https://github.com/openai/chatgpt-retrieval-plugin/blob/main/services/openai.py
export DATASTORE=pinecone
export BEARER_TOKEN="enter key"
export OPENAI_API_KEY="enter key"
export PINECONE_API_KEY="enter key"
export PINECONE_ENVIRONMENT="enter name"
export PINECONE_INDEX="enter name"

#Azure openai
#See references: https://github.com/openai/chatgpt-retrieval-plugin/blob/main/services/openai.py
#export OPENAI_EMBEDDINGMODEL_DEPLOYMENTID="enter name"
#export OPENAI_API_KEY="enter key"
#export AZURE_OPENAI_API_KEY="enter key"
#export OPENAI_API_BASE=https://<yourdomain>.openai.azure.com/
#export OPENAI_API_TYPE=azure
#export OPENAI_METADATA_EXTRACTIONMODEL_DEPLOYMENTID=<Name of deployment of model for metatdata>
#export OPENAI_COMPLETIONMODEL_DEPLOYMENTID=<Name of general model deployment used for completion>
#export OPENAI_EMBEDDING_BATCH_SIZE=1

#to generate BEARER_TOKEN:
#BEARER_TOKEN is a security token set for your Database Interface. It is the API key calling your own server. Create any key using https://jwt.io/. On Decoded tab “PayLoad” section:

#{
# “sub”: “1234567890”,
# “name”: “Write any name”,
# “iat”: 1516239022
#}
#Change the value to whatever you like. Then go back to Encoded tab and copy that generated token. Paste the key to the above export command. Your server will get this variable when it starts and set this as your security token. Save this token somewhere in a file because you need it later for sending the request.




================================================
File: vectordb/gpt-embeddings/get_pdf_text.py
================================================
#!/usr/bin/env python3

from pathlib import Path
import os
import argparse
import sys
from PyPDF2 import PdfReader 
import requests
import subprocess

pdf_source_file_url="https://www.bankofengland.co.uk/-/media/boe/files/monetary-policy-report/2023/november/monetary-policy-report-november-2023.pdf"
pdf_sections = [(5,33),(35,78),(79,102)]
full_text_pdf_list = []
destination_files_dir="destFilesDir"
source_files_dir="databankDir"
dest_fname="extracted_pages"

#cli help
parser = argparse.ArgumentParser(
                    prog='LLM enhanced with embeddings',
                    description='Enhance gpt knowledge with embeddings')

parser.add_argument('-s', '--source', metavar='databankDir', type=str, help='Directory path to download the source file')
parser.add_argument('-d', '--destination', metavar='destFilesDir', type=str, help='Directory path to save chunked pages of the source file')

args = parser.parse_args()
print()

#export variables
# env_file_path = Path(os.environ['PWD'],"env_vars.sh")
# # Use subprocess to execute the shell script and capture its output
# result = subprocess.run(f"bash -c 'source {env_file_path} && env'", shell=True, stdout=subprocess.PIPE)

# # Decode the output and split environment variables
# output = result.stdout.decode('utf-8')
# env_vars = [line.split('=', 1) for line in output.splitlines()]

# # Update Python's environment with the retrieved variables
# for key, value in env_vars:
#     if key:
#         key = key.strip()
#         value = value.strip().strip('"') if value else ''
#         if value:
#             # Set the environment variables within Python
#             subprocess.run(f'export {key}={value}', shell=True)

#create dirs
if not os.path.exists(destination_files_dir):
	os.makedirs(destination_files_dir)
if not os.path.exists(source_files_dir):
	os.makedirs(source_files_dir)

#get pdf
print(); print("-" * 40); print()
print("\033[43mGet Source Document:\033[0m")
resp = requests.get(pdf_source_file_url)
if resp.status_code==200:
	print(f"\033[92;1mGet Success\033[0m")
	source_fname=os.path.basename(pdf_source_file_url)
else:
	print(f"\033[91m[ERROR] Get Failed.\033[0m")

#save pdf to source dir
#dest_path = os.path.join(os.environ['PWD'],destination_files_dir,dest_fname)
dest_path=Path(os.environ['PWD'],source_files_dir,source_fname)
with open(dest_path,"wb") as f:
	f.write(resp.content)
	print(f"Source doc saved to {dest_path}")

# creating a pdf reader object 
reader = PdfReader(dest_path)
  
# printing number of pages in pdf file
print(); print("-" * 40); print()
print("\033[43mChunk Source Document Pages:\033[0m")
pages = reader.pages
print("Pages in source file:",len(pages)) 
  
# getting a specific page from the pdf file
for count, page in enumerate(pages):
	page_text = reader.pages[count]
	# extracting text from page 
	text = str(page.extract_text())
	
	# Try to decode the text as UTF-8.
	if not isinstance(text,bytes):
		text = text.encode("utf-8")
		#print("ENCODING TO UTF")
	try:
		text = text.decode('utf-8')
	except UnicodeDecodeError:
		# If the text cannot be decoded as UTF-8, try to decode it as Latin-1.
		# Latin-1 is a superset of ASCII, so this will ensure that all characters
		# are decoded correctly.
		text = text.decode('latin-1')
		#print("DECODE FAIL")

	#full_text_pdf_list.append("page "+ str(count) + ":" +text)
	#full_text_pdf_list.append(text)

#print(full_text_pdf_list)

	filename = "".join([dest_fname, "_", str(count), ".txt"])
	with open(Path(os.environ['PWD'], destination_files_dir, filename), "w", encoding='utf-8') as filedata:
		#for item in full_text_pdf_list:
		filedata.write("%s\n" % text)
		print(f"Saved page {count} to file",filename)

	filedata.close()
#filedata = open(Path(os.environ['PWD'], destination_files_dir, dest_fname))
#print(filedata.read())


================================================
File: vectordb/gpt-embeddings/main.py
================================================
#!/usr/bin/env python3
import logging
import openai
from chat_utils import ask
from mysecrets import OPENAI_API_KEY

if __name__ == "__main__":
    while True:
        user_query = input("Enter your question: ")
        openai.api_key = OPENAI_API_KEY
        logging.basicConfig(level=logging.WARNING,
                            format="%(asctime)s %(levelname)s %(message)s")
        print(ask(user_query))


================================================
File: vectordb/gpt-embeddings/mysecrets_template.py
================================================
#OpenAI
DATABASE_INTERFACE_BEARER_TOKEN="enter key"
OPENAI_API_KEY="enter key"

#Azure OpenAI
#AZURE_OPENAI_API_KEY="enter key"
#OPENAI_API_KEY="enter key"
#See references: https://github.com/openai/chatgpt-retrieval-plugin/blob/main/services/openai.py



================================================
File: vectordb/gpt-embeddings/requirements.txt
================================================
openai
tiktoken
poetry (sudo apt install python3-dev libpq-dev  and #https://www.digitalocean.com/community/tutorials/how-to-install-poetry-to-manage-python-dependencies-on-ubuntu-22-04)


================================================
File: vectordb/gpt-embeddings/run_vecdb_interface_server.sh
================================================
#!/bin/bash

echo ""
echo "Run Vector DB Interface Server for the Retrieval Plugin on Python3.10"
echo "When application is running, browser to http://localhost:8000/docs"
echo ""

# Prompt the user to confirm the environment variables have been added
echo "Confirm environment variables have been added. If yes, press enter to continue. Else cancel and add."
read confirmation

# Prompt the user to confirm the env has been activated
echo "Confirm Conda environment is active. Enter to continue else cancel"
read confirmation2

# Display messages for each command
echo "Changing directory..."
cd ~/my-repos/vectordb/gpt-embeddings/chatgpt-retrieval-plugin

echo "Installing poetry..."
pip install poetry

echo "Setting up virtual environment, run poetry shell and run the app on FastAPI..."
# Run the application using poetry run
poetry env use python3.10
echo ""
echo -e "\e[31mPlease enter 'exit' on CLI and press enter to switch to FastAPI Interface server.\e[0m"
echo ""
poetry shell
poetry install
poetry run start




================================================
File: vectordb/gpt-embeddings/.gitignore
================================================
env_vars.sh
mysecrets.py

# Onnx dynamo export artifacts
*.sarif

# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST
bin/

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# PEP 582; used by e.g. github.com/David-OConnor/pyflow
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
/.idea

# AzureML
.azureml



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/README.md
================================================
# ChatGPT Retrieval Plugin

Find an example video of a Retrieval Plugin that has access to the UN Annual Reports from 2018 to 2022 [here](https://cdn.openai.com/chat-plugins/retrieval-gh-repo-readme/Retrieval-Final.mp4).

## Introduction

The ChatGPT Retrieval Plugin repository provides a flexible solution for semantic search and retrieval of personal or organizational documents using natural language queries. The repository is organized into several directories:

| Directory                       | Description                                                                                                                |
| ------------------------------- | -------------------------------------------------------------------------------------------------------------------------- |
| [`datastore`](/datastore)       | Contains the core logic for storing and querying document embeddings using various vector database providers.              |
| [`docs`](/docs)                 | Includes documentation for setting up and using each vector database provider, webhooks, and removing unused dependencies. |
| [`examples`](/examples)         | Provides example configurations, authentication methods, and provider-specific examples.                                   |
| [`local_server`](/local_server) | Contains an implementation of the retrieval plugin configured for localhost testing.                                       |
| [`models`](/models)             | Contains the data models used by the plugin, such as document and metadata models.                                         |
| [`scripts`](/scripts)           | Offers scripts for processing and uploading documents from different data sources.                                         |
| [`server`](/server)             | Houses the main FastAPI server implementation.                                                                             |
| [`services`](/services)         | Contains utility services for tasks like chunking, metadata extraction, and PII detection.                                 |
| [`tests`](/tests)               | Includes integration tests for various vector database providers.                                                          |
| [`.well-known`](/.well-known)   | Stores the plugin manifest file and OpenAPI schema, which define the plugin configuration and API specification.           |

This README provides detailed information on how to set up, develop, and deploy the ChatGPT Retrieval Plugin.

## Table of Contents

- [Quickstart](#quickstart)
- [About](#about)
  - [Plugins](#plugins)
  - [Retrieval Plugin](#retrieval-plugin)
  - [Memory Feature](#memory-feature)
  - [Security](#security)
  - [API Endpoints](#api-endpoints)
- [Development](#development)
  - [Setup](#setup)
    - [General Environment Variables](#general-environment-variables)
  - [Choosing a Vector Database](#choosing-a-vector-database)
    - [Pinecone](#pinecone)
    - [Elasticsearch](#elasticsearch)
    - [Weaviate](#weaviate)
    - [Zilliz](#zilliz)
    - [Milvus](#milvus)
    - [Qdrant](#qdrant)
    - [Redis](#redis)
    - [Llama Index](#llamaindex)
    - [Chroma](#chroma)
    - [Azure Cognitive Search](#azure-cognitive-search)
    - [Supabase](#supabase)
    - [Postgres](#postgres)
    - [AnalyticDB](#analyticdb)
  - [Running the API Locally](#running-the-api-locally)
  - [Testing a Localhost Plugin in ChatGPT](#testing-a-localhost-plugin-in-chatgpt)
  - [Personalization](#personalization)
  - [Authentication Methods](#authentication-methods)
- [Deployment](#deployment)
- [Installing a Developer Plugin](#installing-a-developer-plugin)
- [Webhooks](#webhooks)
- [Scripts](#scripts)
- [Limitations](#limitations)
- [Contributors](#contributors)
- [Future Directions](#future-directions)

## Quickstart

Follow these steps to quickly set up and run the ChatGPT Retrieval Plugin:

1. Install Python 3.10, if not already installed.
2. Clone the repository: `git clone https://github.com/openai/chatgpt-retrieval-plugin.git`
3. Navigate to the cloned repository directory: `cd /path/to/chatgpt-retrieval-plugin`
4. Install poetry: `pip install poetry`
5. Create a new virtual environment with Python 3.10: `poetry env use python3.10`
6. Activate the virtual environment: `poetry shell`
7. Install app dependencies: `poetry install`
8. Create a [bearer token](#general-environment-variables)
9. Set the required environment variables:

   ```
   export DATASTORE=<your_datastore>
   export BEARER_TOKEN=<your_bearer_token>
   export OPENAI_API_KEY=<your_openai_api_key>

   # Optional environment variables used when running Azure OpenAI
   export OPENAI_API_BASE=https://<AzureOpenAIName>.openai.azure.com/
   export OPENAI_API_TYPE=azure
   export OPENAI_EMBEDDINGMODEL_DEPLOYMENTID=<Name of text-embedding-ada-002 model deployment>
   export OPENAI_METADATA_EXTRACTIONMODEL_DEPLOYMENTID=<Name of deployment of model for metatdata>
   export OPENAI_COMPLETIONMODEL_DEPLOYMENTID=<Name of general model deployment used for completion>
   export OPENAI_EMBEDDING_BATCH_SIZE=<Batch size of embedding, for AzureOAI, this value need to be set as 1>

   # Add the environment variables for your chosen vector DB.
   # Some of these are optional; read the provider's setup docs in /docs/providers for more information.

   # Pinecone
   export PINECONE_API_KEY=<your_pinecone_api_key>
   export PINECONE_ENVIRONMENT=<your_pinecone_environment>
   export PINECONE_INDEX=<your_pinecone_index>

   # Weaviate
   export WEAVIATE_URL=<your_weaviate_instance_url>
   export WEAVIATE_API_KEY=<your_api_key_for_WCS>
   export WEAVIATE_CLASS=<your_optional_weaviate_class>

   # Zilliz
   export ZILLIZ_COLLECTION=<your_zilliz_collection>
   export ZILLIZ_URI=<your_zilliz_uri>
   export ZILLIZ_USER=<your_zilliz_username>
   export ZILLIZ_PASSWORD=<your_zilliz_password>

   # Milvus
   export MILVUS_COLLECTION=<your_milvus_collection>
   export MILVUS_HOST=<your_milvus_host>
   export MILVUS_PORT=<your_milvus_port>
   export MILVUS_USER=<your_milvus_username>
   export MILVUS_PASSWORD=<your_milvus_password>

   # Qdrant
   export QDRANT_URL=<your_qdrant_url>
   export QDRANT_PORT=<your_qdrant_port>
   export QDRANT_GRPC_PORT=<your_qdrant_grpc_port>
   export QDRANT_API_KEY=<your_qdrant_api_key>
   export QDRANT_COLLECTION=<your_qdrant_collection>

   # AnalyticDB
   export PG_HOST=<your_analyticdb_host>
   export PG_PORT=<your_analyticdb_port>
   export PG_USER=<your_analyticdb_username>
   export PG_PASSWORD=<your_analyticdb_password>
   export PG_DATABASE=<your_analyticdb_database>
   export PG_COLLECTION=<your_analyticdb_collection>


   # Redis
   export REDIS_HOST=<your_redis_host>
   export REDIS_PORT=<your_redis_port>
   export REDIS_PASSWORD=<your_redis_password>
   export REDIS_INDEX_NAME=<your_redis_index_name>
   export REDIS_DOC_PREFIX=<your_redis_doc_prefix>
   export REDIS_DISTANCE_METRIC=<your_redis_distance_metric>
   export REDIS_INDEX_TYPE=<your_redis_index_type>

   # Llama
   export LLAMA_INDEX_TYPE=<gpt_vector_index_type>
   export LLAMA_INDEX_JSON_PATH=<path_to_saved_index_json_file>
   export LLAMA_QUERY_KWARGS_JSON_PATH=<path_to_saved_query_kwargs_json_file>
   export LLAMA_RESPONSE_MODE=<response_mode_for_query>

   # Chroma
   export CHROMA_COLLECTION=<your_chroma_collection>
   export CHROMA_IN_MEMORY=<true_or_false>
   export CHROMA_PERSISTENCE_DIR=<your_chroma_persistence_directory>
   export CHROMA_HOST=<your_chroma_host>
   export CHROMA_PORT=<your_chroma_port>

   # Azure Cognitive Search
   export AZURESEARCH_SERVICE=<your_search_service_name>
   export AZURESEARCH_INDEX=<your_search_index_name>
   export AZURESEARCH_API_KEY=<your_api_key> (optional, uses key-free managed identity if not set)

   # Supabase
   export SUPABASE_URL=<supabase_project_url>
   export SUPABASE_ANON_KEY=<supabase_project_api_anon_key>

   # Postgres
   export PG_HOST=<postgres_host>
   export PG_PORT=<postgres_port>
   export PG_USER=<postgres_user>
   export PG_PASSWORD=<postgres_password>
   export PG_DB=<postgres_database>

   # Elasticsearch
   export ELASTICSEARCH_URL=<elasticsearch_host_and_port> (either specify host or cloud_id)
   export ELASTICSEARCH_CLOUD_ID=<elasticsearch_cloud_id>

   export ELASTICSEARCH_USERNAME=<elasticsearch_username>
   export ELASTICSEARCH_PASSWORD=<elasticsearch_password>
   export ELASTICSEARCH_API_KEY=<elasticsearch_api_key>

   export ELASTICSEARCH_INDEX=<elasticsearch_index_name>
   export ELASTICSEARCH_REPLICAS=<elasticsearch_replicas>
   export ELASTICSEARCH_SHARDS=<elasticsearch_shards>
   ```

10. Run the API locally: `poetry run start`
11. Access the API documentation at `http://0.0.0.0:8000/docs` and test the API endpoints (make sure to add your bearer token).

### Testing in ChatGPT

To test a locally hosted plugin in ChatGPT, follow these steps:

1. Run the API on localhost: `poetry run dev`
2. Follow the instructions in the [Testing a Localhost Plugin in ChatGPT](#testing-a-localhost-plugin-in-chatgpt) section of the README.

For more detailed information on setting up, developing, and deploying the ChatGPT Retrieval Plugin, refer to the full Development section below.

## About

### Plugins

Plugins are chat extensions designed specifically for language models like ChatGPT, enabling them to access up-to-date information, run computations, or interact with third-party services in response to a user's request. They unlock a wide range of potential use cases and enhance the capabilities of language models.

Developers can create a plugin by exposing an API through their website and providing a standardized manifest file that describes the API. ChatGPT consumes these files and allows the AI models to make calls to the API defined by the developer.

A plugin consists of:

- An API
- An API schema (OpenAPI JSON or YAML format)
- A manifest (JSON file) that defines relevant metadata for the plugin

The Retrieval Plugin already contains all of these components. Read the Chat Plugins blogpost [here](https://openai.com/blog/chatgpt-plugins), and find the docs [here](https://platform.openai.com/docs/plugins/introduction).

### Retrieval Plugin

This is a plugin for ChatGPT that enables semantic search and retrieval of personal or organizational documents. It allows users to obtain the most relevant document snippets from their data sources, such as files, notes, or emails, by asking questions or expressing needs in natural language. Enterprises can make their internal documents available to their employees through ChatGPT using this plugin.

The plugin uses OpenAI's `text-embedding-ada-002` embeddings model to generate embeddings of document chunks, and then stores and queries them using a vector database on the backend. As an open-source and self-hosted solution, developers can deploy their own Retrieval Plugin and register it with ChatGPT. The Retrieval Plugin supports several vector database providers, allowing developers to choose their preferred one from a list.

A FastAPI server exposes the plugin's endpoints for upserting, querying, and deleting documents. Users can refine their search results by using metadata filters by source, date, author, or other criteria. The plugin can be hosted on any cloud platform that supports Docker containers, such as Fly.io, Heroku, Render, or Azure Container Apps. To keep the vector database updated with the latest documents, the plugin can process and store documents from various data sources continuously, using incoming webhooks to the upsert and delete endpoints. Tools like [Zapier](https://zapier.com) or [Make](https://www.make.com) can help configure the webhooks based on events or schedules.

### Memory Feature

A notable feature of the Retrieval Plugin is its capacity to provide ChatGPT with memory. By utilizing the plugin's upsert endpoint, ChatGPT can save snippets from the conversation to the vector database for later reference (only when prompted to do so by the user). This functionality contributes to a more context-aware chat experience by allowing ChatGPT to remember and retrieve information from previous conversations. Learn how to configure the Retrieval Plugin with memory [here](/examples/memory).

### Security

The Retrieval Plugin allows ChatGPT to search a vector database of content, and then add the best results into the ChatGPT session. This means it doesnâ€™t have any external effects, and the main risk consideration is data authorization and privacy. Developers should only add content into their Retrieval Plugin that they have authorization for and that they are fine with appearing in usersâ€™ ChatGPT sessions. You can choose from a number of different authentication methods to secure the plugin (more information [here](#authentication-methods)).

### API Endpoints

The Retrieval Plugin is built using FastAPI, a web framework for building APIs with Python. FastAPI allows for easy development, validation, and documentation of API endpoints. Find the FastAPI documentation [here](https://fastapi.tiangolo.com/).

One of the benefits of using FastAPI is the automatic generation of interactive API documentation with Swagger UI. When the API is running locally, Swagger UI at `<local_host_url i.e. http://0.0.0.0:8000>/docs` can be used to interact with the API endpoints, test their functionality, and view the expected request and response models.

The plugin exposes the following endpoints for upserting, querying, and deleting documents from the vector database. All requests and responses are in JSON format, and require a valid bearer token as an authorization header.

- `/upsert`: This endpoint allows uploading one or more documents and storing their text and metadata in the vector database. The documents are split into chunks of around 200 tokens, each with a unique ID. The endpoint expects a list of documents in the request body, each with a `text` field, and optional `id` and `metadata` fields. The `metadata` field can contain the following optional subfields: `source`, `source_id`, `url`, `created_at`, and `author`. The endpoint returns a list of the IDs of the inserted documents (an ID is generated if not initially provided).

- `/upsert-file`: This endpoint allows uploading a single file (PDF, TXT, DOCX, PPTX, or MD) and storing its text and metadata in the vector database. The file is converted to plain text and split into chunks of around 200 tokens, each with a unique ID. The endpoint returns a list containing the generated id of the inserted file.

- `/query`: This endpoint allows querying the vector database using one or more natural language queries and optional metadata filters. The endpoint expects a list of queries in the request body, each with a `query` and optional `filter` and `top_k` fields. The `filter` field should contain a subset of the following subfields: `source`, `source_id`, `document_id`, `url`, `created_at`, and `author`. The `top_k` field specifies how many results to return for a given query, and the default value is 3. The endpoint returns a list of objects that each contain a list of the most relevant document chunks for the given query, along with their text, metadata and similarity scores.

- `/delete`: This endpoint allows deleting one or more documents from the vector database using their IDs, a metadata filter, or a delete_all flag. The endpoint expects at least one of the following parameters in the request body: `ids`, `filter`, or `delete_all`. The `ids` parameter should be a list of document IDs to delete; all document chunks for the document with these IDS will be deleted. The `filter` parameter should contain a subset of the following subfields: `source`, `source_id`, `document_id`, `url`, `created_at`, and `author`. The `delete_all` parameter should be a boolean indicating whether to delete all documents from the vector database. The endpoint returns a boolean indicating whether the deletion was successful.

The detailed specifications and examples of the request and response models can be found by running the app locally and navigating to http://0.0.0.0:8000/openapi.json, or in the OpenAPI schema [here](/.well-known/openapi.yaml). Note that the OpenAPI schema only contains the `/query` endpoint, because that is the only function that ChatGPT needs to access. This way, ChatGPT can use the plugin only to retrieve relevant documents based on natural language queries or needs. However, if developers want to also give ChatGPT the ability to remember things for later, they can use the `/upsert` endpoint to save snippets from the conversation to the vector database. An example of a manifest and OpenAPI schema that gives ChatGPT access to the `/upsert` endpoint can be found [here](/examples/memory).

To include custom metadata fields, edit the `DocumentMetadata` and `DocumentMetadataFilter` data models [here](/models/models.py), and update the OpenAPI schema [here](/.well-known/openapi.yaml). You can update this easily by running the app locally, copying the JSON found at http://0.0.0.0:8000/sub/openapi.json, and converting it to YAML format with [Swagger Editor](https://editor.swagger.io/). Alternatively, you can replace the `openapi.yaml` file with an `openapi.json` file.

## Development

### Setup

This app uses Python 3.10, and [poetry](https://python-poetry.org/) for dependency management.

Install Python 3.10 on your machine if it isn't already installed. It can be downloaded from the official [Python website](https://www.python.org/downloads/) or with a package manager like `brew` or `apt`, depending on your system.

Clone the repository from GitHub:

```
git clone https://github.com/openai/chatgpt-retrieval-plugin.git
```

Navigate to the cloned repository directory:

```
cd /path/to/chatgpt-retrieval-plugin
```

Install poetry:

```
pip install poetry
```

Create a new virtual environment that uses Python 3.10:

```
poetry env use python3.10
poetry shell
```

Install app dependencies using poetry:

```
poetry install
```

**Note:** If adding dependencies in the `pyproject.toml`, make sure to run `poetry lock` and `poetry install`.

#### General Environment Variables

The API requires the following environment variables to work:

| Name             | Required | Description                                                                                                                                                                                                                                                   |
| ---------------- | -------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `DATASTORE`      | Yes      | This specifies the vector database provider you want to use to store and query embeddings. You can choose from `elasticsearch`, `chroma`, `pinecone`, `weaviate`, `zilliz`, `milvus`, `qdrant`, `redis`, `azuresearch`, `supabase`, `postgres`, `analyticdb`. |
| `BEARER_TOKEN`   | Yes      | This is a secret token that you need to authenticate your requests to the API. You can generate one using any tool or method you prefer, such as [jwt.io](https://jwt.io/).                                                                                   |
| `OPENAI_API_KEY` | Yes      | This is your OpenAI API key that you need to generate embeddings using the `text-embedding-ada-002` model. You can get an API key by creating an account on [OpenAI](https://openai.com/).                                                                    |

### Using the plugin with Azure OpenAI

The Azure Open AI uses URLs that are specific to your resource and references models not by model name but by the deployment id. As a result, you need to set additional environment variables for this case.

In addition to the `OPENAI_API_BASE` (your specific URL) and `OPENAI_API_TYPE` (azure), you should also set `OPENAI_EMBEDDINGMODEL_DEPLOYMENTID` which specifies the model to use for getting embeddings on upsert and query. For this, we recommend deploying `text-embedding-ada-002` model and using the deployment name here.

If you wish to use the data preparation scripts, you will also need to set `OPENAI_METADATA_EXTRACTIONMODEL_DEPLOYMENTID`, used for metadata extraction and
`OPENAI_COMPLETIONMODEL_DEPLOYMENTID`, used for PII handling.

### Choosing a Vector Database

The plugin supports several vector database providers, each with different features, performance, and pricing. Depending on which one you choose, you will need to use a different Dockerfile and set different environment variables. The following sections provide brief introductions to each vector database provider.

For more detailed instructions on setting up and using each vector database provider, please refer to the respective documentation in the `/docs/providers/<datastore_name>/setup.md` file ([folders here](/docs/providers)).

#### Pinecone

[Pinecone](https://www.pinecone.io) is a managed vector database designed for speed, scale, and rapid deployment to production. It supports hybrid search and is currently the only datastore to natively support SPLADE sparse vectors. For detailed setup instructions, refer to [`/docs/providers/pinecone/setup.md`](/docs/providers/pinecone/setup.md).

#### Weaviate

[Weaviate](https://weaviate.io/) is an open-source vector search engine built to scale seamlessly into billions of data objects. It supports hybrid search out-of-the-box, making it suitable for users who require efficient keyword searches. Weaviate can be self-hosted or managed, offering flexibility in deployment. For detailed setup instructions, refer to [`/docs/providers/weaviate/setup.md`](/docs/providers/weaviate/setup.md).

#### Zilliz

[Zilliz](https://zilliz.com) is a managed cloud-native vector database designed for billion-scale data. It offers a wide range of features, including multiple indexing algorithms, distance metrics, scalar filtering, time travel searches, rollback with snapshots, full RBAC, 99.9% uptime, separated storage and compute, and multi-language SDKs. For detailed setup instructions, refer to [`/docs/providers/zilliz/setup.md`](/docs/providers/zilliz/setup.md).

#### Milvus

[Milvus](https://milvus.io/) is an open-source, cloud-native vector database that scales to billions of vectors. It is the open-source version of Zilliz and shares many of its features, such as various indexing algorithms, distance metrics, scalar filtering, time travel searches, rollback with snapshots, multi-language SDKs, storage and compute separation, and cloud scalability. For detailed setup instructions, refer to [`/docs/providers/milvus/setup.md`](/docs/providers/milvus/setup.md).

#### Qdrant

[Qdrant](https://qdrant.tech/) is a vector database capable of storing documents and vector embeddings. It offers both self-hosted and managed [Qdrant Cloud](https://cloud.qdrant.io/) deployment options, providing flexibility for users with different requirements. For detailed setup instructions, refer to [`/docs/providers/qdrant/setup.md`](/docs/providers/qdrant/setup.md).

#### Redis

[Redis](https://redis.com/solutions/use-cases/vector-database/) is a real-time data platform suitable for a variety of use cases, including everyday applications and AI/ML workloads. It can be used as a low-latency vector engine by creating a Redis database with the [Redis Stack docker container](/examples/docker/redis/docker-compose.yml). For a hosted/managed solution, [Redis Cloud](https://app.redislabs.com/#/) is available. For detailed setup instructions, refer to [`/docs/providers/redis/setup.md`](/docs/providers/redis/setup.md).

#### LlamaIndex

[LlamaIndex](https://github.com/jerryjliu/llama_index) is a central interface to connect your LLM's with external data.
It provides a suite of in-memory indices over your unstructured and structured data for use with ChatGPT.
Unlike standard vector databases, LlamaIndex supports a wide range of indexing strategies (e.g. tree, keyword table, knowledge graph) optimized for different use-cases.
It is light-weight, easy-to-use, and requires no additional deployment.
All you need to do is specifying a few environment variables (optionally point to an existing saved Index json file).
Note that metadata filters in queries are not yet supported.
For detailed setup instructions, refer to [`/docs/providers/llama/setup.md`](/docs/providers/llama/setup.md).

#### Chroma

[Chroma](https://trychroma.com) is an AI-native open-source embedding database designed to make getting started as easy as possible. Chroma runs in-memory, or in a client-server setup. It supports metadata and keyword filtering out of the box. For detailed instructions, refer to [`/docs/providers/chroma/setup.md`](/docs/providers/chroma/setup.md).

#### Azure Cognitive Search

[Azure Cognitive Search](https://azure.microsoft.com/products/search/) is a complete retrieval cloud service that supports vector search, text search, and hybrid (vectors + text combined to yield the best of the two approaches). It also offers an [optional L2 re-ranking step](https://learn.microsoft.com/azure/search/semantic-search-overview) to further improve results quality. For detailed setup instructions, refer to [`/docs/providers/azuresearch/setup.md`](/docs/providers/azuresearch/setup.md)

#### Supabase

[Supabase](https://supabase.com/blog/openai-embeddings-postgres-vector) offers an easy and efficient way to store vectors via [pgvector](https://github.com/pgvector/pgvector) extension for Postgres Database. [You can use Supabase CLI](https://github.com/supabase/cli) to set up a whole Supabase stack locally or in the cloud or you can also use docker-compose, k8s and other options available. For a hosted/managed solution, try [Supabase.com](https://supabase.com/) and unlock the full power of Postgres with built-in authentication, storage, auto APIs, and Realtime features. For detailed setup instructions, refer to [`/docs/providers/supabase/setup.md`](/docs/providers/supabase/setup.md).

#### Postgres

[Postgres](https://www.postgresql.org) offers an easy and efficient way to store vectors via [pgvector](https://github.com/pgvector/pgvector) extension. To use pgvector, you will need to set up a PostgreSQL database with the pgvector extension enabled. For example, you can [use docker](https://www.docker.com/blog/how-to-use-the-postgres-docker-official-image/) to run locally. For a hosted/managed solution, you can use any of the cloud vendors which support [pgvector](https://github.com/pgvector/pgvector#hosted-postgres). For detailed setup instructions, refer to [`/docs/providers/postgres/setup.md`](/docs/providers/postgres/setup.md).

#### AnalyticDB

[AnalyticDB](https://www.alibabacloud.com/help/en/analyticdb-for-postgresql/latest/product-introduction-overview) is a distributed cloud-native vector database designed for storing documents and vector embeddings. It is fully compatible with PostgreSQL syntax and managed by Alibaba Cloud. AnalyticDB offers a powerful vector compute engine, processing billions of data vectors and providing features such as indexing algorithms, structured and unstructured data capabilities, real-time updates, distance metrics, scalar filtering, and time travel searches. For detailed setup instructions, refer to [`/docs/providers/analyticdb/setup.md`](/docs/providers/analyticdb/setup.md).

#### Elasticsearch

[Elasticsearch](https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html) currently supports storing vectors through the `dense_vector` field type and uses them to calculate document scores. Elasticsearch 8.0 builds on this functionality to support fast, approximate nearest neighbor search (ANN). This represents a much more scalable approach, allowing vector search to run efficiently on large datasets. For detailed setup instructions, refer to [`/docs/providers/elasticsearch/setup.md`](/docs/providers/elasticsearch/setup.md).

### Running the API locally

To run the API locally, you first need to set the requisite environment variables with the `export` command:

```
export DATASTORE=<your_datastore>
export BEARER_TOKEN=<your_bearer_token>
export OPENAI_API_KEY=<your_openai_api_key>
<Add the environment variables for your chosen vector DB here>
```

Start the API with:

```
poetry run start
```

Append `docs` to the URL shown in the terminal and open it in a browser to access the API documentation and try out the endpoints (i.e. http://0.0.0.0:8000/docs). Make sure to enter your bearer token and test the API endpoints.

**Note:** If you add new dependencies to the pyproject.toml file, you need to run `poetry lock` and `poetry install` to update the lock file and install the new dependencies.

### Testing a Localhost Plugin in ChatGPT

To test a localhost plugin in ChatGPT, use the provided [`local_server/main.py`](/local_server/main.py) file, which is specifically configured for localhost testing with CORS settings, no authentication and routes for the manifest, OpenAPI schema and logo.

Follow these steps to test your localhost plugin:

1. Run the localhost server using the `poetry run dev` command. This starts the server at the default address (e.g. `localhost:3333`).

2. Visit [ChatGPT](https://chat.openai.com/), select "Plugins" from the model picker, click on the plugins picker, and click on "Plugin store" at the bottom of the list.

3. Choose "Develop your own plugin" and enter your localhost URL (e.g. `localhost:3333`) when prompted.

4. Your localhost plugin is now enabled for your ChatGPT session.

For more information, refer to the [OpenAI documentation](https://platform.openai.com/docs/plugins/getting-started/openapi-definition).

### Personalization

You can personalize the Retrieval Plugin for your own use case by doing the following:

- **Replace the logo**: Replace the image in [logo.png](/.well-known/logo.png) with your own logo.

- **Edit the data models**: Edit the `DocumentMetadata` and `DocumentMetadataFilter` data models in [models.py](/models/models.py) to add custom metadata fields. Update the OpenAPI schema in [openapi.yaml](/.well-known/openapi.yaml) accordingly. To update the OpenAPI schema more easily, you can run the app locally, then navigate to `http://0.0.0.0:8000/sub/openapi.json` and copy the contents of the webpage. Then go to [Swagger Editor](https://editor.swagger.io/) and paste in the JSON to convert it to a YAML format. You could also replace the [openapi.yaml](/.well-known/openapi.yaml) file with an openapi.json file in the [.well-known](/.well-known) folder.

- **Change the plugin name, description, and usage instructions**: Update the plugin name, user-facing description, and usage instructions for the model. You can either edit the descriptions in the [main.py](/server/main.py) file or update the [openapi.yaml](/.well-known/openapi.yaml) file. Follow the same instructions as in the previous step to update the OpenAPI schema.

- **Enable ChatGPT to save information from conversations**: See the instructions in the [memory example folder](/examples/memory).

### Authentication Methods

You can choose from four options for authenticating requests to your plugin:

1. **No Authentication**: Anyone can add your plugin and use its API without any credentials. This option is suitable if you are only exposing documents that are not sensitive or already public. It provides no security for your data. If using this method, copy the contents of this [main.py](/examples/authentication-methods/no-auth/main.py) into the [actual main.py file](/server/main.py). Example manifest [here](/examples/authentication-methods/no-auth/ai-plugin.json).

2. **HTTP Bearer**: You can use a secret token as a header to authorize requests to your plugin. There are two variants of this option:

   - **User Level** (default for this implementation): Each user who adds your plugin to ChatGPT must provide the bearer token when adding the plugin. You can generate and distribute these tokens using any tool or method you prefer, such as [jwt.io](https://jwt.io/). This method provides better security as each user has to enter the shared access token. If you require a unique access token for each user, you will need to implement this yourself in the [main.py](/server/main.py) file. Example manifest [here](/examples/authentication-methods/user-http/ai-plugin.json).

   - **Service Level**: Anyone can add your plugin and use its API without credentials, but you must add a bearer token when registering the plugin. When you install your plugin, you need to add your bearer token, and will then receive a token from ChatGPT that you must include in your hosted manifest file. Your token will be used by ChatGPT to authorize requests to your plugin on behalf of all users who add it. This method is more convenient for users, but it may be less secure as all users share the same token and do not need to add a token to install the plugin. Example manifest [here](/examples/authentication-methods/service-http/ai-plugin.json).

3. **OAuth**: Users must go through an OAuth flow to add your plugin. You can use an OAuth provider to authenticate users who add your plugin and grant them access to your API. This method offers the highest level of security and control, as users authenticate through a trusted third-party provider. However, you will need to implement the OAuth flow yourself in the [main.py](/server/main.py) file and provide the necessary parameters in your manifest file. Example manifest [here](/examples/authentication-methods/oauth/ai-plugin.json).

Consider the benefits and drawbacks of each authentication method before choosing the one that best suits your use case and security requirements. If you choose to use a method different to the default (User Level HTTP), make sure to update the manifest file [here](/.well-known/ai-plugin.json).

## Deployment

You can deploy your app to different cloud providers, depending on your preferences and requirements. However, regardless of the provider you choose, you will need to update two files in your app: [openapi.yaml](/.well-known/openapi.yaml) and [ai-plugin.json](/.well-known/ai-plugin.json). As outlined above, these files define the API specification and the AI plugin configuration for your app, respectively. You need to change the url field in both files to match the address of your deployed app.

Render has a 1-click deploy option that automatically updates the url field in both files:

[<img src="https://render.com/images/deploy-to-render-button.svg" alt="Deploy to Render" />](https://render.com/deploy?repo=https://github.com/render-examples/chatgpt-retrieval-plugin/tree/main)

Before deploying your app, you might want to remove unused dependencies from your [pyproject.toml](/pyproject.toml) file to reduce the size of your app and improve its performance. Depending on the vector database provider you choose, you can remove the packages that are not needed for your specific provider. Refer to the respective documentation in the [`/docs/deployment/removing-unused-dependencies.md`](/docs/deployment/removing-unused-dependencies.md) file for information on removing unused dependencies for each provider.

Instructions:

- [Deploying to Fly.io](/docs/deployment/flyio.md)
- [Deploying to Heroku](/docs/deployment/heroku.md)
- [Deploying to Render](/docs/deployment/render.md)
- [Other Deployment Options](/docs/deployment/other-options.md) (Azure Container Apps, Google Cloud Run, AWS Elastic Container Service, etc.)

Once you have deployed your app, consider uploading an initial batch of documents using one of [these scripts](/scripts) or by calling the `/upsert` endpoint.

## Installing a Developer Plugin

To install a developer plugin, follow the steps below:

- First, create your developer plugin by deploying it to your preferred hosting platform (e.g. Fly.io, Heroku, etc.) and updating the plugin URL in the manifest file and OpenAPI schema.

- Go to [ChatGPT](https://chat.openai.com/) and select "Plugins" from the model picker.

- From the plugins picker, scroll to the bottom and click on "Plugin store."

- Go to "Develop your own plugin" and follow the instructions provided. You will need to enter the domain where your plugin is deployed.

- Follow the instructions based on the authentication type you have chosen for your plugin (e.g. if your plugin uses Service Level HTTP, you will have to paste in your access token, then paste the new access token you receive from the plugin flow into your [ai-plugin.json](/.well-known/ai-plugin.json) file and redeploy your app).

- Next, you must add your plugin. Go to the "Plugin store" again and click on "Install an unverified plugin."

- Follow the instructions provided, which will require you to enter the domain where your plugin is deployed.

- Follow the instructions based on the authentication type you have chosen for your plugin (e.g. if your plugin uses User Level HTTP, you will have to paste in your bearer token).

After completing these steps, your developer plugin should be installed and ready to use in ChatGPT.

## Webhooks

To keep the documents stored in the vector database up-to-date, consider using tools like [Zapier](https://zapier.com) or [Make](https://www.make.com) to configure incoming webhooks to your plugin's API based on events or schedules. For example, this could allow you to sync new information as you update your notes or receive emails. You can also use a [Zapier Transfer](https://zapier.com/blog/zapier-transfer-guide/) to batch process a collection of existing documents and upload them to the vector database.

If you need to pass custom fields from these tools to your plugin, you might want to create an additional Retrieval Plugin API endpoint that calls the datastore's upsert function, such as `upsert-email`. This custom endpoint can be designed to accept specific fields from the webhook and process them accordingly.

To set up an incoming webhook, follow these general steps:

- Choose a webhook tool like Zapier or Make and create an account.
- Set up a new webhook or transfer in the tool, and configure it to trigger based on events or schedules.
- Specify the target URL for the webhook, which should be the API endpoint of your retrieval plugin (e.g. `https://your-plugin-url.com/upsert`).
- Configure the webhook payload to include the necessary data fields and format them according to your retrieval plugin's API requirements.
- Test the webhook to ensure it's working correctly and sending data to your retrieval plugin as expected.

After setting up the webhook, you may want to run a backfill to ensure that any previously missed data is included in the vector database.

Remember that if you want to use incoming webhooks to continuously sync data, you should consider running a backfill after setting these up to avoid missing any data.

In addition to using tools like Zapier and Make, you can also build your own custom integrations to sync data with your Retrieval Plugin. This allows you to have more control over the data flow and tailor the integration to your specific needs and requirements.

## Scripts

The `scripts` folder contains scripts to batch upsert or process text documents from different data sources, such as a zip file, JSON file, or JSONL file. These scripts use the plugin's upsert utility functions to upload the documents and their metadata to the vector database, after converting them to plain text and splitting them into chunks. Each script folder has a README file that explains how to use it and what parameters it requires. You can also optionally screen the documents for personally identifiable information (PII) using a language model and skip them if detected, with the [`services.pii_detection`](/services/pii_detection.py) module. This can be helpful if you want to avoid uploading sensitive or private documents to the vector database unintentionally. Additionally, you can optionally extract metadata from the document text using a language model, with the [`services.extract_metadata`](/services/extract_metadata.py) module. This can be useful if you want to enrich the document metadata. **Note:** if using incoming webhooks to continuously sync data, consider running a backfill after setting these up to avoid missing any data.

The scripts are:

- [`process_json`](scripts/process_json/): This script processes a file dump of documents in a JSON format and stores them in the vector database with some metadata. The format of the JSON file should be a list of JSON objects, where each object represents a document. The JSON object should have a `text` field and optionally other fields to populate the metadata. You can provide custom metadata as a JSON string and flags to screen for PII and extract metadata.
- [`process_jsonl`](scripts/process_jsonl/): This script processes a file dump of documents in a JSONL format and stores them in the vector database with some metadata. The format of the JSONL file should be a newline-delimited JSON file, where each line is a valid JSON object representing a document. The JSON object should have a `text` field and optionally other fields to populate the metadata. You can provide custom metadata as a JSON string and flags to screen for PII and extract metadata.
- [`process_zip`](scripts/process_zip/): This script processes a file dump of documents in a zip file and stores them in the vector database with some metadata. The format of the zip file should be a flat zip file folder of docx, pdf, txt, md, pptx or csv files. You can provide custom metadata as a JSON string and flags to screen for PII and extract metadata.

## Pull Request (PR) Checklist

If you'd like to contribute, please follow the checklist below when submitting a PR. This will help us review and merge your changes faster! Thank you for contributing!

1. **Type of PR**: Indicate the type of PR by adding a label in square brackets at the beginning of the title, such as `[Bugfix]`, `[Feature]`, `[Enhancement]`, `[Refactor]`, or `[Documentation]`.

2. **Short Description**: Provide a brief, informative description of the PR that explains the changes made.

3. **Issue(s) Linked**: Mention any related issue(s) by using the keyword `Fixes` or `Closes` followed by the respective issue number(s) (e.g., Fixes #123, Closes #456).

4. **Branch**: Ensure that you have created a new branch for the changes, and it is based on the latest version of the `main` branch.

5. **Code Changes**: Make sure the code changes are minimal, focused, and relevant to the issue or feature being addressed.

6. **Commit Messages**: Write clear and concise commit messages that explain the purpose of each commit.

7. **Tests**: Include unit tests and/or integration tests for any new code or changes to existing code. Make sure all tests pass before submitting the PR.

8. **Documentation**: Update relevant documentation (e.g., README, inline comments, or external documentation) to reflect any changes made.

9. **Review Requested**: Request a review from at least one other contributor or maintainer of the repository.

10. **Video Submission** (For Complex/Large PRs): If your PR introduces significant changes, complexities, or a large number of lines of code, submit a brief video walkthrough along with the PR. The video should explain the purpose of the changes, the logic behind them, and how they address the issue or add the proposed feature. This will help reviewers to better understand your contribution and expedite the review process.

## Pull Request Naming Convention

Use the following naming convention for your PR branches:

```
<type>/<short-description>-<issue-number>
```

- `<type>`: The type of PR, such as `bugfix`, `feature`, `enhancement`, `refactor`, or `docs`. Multiple types are ok and should appear as <type>, <type2>
- `<short-description>`: A brief description of the changes made, using hyphens to separate words.
- `<issue-number>`: The issue number associated with the changes made (if applicable).

Example:

```
feature/advanced-chunking-strategy-123
```

## Limitations

While the ChatGPT Retrieval Plugin is designed to provide a flexible solution for semantic search and retrieval, it does have some limitations:

- **Keyword search limitations**: The embeddings generated by the `text-embedding-ada-002` model may not always be effective at capturing exact keyword matches. As a result, the plugin might not return the most relevant results for queries that rely heavily on specific keywords. Some vector databases, like Elasticsearch, Pinecone, Weaviate and Azure Cognitive Search, use hybrid search and might perform better for keyword searches.
- **Sensitive data handling**: The plugin does not automatically detect or filter sensitive data. It is the responsibility of the developers to ensure that they have the necessary authorization to include content in the Retrieval Plugin and that the content complies with data privacy requirements.
- **Scalability**: The performance of the plugin may vary depending on the chosen vector database provider and the size of the dataset. Some providers may offer better scalability and performance than others.
- **Language support**: The plugin currently uses OpenAI's `text-embedding-ada-002` model, which is optimized for use in English. However, it is still robust enough to generate good results for a variety of languages.
- **Metadata extraction**: The optional metadata extraction feature relies on a language model to extract information from the document text. This process may not always be accurate, and the quality of the extracted metadata may vary depending on the document content and structure.
- **PII detection**: The optional PII detection feature is not foolproof and may not catch all instances of personally identifiable information. Use this feature with caution and verify its effectiveness for your specific use case.

## Future Directions

The ChatGPT Retrieval Plugin provides a flexible solution for semantic search and retrieval, but there is always potential for further development. We encourage users to contribute to the project by submitting pull requests for new features or enhancements. Notable contributions may be acknowledged with OpenAI credits.

Some ideas for future directions include:

- **More vector database providers**: If you are interested in integrating another vector database provider with the ChatGPT Retrieval Plugin, feel free to submit an implementation.
- **Additional scripts**: Expanding the range of scripts available for processing and uploading documents from various data sources would make the plugin even more versatile.
- **User Interface**: Developing a user interface for managing documents and interacting with the plugin could improve the user experience.
- **Hybrid search / TF-IDF option**: Enhancing the [datastore's upsert function](/datastore/datastore.py#L18) with an option to use hybrid search or TF-IDF indexing could improve the plugin's performance for keyword-based queries.
- **Advanced chunking strategies and embeddings calculations**: Implementing more sophisticated chunking strategies and embeddings calculations, such as embedding document titles and summaries, performing weighted averaging of document chunks and summaries, or calculating the average embedding for a document, could lead to better search results.
- **Custom metadata**: Allowing users to add custom metadata to document chunks, such as titles or other relevant information, might improve the retrieved results in some use cases.
- **Additional optional services**: Integrating more optional services, such as summarizing documents or pre-processing documents before embedding them, could enhance the plugin's functionality and quality of retrieved results. These services could be implemented using language models and integrated directly into the plugin, rather than just being available in the scripts.

We welcome contributions from the community to help improve the ChatGPT Retrieval Plugin and expand its capabilities. If you have an idea or feature you'd like to contribute, please submit a pull request to the repository.

## Contributors

We would like to extend our gratitude to the following contributors for their code / documentation contributions, and support in integrating various vector database providers with the ChatGPT Retrieval Plugin:

- [Pinecone](https://www.pinecone.io/)
  - [acatav](https://github.com/acatav)
  - [gkogan](https://github.com/gkogan)
  - [jamescalam](https://github.com/jamescalam)
- [Weaviate](https://www.semi.technology/)
  - [byronvoorbach](https://github.com/byronvoorbach)
  - [hsm207](https://github.com/hsm207)
  - [sebawita](https://github.com/sebawita)
- [Zilliz](https://zilliz.com/)
  - [filip-halt](https://github.com/filip-halt)
- [Milvus](https://milvus.io/)
  - [filip-halt](https://github.com/filip-halt)
- [Qdrant](https://qdrant.tech/)
  - [kacperlukawski](https://github.com/kacperlukawski)
- [Redis](https://redis.io/)
  - [spartee](https://github.com/spartee)
  - [tylerhutcherson](https://github.com/tylerhutcherson)
- [LlamaIndex](https://github.com/jerryjliu/llama_index)
  - [jerryjliu](https://github.com/jerryjliu)
  - [Disiok](https://github.com/Disiok)
- [Supabase](https://supabase.com/)
  - [egor-romanov](https://github.com/egor-romanov)
- [Postgres](https://www.postgresql.org/)
  - [egor-romanov](https://github.com/egor-romanov)
  - [mmmaia](https://github.com/mmmaia)
- [Elasticsearch](https://www.elastic.co/)
  - [joemcelroy](https://github.com/joemcelroy)



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/Dockerfile
================================================

FROM python:3.10 as requirements-stage

WORKDIR /tmp

RUN pip install poetry

COPY ./pyproject.toml ./poetry.lock* /tmp/


RUN poetry export -f requirements.txt --output requirements.txt --without-hashes

FROM python:3.10

WORKDIR /code

COPY --from=requirements-stage /tmp/requirements.txt /code/requirements.txt

RUN pip install --no-cache-dir --upgrade -r /code/requirements.txt

COPY . /code/

# Heroku uses PORT, Azure App Services uses WEBSITES_PORT, Fly.io uses 8080 by default
CMD ["sh", "-c", "uvicorn server.main:app --host 0.0.0.0 --port ${PORT:-${WEBSITES_PORT:-8080}}"]



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/LICENSE
================================================
MIT License

Copyright (c) 2023 OpenAI

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/Makefile
================================================
# Heroku
# make heroku-login
# make heroku-push

HEROKU_APP = <your app name> 

heroku-push:
	docker buildx build --platform linux/amd64 -t ${HEROKU_APP} .
	docker tag ${HEROKU_APP} registry.heroku.com/${HEROKU_APP}/web
	docker push registry.heroku.com/${HEROKU_APP}/web
	heroku container:release web -a ${HEROKU_APP}

heroku-login:
	heroku container:login



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/poetry.lock
================================================
# This file is automatically @generated by Poetry and should not be changed by hand.

[[package]]
name = "aiohttp"
version = "3.8.4"
description = "Async http client/server framework (asyncio)"
category = "main"
optional = false
python-versions = ">=3.6"
files = [
    {file = "aiohttp-3.8.4-cp310-cp310-macosx_10_9_universal2.whl", hash = "sha256:5ce45967538fb747370308d3145aa68a074bdecb4f3a300869590f725ced69c1"},
    {file = "aiohttp-3.8.4-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:b744c33b6f14ca26b7544e8d8aadff6b765a80ad6164fb1a430bbadd593dfb1a"},
    {file = "aiohttp-3.8.4-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:1a45865451439eb320784918617ba54b7a377e3501fb70402ab84d38c2cd891b"},
    {file = "aiohttp-3.8.4-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:a86d42d7cba1cec432d47ab13b6637bee393a10f664c425ea7b305d1301ca1a3"},
    {file = "aiohttp-3.8.4-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:ee3c36df21b5714d49fc4580247947aa64bcbe2939d1b77b4c8dcb8f6c9faecc"},
    {file = "aiohttp-3.8.4-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:176a64b24c0935869d5bbc4c96e82f89f643bcdf08ec947701b9dbb3c956b7dd"},
    {file = "aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:c844fd628851c0bc309f3c801b3a3d58ce430b2ce5b359cd918a5a76d0b20cb5"},
    {file = "aiohttp-3.8.4-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:5393fb786a9e23e4799fec788e7e735de18052f83682ce2dfcabaf1c00c2c08e"},
    {file = "aiohttp-3.8.4-cp310-cp310-musllinux_1_1_aarch64.whl", hash = "sha256:e4b09863aae0dc965c3ef36500d891a3ff495a2ea9ae9171e4519963c12ceefd"},
    {file = "aiohttp-3.8.4-cp310-cp310-musllinux_1_1_i686.whl", hash = "sha256:adfbc22e87365a6e564c804c58fc44ff7727deea782d175c33602737b7feadb6"},
    {file = "aiohttp-3.8.4-cp310-cp310-musllinux_1_1_ppc64le.whl", hash = "sha256:147ae376f14b55f4f3c2b118b95be50a369b89b38a971e80a17c3fd623f280c9"},
    {file = "aiohttp-3.8.4-cp310-cp310-musllinux_1_1_s390x.whl", hash = "sha256:eafb3e874816ebe2a92f5e155f17260034c8c341dad1df25672fb710627c6949"},
    {file = "aiohttp-3.8.4-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:c6cc15d58053c76eacac5fa9152d7d84b8d67b3fde92709195cb984cfb3475ea"},
    {file = "aiohttp-3.8.4-cp310-cp310-win32.whl", hash = "sha256:59f029a5f6e2d679296db7bee982bb3d20c088e52a2977e3175faf31d6fb75d1"},
    {file = "aiohttp-3.8.4-cp310-cp310-win_amd64.whl", hash = "sha256:fe7ba4a51f33ab275515f66b0a236bcde4fb5561498fe8f898d4e549b2e4509f"},
    {file = "aiohttp-3.8.4-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:3d8ef1a630519a26d6760bc695842579cb09e373c5f227a21b67dc3eb16cfea4"},
    {file = "aiohttp-3.8.4-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:5b3f2e06a512e94722886c0827bee9807c86a9f698fac6b3aee841fab49bbfb4"},
    {file = "aiohttp-3.8.4-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:3a80464982d41b1fbfe3154e440ba4904b71c1a53e9cd584098cd41efdb188ef"},
    {file = "aiohttp-3.8.4-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:8b631e26df63e52f7cce0cce6507b7a7f1bc9b0c501fcde69742130b32e8782f"},
    {file = "aiohttp-3.8.4-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:3f43255086fe25e36fd5ed8f2ee47477408a73ef00e804cb2b5cba4bf2ac7f5e"},
    {file = "aiohttp-3.8.4-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:4d347a172f866cd1d93126d9b239fcbe682acb39b48ee0873c73c933dd23bd0f"},
    {file = "aiohttp-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:a3fec6a4cb5551721cdd70473eb009d90935b4063acc5f40905d40ecfea23e05"},
    {file = "aiohttp-3.8.4-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:80a37fe8f7c1e6ce8f2d9c411676e4bc633a8462844e38f46156d07a7d401654"},
    {file = "aiohttp-3.8.4-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:d1e6a862b76f34395a985b3cd39a0d949ca80a70b6ebdea37d3ab39ceea6698a"},
    {file = "aiohttp-3.8.4-cp311-cp311-musllinux_1_1_i686.whl", hash = "sha256:cd468460eefef601ece4428d3cf4562459157c0f6523db89365202c31b6daebb"},
    {file = "aiohttp-3.8.4-cp311-cp311-musllinux_1_1_ppc64le.whl", hash = "sha256:618c901dd3aad4ace71dfa0f5e82e88b46ef57e3239fc7027773cb6d4ed53531"},
    {file = "aiohttp-3.8.4-cp311-cp311-musllinux_1_1_s390x.whl", hash = "sha256:652b1bff4f15f6287550b4670546a2947f2a4575b6c6dff7760eafb22eacbf0b"},
    {file = "aiohttp-3.8.4-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:80575ba9377c5171407a06d0196b2310b679dc752d02a1fcaa2bc20b235dbf24"},
    {file = "aiohttp-3.8.4-cp311-cp311-win32.whl", hash = "sha256:bbcf1a76cf6f6dacf2c7f4d2ebd411438c275faa1dc0c68e46eb84eebd05dd7d"},
    {file = "aiohttp-3.8.4-cp311-cp311-win_amd64.whl", hash = "sha256:6e74dd54f7239fcffe07913ff8b964e28b712f09846e20de78676ce2a3dc0bfc"},
    {file = "aiohttp-3.8.4-cp36-cp36m-macosx_10_9_x86_64.whl", hash = "sha256:880e15bb6dad90549b43f796b391cfffd7af373f4646784795e20d92606b7a51"},
    {file = "aiohttp-3.8.4-cp36-cp36m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:bb96fa6b56bb536c42d6a4a87dfca570ff8e52de2d63cabebfd6fb67049c34b6"},
    {file = "aiohttp-3.8.4-cp36-cp36m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:4a6cadebe132e90cefa77e45f2d2f1a4b2ce5c6b1bfc1656c1ddafcfe4ba8131"},
    {file = "aiohttp-3.8.4-cp36-cp36m-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:f352b62b45dff37b55ddd7b9c0c8672c4dd2eb9c0f9c11d395075a84e2c40f75"},
    {file = "aiohttp-3.8.4-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:7ab43061a0c81198d88f39aaf90dae9a7744620978f7ef3e3708339b8ed2ef01"},
    {file = "aiohttp-3.8.4-cp36-cp36m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:c9cb1565a7ad52e096a6988e2ee0397f72fe056dadf75d17fa6b5aebaea05622"},
    {file = "aiohttp-3.8.4-cp36-cp36m-musllinux_1_1_aarch64.whl", hash = "sha256:1b3ea7edd2d24538959c1c1abf97c744d879d4e541d38305f9bd7d9b10c9ec41"},
    {file = "aiohttp-3.8.4-cp36-cp36m-musllinux_1_1_i686.whl", hash = "sha256:7c7837fe8037e96b6dd5cfcf47263c1620a9d332a87ec06a6ca4564e56bd0f36"},
    {file = "aiohttp-3.8.4-cp36-cp36m-musllinux_1_1_ppc64le.whl", hash = "sha256:3b90467ebc3d9fa5b0f9b6489dfb2c304a1db7b9946fa92aa76a831b9d587e99"},
    {file = "aiohttp-3.8.4-cp36-cp36m-musllinux_1_1_s390x.whl", hash = "sha256:cab9401de3ea52b4b4c6971db5fb5c999bd4260898af972bf23de1c6b5dd9d71"},
    {file = "aiohttp-3.8.4-cp36-cp36m-musllinux_1_1_x86_64.whl", hash = "sha256:d1f9282c5f2b5e241034a009779e7b2a1aa045f667ff521e7948ea9b56e0c5ff"},
    {file = "aiohttp-3.8.4-cp36-cp36m-win32.whl", hash = "sha256:5e14f25765a578a0a634d5f0cd1e2c3f53964553a00347998dfdf96b8137f777"},
    {file = "aiohttp-3.8.4-cp36-cp36m-win_amd64.whl", hash = "sha256:4c745b109057e7e5f1848c689ee4fb3a016c8d4d92da52b312f8a509f83aa05e"},
    {file = "aiohttp-3.8.4-cp37-cp37m-macosx_10_9_x86_64.whl", hash = "sha256:aede4df4eeb926c8fa70de46c340a1bc2c6079e1c40ccf7b0eae1313ffd33519"},
    {file = "aiohttp-3.8.4-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:4ddaae3f3d32fc2cb4c53fab020b69a05c8ab1f02e0e59665c6f7a0d3a5be54f"},
    {file = "aiohttp-3.8.4-cp37-cp37m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:c4eb3b82ca349cf6fadcdc7abcc8b3a50ab74a62e9113ab7a8ebc268aad35bb9"},
    {file = "aiohttp-3.8.4-cp37-cp37m-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:9bcb89336efa095ea21b30f9e686763f2be4478f1b0a616969551982c4ee4c3b"},
    {file = "aiohttp-3.8.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6c08e8ed6fa3d477e501ec9db169bfac8140e830aa372d77e4a43084d8dd91ab"},
    {file = "aiohttp-3.8.4-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:c6cd05ea06daca6ad6a4ca3ba7fe7dc5b5de063ff4daec6170ec0f9979f6c332"},
    {file = "aiohttp-3.8.4-cp37-cp37m-musllinux_1_1_aarch64.whl", hash = "sha256:b7a00a9ed8d6e725b55ef98b1b35c88013245f35f68b1b12c5cd4100dddac333"},
    {file = "aiohttp-3.8.4-cp37-cp37m-musllinux_1_1_i686.whl", hash = "sha256:de04b491d0e5007ee1b63a309956eaed959a49f5bb4e84b26c8f5d49de140fa9"},
    {file = "aiohttp-3.8.4-cp37-cp37m-musllinux_1_1_ppc64le.whl", hash = "sha256:40653609b3bf50611356e6b6554e3a331f6879fa7116f3959b20e3528783e699"},
    {file = "aiohttp-3.8.4-cp37-cp37m-musllinux_1_1_s390x.whl", hash = "sha256:dbf3a08a06b3f433013c143ebd72c15cac33d2914b8ea4bea7ac2c23578815d6"},
    {file = "aiohttp-3.8.4-cp37-cp37m-musllinux_1_1_x86_64.whl", hash = "sha256:854f422ac44af92bfe172d8e73229c270dc09b96535e8a548f99c84f82dde241"},
    {file = "aiohttp-3.8.4-cp37-cp37m-win32.whl", hash = "sha256:aeb29c84bb53a84b1a81c6c09d24cf33bb8432cc5c39979021cc0f98c1292a1a"},
    {file = "aiohttp-3.8.4-cp37-cp37m-win_amd64.whl", hash = "sha256:db3fc6120bce9f446d13b1b834ea5b15341ca9ff3f335e4a951a6ead31105480"},
    {file = "aiohttp-3.8.4-cp38-cp38-macosx_10_9_universal2.whl", hash = "sha256:fabb87dd8850ef0f7fe2b366d44b77d7e6fa2ea87861ab3844da99291e81e60f"},
    {file = "aiohttp-3.8.4-cp38-cp38-macosx_10_9_x86_64.whl", hash = "sha256:91f6d540163f90bbaef9387e65f18f73ffd7c79f5225ac3d3f61df7b0d01ad15"},
    {file = "aiohttp-3.8.4-cp38-cp38-macosx_11_0_arm64.whl", hash = "sha256:d265f09a75a79a788237d7f9054f929ced2e69eb0bb79de3798c468d8a90f945"},
    {file = "aiohttp-3.8.4-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:3d89efa095ca7d442a6d0cbc755f9e08190ba40069b235c9886a8763b03785da"},
    {file = "aiohttp-3.8.4-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:4dac314662f4e2aa5009977b652d9b8db7121b46c38f2073bfeed9f4049732cd"},
    {file = "aiohttp-3.8.4-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:fe11310ae1e4cd560035598c3f29d86cef39a83d244c7466f95c27ae04850f10"},
    {file = "aiohttp-3.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6ddb2a2026c3f6a68c3998a6c47ab6795e4127315d2e35a09997da21865757f8"},
    {file = "aiohttp-3.8.4-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:e75b89ac3bd27d2d043b234aa7b734c38ba1b0e43f07787130a0ecac1e12228a"},
    {file = "aiohttp-3.8.4-cp38-cp38-musllinux_1_1_aarch64.whl", hash = "sha256:6e601588f2b502c93c30cd5a45bfc665faaf37bbe835b7cfd461753068232074"},
    {file = "aiohttp-3.8.4-cp38-cp38-musllinux_1_1_i686.whl", hash = "sha256:a5d794d1ae64e7753e405ba58e08fcfa73e3fad93ef9b7e31112ef3c9a0efb52"},
    {file = "aiohttp-3.8.4-cp38-cp38-musllinux_1_1_ppc64le.whl", hash = "sha256:a1f4689c9a1462f3df0a1f7e797791cd6b124ddbee2b570d34e7f38ade0e2c71"},
    {file = "aiohttp-3.8.4-cp38-cp38-musllinux_1_1_s390x.whl", hash = "sha256:3032dcb1c35bc330134a5b8a5d4f68c1a87252dfc6e1262c65a7e30e62298275"},
    {file = "aiohttp-3.8.4-cp38-cp38-musllinux_1_1_x86_64.whl", hash = "sha256:8189c56eb0ddbb95bfadb8f60ea1b22fcfa659396ea36f6adcc521213cd7b44d"},
    {file = "aiohttp-3.8.4-cp38-cp38-win32.whl", hash = "sha256:33587f26dcee66efb2fff3c177547bd0449ab7edf1b73a7f5dea1e38609a0c54"},
    {file = "aiohttp-3.8.4-cp38-cp38-win_amd64.whl", hash = "sha256:e595432ac259af2d4630008bf638873d69346372d38255774c0e286951e8b79f"},
    {file = "aiohttp-3.8.4-cp39-cp39-macosx_10_9_universal2.whl", hash = "sha256:5a7bdf9e57126dc345b683c3632e8ba317c31d2a41acd5800c10640387d193ed"},
    {file = "aiohttp-3.8.4-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:22f6eab15b6db242499a16de87939a342f5a950ad0abaf1532038e2ce7d31567"},
    {file = "aiohttp-3.8.4-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:7235604476a76ef249bd64cb8274ed24ccf6995c4a8b51a237005ee7a57e8643"},
    {file = "aiohttp-3.8.4-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:ea9eb976ffdd79d0e893869cfe179a8f60f152d42cb64622fca418cd9b18dc2a"},
    {file = "aiohttp-3.8.4-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:92c0cea74a2a81c4c76b62ea1cac163ecb20fb3ba3a75c909b9fa71b4ad493cf"},
    {file = "aiohttp-3.8.4-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:493f5bc2f8307286b7799c6d899d388bbaa7dfa6c4caf4f97ef7521b9cb13719"},
    {file = "aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:0a63f03189a6fa7c900226e3ef5ba4d3bd047e18f445e69adbd65af433add5a2"},
    {file = "aiohttp-3.8.4-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:10c8cefcff98fd9168cdd86c4da8b84baaa90bf2da2269c6161984e6737bf23e"},
    {file = "aiohttp-3.8.4-cp39-cp39-musllinux_1_1_aarch64.whl", hash = "sha256:bca5f24726e2919de94f047739d0a4fc01372801a3672708260546aa2601bf57"},
    {file = "aiohttp-3.8.4-cp39-cp39-musllinux_1_1_i686.whl", hash = "sha256:03baa76b730e4e15a45f81dfe29a8d910314143414e528737f8589ec60cf7391"},
    {file = "aiohttp-3.8.4-cp39-cp39-musllinux_1_1_ppc64le.whl", hash = "sha256:8c29c77cc57e40f84acef9bfb904373a4e89a4e8b74e71aa8075c021ec9078c2"},
    {file = "aiohttp-3.8.4-cp39-cp39-musllinux_1_1_s390x.whl", hash = "sha256:03543dcf98a6619254b409be2d22b51f21ec66272be4ebda7b04e6412e4b2e14"},
    {file = "aiohttp-3.8.4-cp39-cp39-musllinux_1_1_x86_64.whl", hash = "sha256:17b79c2963db82086229012cff93ea55196ed31f6493bb1ccd2c62f1724324e4"},
    {file = "aiohttp-3.8.4-cp39-cp39-win32.whl", hash = "sha256:34ce9f93a4a68d1272d26030655dd1b58ff727b3ed2a33d80ec433561b03d67a"},
    {file = "aiohttp-3.8.4-cp39-cp39-win_amd64.whl", hash = "sha256:41a86a69bb63bb2fc3dc9ad5ea9f10f1c9c8e282b471931be0268ddd09430b04"},
    {file = "aiohttp-3.8.4.tar.gz", hash = "sha256:bf2e1a9162c1e441bf805a1fd166e249d574ca04e03b34f97e2928769e91ab5c"},
]

[package.dependencies]
aiosignal = ">=1.1.2"
async-timeout = ">=4.0.0a3,<5.0"
attrs = ">=17.3.0"
charset-normalizer = ">=2.0,<4.0"
frozenlist = ">=1.1.1"
multidict = ">=4.5,<7.0"
yarl = ">=1.0,<2.0"

[package.extras]
speedups = ["Brotli", "aiodns", "cchardet"]

[[package]]
name = "aiosignal"
version = "1.3.1"
description = "aiosignal: a list of registered asynchronous callbacks"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "aiosignal-1.3.1-py3-none-any.whl", hash = "sha256:f8376fb07dd1e86a584e4fcdec80b36b7f81aac666ebc724e2c090300dd83b17"},
    {file = "aiosignal-1.3.1.tar.gz", hash = "sha256:54cd96e15e1649b75d6c87526a6ff0b6c1b0dd3459f43d9ca11d48c339b68cfc"},
]

[package.dependencies]
frozenlist = ">=1.1.0"

[[package]]
name = "anyio"
version = "3.7.0"
description = "High level compatibility layer for multiple asynchronous event loop implementations"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "anyio-3.7.0-py3-none-any.whl", hash = "sha256:eddca883c4175f14df8aedce21054bfca3adb70ffe76a9f607aef9d7fa2ea7f0"},
    {file = "anyio-3.7.0.tar.gz", hash = "sha256:275d9973793619a5374e1c89a4f4ad3f4b0a5510a2b5b939444bee8f4c4d37ce"},
]

[package.dependencies]
exceptiongroup = {version = "*", markers = "python_version < \"3.11\""}
idna = ">=2.8"
sniffio = ">=1.1"

[package.extras]
doc = ["Sphinx (>=6.1.0)", "packaging", "sphinx-autodoc-typehints (>=1.2.0)", "sphinx-rtd-theme", "sphinxcontrib-jquery"]
test = ["anyio[trio]", "coverage[toml] (>=4.5)", "hypothesis (>=4.0)", "mock (>=4)", "psutil (>=5.9)", "pytest (>=7.0)", "pytest-mock (>=3.6.1)", "trustme", "uvloop (>=0.17)"]
trio = ["trio (<0.22)"]

[[package]]
name = "arrow"
version = "1.2.3"
description = "Better dates & times for Python"
category = "main"
optional = false
python-versions = ">=3.6"
files = [
    {file = "arrow-1.2.3-py3-none-any.whl", hash = "sha256:5a49ab92e3b7b71d96cd6bfcc4df14efefc9dfa96ea19045815914a6ab6b1fe2"},
    {file = "arrow-1.2.3.tar.gz", hash = "sha256:3934b30ca1b9f292376d9db15b19446088d12ec58629bc3f0da28fd55fb633a1"},
]

[package.dependencies]
python-dateutil = ">=2.7.0"

[[package]]
name = "async-timeout"
version = "4.0.2"
description = "Timeout context manager for asyncio programs"
category = "main"
optional = false
python-versions = ">=3.6"
files = [
    {file = "async-timeout-4.0.2.tar.gz", hash = "sha256:2163e1640ddb52b7a8c80d0a67a08587e5d245cc9c553a74a847056bc2976b15"},
    {file = "async_timeout-4.0.2-py3-none-any.whl", hash = "sha256:8ca1e4fcf50d07413d66d1a5e416e42cfdf5851c981d679a09851a6853383b3c"},
]

[[package]]
name = "attrs"
version = "23.1.0"
description = "Classes Without Boilerplate"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "attrs-23.1.0-py3-none-any.whl", hash = "sha256:1f28b4522cdc2fb4256ac1a020c78acf9cba2c6b461ccd2c126f3aa8e8335d04"},
    {file = "attrs-23.1.0.tar.gz", hash = "sha256:6279836d581513a26f1bf235f9acd333bc9115683f14f7e8fae46c98fc50e015"},
]

[package.extras]
cov = ["attrs[tests]", "coverage[toml] (>=5.3)"]
dev = ["attrs[docs,tests]", "pre-commit"]
docs = ["furo", "myst-parser", "sphinx", "sphinx-notfound-page", "sphinxcontrib-towncrier", "towncrier", "zope-interface"]
tests = ["attrs[tests-no-zope]", "zope-interface"]
tests-no-zope = ["cloudpickle", "hypothesis", "mypy (>=1.1.1)", "pympler", "pytest (>=4.3.0)", "pytest-mypy-plugins", "pytest-xdist[psutil]"]

[[package]]
name = "authlib"
version = "1.2.0"
description = "The ultimate Python library in building OAuth and OpenID Connect servers and clients."
category = "main"
optional = false
python-versions = "*"
files = [
    {file = "Authlib-1.2.0-py2.py3-none-any.whl", hash = "sha256:4ddf4fd6cfa75c9a460b361d4bd9dac71ffda0be879dbe4292a02e92349ad55a"},
    {file = "Authlib-1.2.0.tar.gz", hash = "sha256:4fa3e80883a5915ef9f5bc28630564bc4ed5b5af39812a3ff130ec76bd631e9d"},
]

[package.dependencies]
cryptography = ">=3.2"

[[package]]
name = "azure-common"
version = "1.1.28"
description = "Microsoft Azure Client Library for Python (Common)"
category = "main"
optional = false
python-versions = "*"
files = [
    {file = "azure-common-1.1.28.zip", hash = "sha256:4ac0cd3214e36b6a1b6a442686722a5d8cc449603aa833f3f0f40bda836704a3"},
    {file = "azure_common-1.1.28-py2.py3-none-any.whl", hash = "sha256:5c12d3dcf4ec20599ca6b0d3e09e86e146353d443e7fcc050c9a19c1f9df20ad"},
]

[[package]]
name = "azure-core"
version = "1.26.4"
description = "Microsoft Azure Core Library for Python"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "azure-core-1.26.4.zip", hash = "sha256:075fe06b74c3007950dd93d49440c2f3430fd9b4a5a2756ec8c79454afc989c6"},
    {file = "azure_core-1.26.4-py3-none-any.whl", hash = "sha256:d9664b4bc2675d72fba461a285ac43ae33abb2967014a955bf136d9703a2ab3c"},
]

[package.dependencies]
requests = ">=2.18.4"
six = ">=1.11.0"
typing-extensions = ">=4.3.0"

[package.extras]
aio = ["aiohttp (>=3.0)"]

[[package]]
name = "azure-identity"
version = "1.13.0"
description = "Microsoft Azure Identity Library for Python"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "azure-identity-1.13.0.zip", hash = "sha256:c931c27301ffa86b07b4dcf574e29da73e3deba9ab5d1fe4f445bb6a3117e260"},
    {file = "azure_identity-1.13.0-py3-none-any.whl", hash = "sha256:bd700cebb80cd9862098587c29d8677e819beca33c62568ced6d5a8e5e332b82"},
]

[package.dependencies]
azure-core = ">=1.11.0,<2.0.0"
cryptography = ">=2.5"
msal = ">=1.20.0,<2.0.0"
msal-extensions = ">=0.3.0,<2.0.0"
six = ">=1.12.0"

[[package]]
name = "azure-search-documents"
version = "11.4.0b8"
description = "Microsoft Azure Cognitive Search Client Library for Python"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "azure-search-documents-11.4.0b8.zip", hash = "sha256:b178ff52918590191a9cb7f411a9ab3cb517663666a501a3e84b715d19b0d93b"},
    {file = "azure_search_documents-11.4.0b8-py3-none-any.whl", hash = "sha256:4137daa2db75bff9484d394c16c0604822a51281cad2f50e11d7c48dd8d4b4cf"},
]

[package.dependencies]
azure-common = ">=1.1,<2.0"
azure-core = ">=1.24.0,<2.0.0"
isodate = ">=0.6.0"

[[package]]
name = "backoff"
version = "2.2.1"
description = "Function decoration for backoff and retry"
category = "main"
optional = false
python-versions = ">=3.7,<4.0"
files = [
    {file = "backoff-2.2.1-py3-none-any.whl", hash = "sha256:63579f9a0628e06278f7e47b7d7d5b6ce20dc65c5e96a6f3ca99a6adca0396e8"},
    {file = "backoff-2.2.1.tar.gz", hash = "sha256:03f829f5bb1923180821643f8753b0502c3b682293992485b0eef2807afa5cba"},
]

[[package]]
name = "bleach"
version = "6.0.0"
description = "An easy safelist-based HTML-sanitizing tool."
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "bleach-6.0.0-py3-none-any.whl", hash = "sha256:33c16e3353dbd13028ab4799a0f89a83f113405c766e9c122df8a06f5b85b3f4"},
    {file = "bleach-6.0.0.tar.gz", hash = "sha256:1a1a85c1595e07d8db14c5f09f09e6433502c51c595970edc090551f0db99414"},
]

[package.dependencies]
six = ">=1.9.0"
webencodings = "*"

[package.extras]
css = ["tinycss2 (>=1.1.0,<1.2)"]

[[package]]
name = "blobfile"
version = "2.0.2"
description = "Read GCS, ABS and local paths with the same interface, clone of tensorflow.io.gfile"
category = "main"
optional = false
python-versions = ">=3.7.0"
files = [
    {file = "blobfile-2.0.2-py3-none-any.whl", hash = "sha256:c48afb61d14d6f94b0c109aa35475bc8d586e5eecde9a25dc2c9c52e7dd4feaf"},
]

[package.dependencies]
filelock = ">=3.0,<4.0"
lxml = ">=4.9,<5.0"
pycryptodomex = ">=3.8,<4.0"
urllib3 = ">=1.25.3,<3"

[[package]]
name = "certifi"
version = "2023.5.7"
description = "Python package for providing Mozilla's CA Bundle."
category = "main"
optional = false
python-versions = ">=3.6"
files = [
    {file = "certifi-2023.5.7-py3-none-any.whl", hash = "sha256:c6c2e98f5c7869efca1f8916fed228dd91539f9f1b444c314c06eef02980c716"},
    {file = "certifi-2023.5.7.tar.gz", hash = "sha256:0f0d56dc5a6ad56fd4ba36484d6cc34451e1c6548c61daad8c320169f91eddc7"},
]

[[package]]
name = "cffi"
version = "1.15.1"
description = "Foreign Function Interface for Python calling C code."
category = "main"
optional = false
python-versions = "*"
files = [
    {file = "cffi-1.15.1-cp27-cp27m-macosx_10_9_x86_64.whl", hash = "sha256:a66d3508133af6e8548451b25058d5812812ec3798c886bf38ed24a98216fab2"},
    {file = "cffi-1.15.1-cp27-cp27m-manylinux1_i686.whl", hash = "sha256:470c103ae716238bbe698d67ad020e1db9d9dba34fa5a899b5e21577e6d52ed2"},
    {file = "cffi-1.15.1-cp27-cp27m-manylinux1_x86_64.whl", hash = "sha256:9ad5db27f9cabae298d151c85cf2bad1d359a1b9c686a275df03385758e2f914"},
    {file = "cffi-1.15.1-cp27-cp27m-win32.whl", hash = "sha256:b3bbeb01c2b273cca1e1e0c5df57f12dce9a4dd331b4fa1635b8bec26350bde3"},
    {file = "cffi-1.15.1-cp27-cp27m-win_amd64.whl", hash = "sha256:e00b098126fd45523dd056d2efba6c5a63b71ffe9f2bbe1a4fe1716e1d0c331e"},
    {file = "cffi-1.15.1-cp27-cp27mu-manylinux1_i686.whl", hash = "sha256:d61f4695e6c866a23a21acab0509af1cdfd2c013cf256bbf5b6b5e2695827162"},
    {file = "cffi-1.15.1-cp27-cp27mu-manylinux1_x86_64.whl", hash = "sha256:ed9cb427ba5504c1dc15ede7d516b84757c3e3d7868ccc85121d9310d27eed0b"},
    {file = "cffi-1.15.1-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:39d39875251ca8f612b6f33e6b1195af86d1b3e60086068be9cc053aa4376e21"},
    {file = "cffi-1.15.1-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:285d29981935eb726a4399badae8f0ffdff4f5050eaa6d0cfc3f64b857b77185"},
    {file = "cffi-1.15.1-cp310-cp310-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:3eb6971dcff08619f8d91607cfc726518b6fa2a9eba42856be181c6d0d9515fd"},
    {file = "cffi-1.15.1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:21157295583fe8943475029ed5abdcf71eb3911894724e360acff1d61c1d54bc"},
    {file = "cffi-1.15.1-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:5635bd9cb9731e6d4a1132a498dd34f764034a8ce60cef4f5319c0541159392f"},
    {file = "cffi-1.15.1-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:2012c72d854c2d03e45d06ae57f40d78e5770d252f195b93f581acf3ba44496e"},
    {file = "cffi-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:dd86c085fae2efd48ac91dd7ccffcfc0571387fe1193d33b6394db7ef31fe2a4"},
    {file = "cffi-1.15.1-cp310-cp310-musllinux_1_1_i686.whl", hash = "sha256:fa6693661a4c91757f4412306191b6dc88c1703f780c8234035eac011922bc01"},
    {file = "cffi-1.15.1-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:59c0b02d0a6c384d453fece7566d1c7e6b7bae4fc5874ef2ef46d56776d61c9e"},
    {file = "cffi-1.15.1-cp310-cp310-win32.whl", hash = "sha256:cba9d6b9a7d64d4bd46167096fc9d2f835e25d7e4c121fb2ddfc6528fb0413b2"},
    {file = "cffi-1.15.1-cp310-cp310-win_amd64.whl", hash = "sha256:ce4bcc037df4fc5e3d184794f27bdaab018943698f4ca31630bc7f84a7b69c6d"},
    {file = "cffi-1.15.1-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:3d08afd128ddaa624a48cf2b859afef385b720bb4b43df214f85616922e6a5ac"},
    {file = "cffi-1.15.1-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:3799aecf2e17cf585d977b780ce79ff0dc9b78d799fc694221ce814c2c19db83"},
    {file = "cffi-1.15.1-cp311-cp311-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:a591fe9e525846e4d154205572a029f653ada1a78b93697f3b5a8f1f2bc055b9"},
    {file = "cffi-1.15.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:3548db281cd7d2561c9ad9984681c95f7b0e38881201e157833a2342c30d5e8c"},
    {file = "cffi-1.15.1-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:91fc98adde3d7881af9b59ed0294046f3806221863722ba7d8d120c575314325"},
    {file = "cffi-1.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:94411f22c3985acaec6f83c6df553f2dbe17b698cc7f8ae751ff2237d96b9e3c"},
    {file = "cffi-1.15.1-cp311-cp311-musllinux_1_1_i686.whl", hash = "sha256:03425bdae262c76aad70202debd780501fabeaca237cdfddc008987c0e0f59ef"},
    {file = "cffi-1.15.1-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:cc4d65aeeaa04136a12677d3dd0b1c0c94dc43abac5860ab33cceb42b801c1e8"},
    {file = "cffi-1.15.1-cp311-cp311-win32.whl", hash = "sha256:a0f100c8912c114ff53e1202d0078b425bee3649ae34d7b070e9697f93c5d52d"},
    {file = "cffi-1.15.1-cp311-cp311-win_amd64.whl", hash = "sha256:04ed324bda3cda42b9b695d51bb7d54b680b9719cfab04227cdd1e04e5de3104"},
    {file = "cffi-1.15.1-cp36-cp36m-macosx_10_9_x86_64.whl", hash = "sha256:50a74364d85fd319352182ef59c5c790484a336f6db772c1a9231f1c3ed0cbd7"},
    {file = "cffi-1.15.1-cp36-cp36m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:e263d77ee3dd201c3a142934a086a4450861778baaeeb45db4591ef65550b0a6"},
    {file = "cffi-1.15.1-cp36-cp36m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:cec7d9412a9102bdc577382c3929b337320c4c4c4849f2c5cdd14d7368c5562d"},
    {file = "cffi-1.15.1-cp36-cp36m-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:4289fc34b2f5316fbb762d75362931e351941fa95fa18789191b33fc4cf9504a"},
    {file = "cffi-1.15.1-cp36-cp36m-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:173379135477dc8cac4bc58f45db08ab45d228b3363adb7af79436135d028405"},
    {file = "cffi-1.15.1-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.whl", hash = "sha256:6975a3fac6bc83c4a65c9f9fcab9e47019a11d3d2cf7f3c0d03431bf145a941e"},
    {file = "cffi-1.15.1-cp36-cp36m-win32.whl", hash = "sha256:2470043b93ff09bf8fb1d46d1cb756ce6132c54826661a32d4e4d132e1977adf"},
    {file = "cffi-1.15.1-cp36-cp36m-win_amd64.whl", hash = "sha256:30d78fbc8ebf9c92c9b7823ee18eb92f2e6ef79b45ac84db507f52fbe3ec4497"},
    {file = "cffi-1.15.1-cp37-cp37m-macosx_10_9_x86_64.whl", hash = "sha256:198caafb44239b60e252492445da556afafc7d1e3ab7a1fb3f0584ef6d742375"},
    {file = "cffi-1.15.1-cp37-cp37m-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:5ef34d190326c3b1f822a5b7a45f6c4535e2f47ed06fec77d3d799c450b2651e"},
    {file = "cffi-1.15.1-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:8102eaf27e1e448db915d08afa8b41d6c7ca7a04b7d73af6514df10a3e74bd82"},
    {file = "cffi-1.15.1-cp37-cp37m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:5df2768244d19ab7f60546d0c7c63ce1581f7af8b5de3eb3004b9b6fc8a9f84b"},
    {file = "cffi-1.15.1-cp37-cp37m-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:a8c4917bd7ad33e8eb21e9a5bbba979b49d9a97acb3a803092cbc1133e20343c"},
    {file = "cffi-1.15.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:0e2642fe3142e4cc4af0799748233ad6da94c62a8bec3a6648bf8ee68b1c7426"},
    {file = "cffi-1.15.1-cp37-cp37m-win32.whl", hash = "sha256:e229a521186c75c8ad9490854fd8bbdd9a0c9aa3a524326b55be83b54d4e0ad9"},
    {file = "cffi-1.15.1-cp37-cp37m-win_amd64.whl", hash = "sha256:a0b71b1b8fbf2b96e41c4d990244165e2c9be83d54962a9a1d118fd8657d2045"},
    {file = "cffi-1.15.1-cp38-cp38-macosx_10_9_x86_64.whl", hash = "sha256:320dab6e7cb2eacdf0e658569d2575c4dad258c0fcc794f46215e1e39f90f2c3"},
    {file = "cffi-1.15.1-cp38-cp38-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:1e74c6b51a9ed6589199c787bf5f9875612ca4a8a0785fb2d4a84429badaf22a"},
    {file = "cffi-1.15.1-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:a5c84c68147988265e60416b57fc83425a78058853509c1b0629c180094904a5"},
    {file = "cffi-1.15.1-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:3b926aa83d1edb5aa5b427b4053dc420ec295a08e40911296b9eb1b6170f6cca"},
    {file = "cffi-1.15.1-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:87c450779d0914f2861b8526e035c5e6da0a3199d8f1add1a665e1cbc6fc6d02"},
    {file = "cffi-1.15.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:4f2c9f67e9821cad2e5f480bc8d83b8742896f1242dba247911072d4fa94c192"},
    {file = "cffi-1.15.1-cp38-cp38-win32.whl", hash = "sha256:8b7ee99e510d7b66cdb6c593f21c043c248537a32e0bedf02e01e9553a172314"},
    {file = "cffi-1.15.1-cp38-cp38-win_amd64.whl", hash = "sha256:00a9ed42e88df81ffae7a8ab6d9356b371399b91dbdf0c3cb1e84c03a13aceb5"},
    {file = "cffi-1.15.1-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:54a2db7b78338edd780e7ef7f9f6c442500fb0d41a5a4ea24fff1c929d5af585"},
    {file = "cffi-1.15.1-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:fcd131dd944808b5bdb38e6f5b53013c5aa4f334c5cad0c72742f6eba4b73db0"},
    {file = "cffi-1.15.1-cp39-cp39-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:7473e861101c9e72452f9bf8acb984947aa1661a7704553a9f6e4baa5ba64415"},
    {file = "cffi-1.15.1-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:6c9a799e985904922a4d207a94eae35c78ebae90e128f0c4e521ce339396be9d"},
    {file = "cffi-1.15.1-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:3bcde07039e586f91b45c88f8583ea7cf7a0770df3a1649627bf598332cb6984"},
    {file = "cffi-1.15.1-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:33ab79603146aace82c2427da5ca6e58f2b3f2fb5da893ceac0c42218a40be35"},
    {file = "cffi-1.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:5d598b938678ebf3c67377cdd45e09d431369c3b1a5b331058c338e201f12b27"},
    {file = "cffi-1.15.1-cp39-cp39-musllinux_1_1_i686.whl", hash = "sha256:db0fbb9c62743ce59a9ff687eb5f4afbe77e5e8403d6697f7446e5f609976f76"},
    {file = "cffi-1.15.1-cp39-cp39-musllinux_1_1_x86_64.whl", hash = "sha256:98d85c6a2bef81588d9227dde12db8a7f47f639f4a17c9ae08e773aa9c697bf3"},
    {file = "cffi-1.15.1-cp39-cp39-win32.whl", hash = "sha256:40f4774f5a9d4f5e344f31a32b5096977b5d48560c5592e2f3d2c4374bd543ee"},
    {file = "cffi-1.15.1-cp39-cp39-win_amd64.whl", hash = "sha256:70df4e3b545a17496c9b3f41f5115e69a4f2e77e94e1d2a8e1070bc0c38c8a3c"},
    {file = "cffi-1.15.1.tar.gz", hash = "sha256:d400bfb9a37b1351253cb402671cea7e89bdecc294e8016a707f6d1d8ac934f9"},
]

[package.dependencies]
pycparser = "*"

[[package]]
name = "charset-normalizer"
version = "3.1.0"
description = "The Real First Universal Charset Detector. Open, modern and actively maintained alternative to Chardet."
category = "main"
optional = false
python-versions = ">=3.7.0"
files = [
    {file = "charset-normalizer-3.1.0.tar.gz", hash = "sha256:34e0a2f9c370eb95597aae63bf85eb5e96826d81e3dcf88b8886012906f509b5"},
    {file = "charset_normalizer-3.1.0-cp310-cp310-macosx_10_9_universal2.whl", hash = "sha256:e0ac8959c929593fee38da1c2b64ee9778733cdf03c482c9ff1d508b6b593b2b"},
    {file = "charset_normalizer-3.1.0-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:d7fc3fca01da18fbabe4625d64bb612b533533ed10045a2ac3dd194bfa656b60"},
    {file = "charset_normalizer-3.1.0-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:04eefcee095f58eaabe6dc3cc2262f3bcd776d2c67005880894f447b3f2cb9c1"},
    {file = "charset_normalizer-3.1.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:20064ead0717cf9a73a6d1e779b23d149b53daf971169289ed2ed43a71e8d3b0"},
    {file = "charset_normalizer-3.1.0-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:1435ae15108b1cb6fffbcea2af3d468683b7afed0169ad718451f8db5d1aff6f"},
    {file = "charset_normalizer-3.1.0-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:c84132a54c750fda57729d1e2599bb598f5fa0344085dbde5003ba429a4798c0"},
    {file = "charset_normalizer-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:75f2568b4189dda1c567339b48cba4ac7384accb9c2a7ed655cd86b04055c795"},
    {file = "charset_normalizer-3.1.0-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:11d3bcb7be35e7b1bba2c23beedac81ee893ac9871d0ba79effc7fc01167db6c"},
    {file = "charset_normalizer-3.1.0-cp310-cp310-musllinux_1_1_aarch64.whl", hash = "sha256:891cf9b48776b5c61c700b55a598621fdb7b1e301a550365571e9624f270c203"},
    {file = "charset_normalizer-3.1.0-cp310-cp310-musllinux_1_1_i686.whl", hash = "sha256:5f008525e02908b20e04707a4f704cd286d94718f48bb33edddc7d7b584dddc1"},
    {file = "charset_normalizer-3.1.0-cp310-cp310-musllinux_1_1_ppc64le.whl", hash = "sha256:b06f0d3bf045158d2fb8837c5785fe9ff9b8c93358be64461a1089f5da983137"},
    {file = "charset_normalizer-3.1.0-cp310-cp310-musllinux_1_1_s390x.whl", hash = "sha256:49919f8400b5e49e961f320c735388ee686a62327e773fa5b3ce6721f7e785ce"},
    {file = "charset_normalizer-3.1.0-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:22908891a380d50738e1f978667536f6c6b526a2064156203d418f4856d6e86a"},
    {file = "charset_normalizer-3.1.0-cp310-cp310-win32.whl", hash = "sha256:12d1a39aa6b8c6f6248bb54550efcc1c38ce0d8096a146638fd4738e42284448"},
    {file = "charset_normalizer-3.1.0-cp310-cp310-win_amd64.whl", hash = "sha256:65ed923f84a6844de5fd29726b888e58c62820e0769b76565480e1fdc3d062f8"},
    {file = "charset_normalizer-3.1.0-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:9a3267620866c9d17b959a84dd0bd2d45719b817245e49371ead79ed4f710d19"},
    {file = "charset_normalizer-3.1.0-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:6734e606355834f13445b6adc38b53c0fd45f1a56a9ba06c2058f86893ae8017"},
    {file = "charset_normalizer-3.1.0-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:f8303414c7b03f794347ad062c0516cee0e15f7a612abd0ce1e25caf6ceb47df"},
    {file = "charset_normalizer-3.1.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:aaf53a6cebad0eae578f062c7d462155eada9c172bd8c4d250b8c1d8eb7f916a"},
    {file = "charset_normalizer-3.1.0-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:3dc5b6a8ecfdc5748a7e429782598e4f17ef378e3e272eeb1340ea57c9109f41"},
    {file = "charset_normalizer-3.1.0-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:e1b25e3ad6c909f398df8921780d6a3d120d8c09466720226fc621605b6f92b1"},
    {file = "charset_normalizer-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:0ca564606d2caafb0abe6d1b5311c2649e8071eb241b2d64e75a0d0065107e62"},
    {file = "charset_normalizer-3.1.0-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:b82fab78e0b1329e183a65260581de4375f619167478dddab510c6c6fb04d9b6"},
    {file = "charset_normalizer-3.1.0-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:bd7163182133c0c7701b25e604cf1611c0d87712e56e88e7ee5d72deab3e76b5"},
    {file = "charset_normalizer-3.1.0-cp311-cp311-musllinux_1_1_i686.whl", hash = "sha256:11d117e6c63e8f495412d37e7dc2e2fff09c34b2d09dbe2bee3c6229577818be"},
    {file = "charset_normalizer-3.1.0-cp311-cp311-musllinux_1_1_ppc64le.whl", hash = "sha256:cf6511efa4801b9b38dc5546d7547d5b5c6ef4b081c60b23e4d941d0eba9cbeb"},
    {file = "charset_normalizer-3.1.0-cp311-cp311-musllinux_1_1_s390x.whl", hash = "sha256:abc1185d79f47c0a7aaf7e2412a0eb2c03b724581139193d2d82b3ad8cbb00ac"},
    {file = "charset_normalizer-3.1.0-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:cb7b2ab0188829593b9de646545175547a70d9a6e2b63bf2cd87a0a391599324"},
    {file = "charset_normalizer-3.1.0-cp311-cp311-win32.whl", hash = "sha256:c36bcbc0d5174a80d6cccf43a0ecaca44e81d25be4b7f90f0ed7bcfbb5a00909"},
    {file = "charset_normalizer-3.1.0-cp311-cp311-win_amd64.whl", hash = "sha256:cca4def576f47a09a943666b8f829606bcb17e2bc2d5911a46c8f8da45f56755"},
    {file = "charset_normalizer-3.1.0-cp37-cp37m-macosx_10_9_x86_64.whl", hash = "sha256:0c95f12b74681e9ae127728f7e5409cbbef9cd914d5896ef238cc779b8152373"},
    {file = "charset_normalizer-3.1.0-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:fca62a8301b605b954ad2e9c3666f9d97f63872aa4efcae5492baca2056b74ab"},
    {file = "charset_normalizer-3.1.0-cp37-cp37m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:ac0aa6cd53ab9a31d397f8303f92c42f534693528fafbdb997c82bae6e477ad9"},
    {file = "charset_normalizer-3.1.0-cp37-cp37m-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:c3af8e0f07399d3176b179f2e2634c3ce9c1301379a6b8c9c9aeecd481da494f"},
    {file = "charset_normalizer-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:3a5fc78f9e3f501a1614a98f7c54d3969f3ad9bba8ba3d9b438c3bc5d047dd28"},
    {file = "charset_normalizer-3.1.0-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:628c985afb2c7d27a4800bfb609e03985aaecb42f955049957814e0491d4006d"},
    {file = "charset_normalizer-3.1.0-cp37-cp37m-musllinux_1_1_aarch64.whl", hash = "sha256:74db0052d985cf37fa111828d0dd230776ac99c740e1a758ad99094be4f1803d"},
    {file = "charset_normalizer-3.1.0-cp37-cp37m-musllinux_1_1_i686.whl", hash = "sha256:1e8fcdd8f672a1c4fc8d0bd3a2b576b152d2a349782d1eb0f6b8e52e9954731d"},
    {file = "charset_normalizer-3.1.0-cp37-cp37m-musllinux_1_1_ppc64le.whl", hash = "sha256:04afa6387e2b282cf78ff3dbce20f0cc071c12dc8f685bd40960cc68644cfea6"},
    {file = "charset_normalizer-3.1.0-cp37-cp37m-musllinux_1_1_s390x.whl", hash = "sha256:dd5653e67b149503c68c4018bf07e42eeed6b4e956b24c00ccdf93ac79cdff84"},
    {file = "charset_normalizer-3.1.0-cp37-cp37m-musllinux_1_1_x86_64.whl", hash = "sha256:d2686f91611f9e17f4548dbf050e75b079bbc2a82be565832bc8ea9047b61c8c"},
    {file = "charset_normalizer-3.1.0-cp37-cp37m-win32.whl", hash = "sha256:4155b51ae05ed47199dc5b2a4e62abccb274cee6b01da5b895099b61b1982974"},
    {file = "charset_normalizer-3.1.0-cp37-cp37m-win_amd64.whl", hash = "sha256:322102cdf1ab682ecc7d9b1c5eed4ec59657a65e1c146a0da342b78f4112db23"},
    {file = "charset_normalizer-3.1.0-cp38-cp38-macosx_10_9_universal2.whl", hash = "sha256:e633940f28c1e913615fd624fcdd72fdba807bf53ea6925d6a588e84e1151531"},
    {file = "charset_normalizer-3.1.0-cp38-cp38-macosx_10_9_x86_64.whl", hash = "sha256:3a06f32c9634a8705f4ca9946d667609f52cf130d5548881401f1eb2c39b1e2c"},
    {file = "charset_normalizer-3.1.0-cp38-cp38-macosx_11_0_arm64.whl", hash = "sha256:7381c66e0561c5757ffe616af869b916c8b4e42b367ab29fedc98481d1e74e14"},
    {file = "charset_normalizer-3.1.0-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:3573d376454d956553c356df45bb824262c397c6e26ce43e8203c4c540ee0acb"},
    {file = "charset_normalizer-3.1.0-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:e89df2958e5159b811af9ff0f92614dabf4ff617c03a4c1c6ff53bf1c399e0e1"},
    {file = "charset_normalizer-3.1.0-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:78cacd03e79d009d95635e7d6ff12c21eb89b894c354bd2b2ed0b4763373693b"},
    {file = "charset_normalizer-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:de5695a6f1d8340b12a5d6d4484290ee74d61e467c39ff03b39e30df62cf83a0"},
    {file = "charset_normalizer-3.1.0-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:1c60b9c202d00052183c9be85e5eaf18a4ada0a47d188a83c8f5c5b23252f649"},
    {file = "charset_normalizer-3.1.0-cp38-cp38-musllinux_1_1_aarch64.whl", hash = "sha256:f645caaf0008bacf349875a974220f1f1da349c5dbe7c4ec93048cdc785a3326"},
    {file = "charset_normalizer-3.1.0-cp38-cp38-musllinux_1_1_i686.whl", hash = "sha256:ea9f9c6034ea2d93d9147818f17c2a0860d41b71c38b9ce4d55f21b6f9165a11"},
    {file = "charset_normalizer-3.1.0-cp38-cp38-musllinux_1_1_ppc64le.whl", hash = "sha256:80d1543d58bd3d6c271b66abf454d437a438dff01c3e62fdbcd68f2a11310d4b"},
    {file = "charset_normalizer-3.1.0-cp38-cp38-musllinux_1_1_s390x.whl", hash = "sha256:73dc03a6a7e30b7edc5b01b601e53e7fc924b04e1835e8e407c12c037e81adbd"},
    {file = "charset_normalizer-3.1.0-cp38-cp38-musllinux_1_1_x86_64.whl", hash = "sha256:6f5c2e7bc8a4bf7c426599765b1bd33217ec84023033672c1e9a8b35eaeaaaf8"},
    {file = "charset_normalizer-3.1.0-cp38-cp38-win32.whl", hash = "sha256:12a2b561af122e3d94cdb97fe6fb2bb2b82cef0cdca131646fdb940a1eda04f0"},
    {file = "charset_normalizer-3.1.0-cp38-cp38-win_amd64.whl", hash = "sha256:3160a0fd9754aab7d47f95a6b63ab355388d890163eb03b2d2b87ab0a30cfa59"},
    {file = "charset_normalizer-3.1.0-cp39-cp39-macosx_10_9_universal2.whl", hash = "sha256:38e812a197bf8e71a59fe55b757a84c1f946d0ac114acafaafaf21667a7e169e"},
    {file = "charset_normalizer-3.1.0-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:6baf0baf0d5d265fa7944feb9f7451cc316bfe30e8df1a61b1bb08577c554f31"},
    {file = "charset_normalizer-3.1.0-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:8f25e17ab3039b05f762b0a55ae0b3632b2e073d9c8fc88e89aca31a6198e88f"},
    {file = "charset_normalizer-3.1.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:3747443b6a904001473370d7810aa19c3a180ccd52a7157aacc264a5ac79265e"},
    {file = "charset_normalizer-3.1.0-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:b116502087ce8a6b7a5f1814568ccbd0e9f6cfd99948aa59b0e241dc57cf739f"},
    {file = "charset_normalizer-3.1.0-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:d16fd5252f883eb074ca55cb622bc0bee49b979ae4e8639fff6ca3ff44f9f854"},
    {file = "charset_normalizer-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:21fa558996782fc226b529fdd2ed7866c2c6ec91cee82735c98a197fae39f706"},
    {file = "charset_normalizer-3.1.0-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:6f6c7a8a57e9405cad7485f4c9d3172ae486cfef1344b5ddd8e5239582d7355e"},
    {file = "charset_normalizer-3.1.0-cp39-cp39-musllinux_1_1_aarch64.whl", hash = "sha256:ac3775e3311661d4adace3697a52ac0bab17edd166087d493b52d4f4f553f9f0"},
    {file = "charset_normalizer-3.1.0-cp39-cp39-musllinux_1_1_i686.whl", hash = "sha256:10c93628d7497c81686e8e5e557aafa78f230cd9e77dd0c40032ef90c18f2230"},
    {file = "charset_normalizer-3.1.0-cp39-cp39-musllinux_1_1_ppc64le.whl", hash = "sha256:6f4f4668e1831850ebcc2fd0b1cd11721947b6dc7c00bf1c6bd3c929ae14f2c7"},
    {file = "charset_normalizer-3.1.0-cp39-cp39-musllinux_1_1_s390x.whl", hash = "sha256:0be65ccf618c1e7ac9b849c315cc2e8a8751d9cfdaa43027d4f6624bd587ab7e"},
    {file = "charset_normalizer-3.1.0-cp39-cp39-musllinux_1_1_x86_64.whl", hash = "sha256:53d0a3fa5f8af98a1e261de6a3943ca631c526635eb5817a87a59d9a57ebf48f"},
    {file = "charset_normalizer-3.1.0-cp39-cp39-win32.whl", hash = "sha256:a04f86f41a8916fe45ac5024ec477f41f886b3c435da2d4e3d2709b22ab02af1"},
    {file = "charset_normalizer-3.1.0-cp39-cp39-win_amd64.whl", hash = "sha256:830d2948a5ec37c386d3170c483063798d7879037492540f10a475e3fd6f244b"},
    {file = "charset_normalizer-3.1.0-py3-none-any.whl", hash = "sha256:3d9098b479e78c85080c98e1e35ff40b4a31d8953102bb0fd7d1b6f8a2111a3d"},
]

[[package]]
name = "chromadb"
version = "0.3.25"
description = "Chroma."
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "chromadb-0.3.25-py3-none-any.whl", hash = "sha256:81afe1850de5b5621570bb3f9d521f42c545091d53d650f8b94a55cd9b69f768"},
    {file = "chromadb-0.3.25.tar.gz", hash = "sha256:78f35c65de7c21622dfc0fe02b6db6b59ba8f04fb8b8cd7b5e5f8f68f9806e8a"},
]

[package.dependencies]
clickhouse-connect = ">=0.5.7"
duckdb = ">=0.7.1"
fastapi = ">=0.85.1"
hnswlib = ">=0.7"
numpy = ">=1.21.6"
onnxruntime = ">=1.14.1"
overrides = ">=7.3.1"
pandas = ">=1.3"
posthog = ">=2.4.0"
pydantic = ">=1.9"
requests = ">=2.28"
tokenizers = ">=0.13.2"
tqdm = ">=4.65.0"
typing-extensions = ">=4.5.0"
uvicorn = {version = ">=0.18.3", extras = ["standard"]}

[[package]]
name = "click"
version = "8.1.3"
description = "Composable command line interface toolkit"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "click-8.1.3-py3-none-any.whl", hash = "sha256:bb4d8133cb15a609f44e8213d9b391b0809795062913b383c62be0ee95b1db48"},
    {file = "click-8.1.3.tar.gz", hash = "sha256:7682dc8afb30297001674575ea00d1814d808d6a36af415a82bd481d37ba7b8e"},
]

[package.dependencies]
colorama = {version = "*", markers = "platform_system == \"Windows\""}

[[package]]
name = "click-log"
version = "0.4.0"
description = "Logging integration for Click"
category = "main"
optional = false
python-versions = "*"
files = [
    {file = "click-log-0.4.0.tar.gz", hash = "sha256:3970f8570ac54491237bcdb3d8ab5e3eef6c057df29f8c3d1151a51a9c23b975"},
    {file = "click_log-0.4.0-py2.py3-none-any.whl", hash = "sha256:a43e394b528d52112af599f2fc9e4b7cf3c15f94e53581f74fa6867e68c91756"},
]

[package.dependencies]
click = "*"

[[package]]
name = "clickhouse-connect"
version = "0.5.25"
description = "ClickHouse core driver, SqlAlchemy, and Superset libraries"
category = "main"
optional = false
python-versions = "~=3.7"
files = [
    {file = "clickhouse-connect-0.5.25.tar.gz", hash = "sha256:98af3fff571d1069d2c6dd2f4c0feb220fe4c55bd12608e841c842582061982f"},
    {file = "clickhouse_connect-0.5.25-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:7c8ec7ca17efe105211e7b1271b49e0f6c3c56846488a14a866712ce497ef5a5"},
    {file = "clickhouse_connect-0.5.25-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:da4153d44461535b31f5bf25b79504ba4afa1ed1f03b50fbfc595e34b2b3d2f2"},
    {file = "clickhouse_connect-0.5.25-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0cb5ea544cbabefa99ac588e6a452be6b9e896506b306ebc7a4b073fb3237e6f"},
    {file = "clickhouse_connect-0.5.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:0623d7086710f5c9d04327da1a791ffbf519c0f54b25e3584b6eb88f5496c06b"},
    {file = "clickhouse_connect-0.5.25-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:243edb0e30fb52e1e9f137519d342e09e5b804e2e4d1b5d9eea6f90875bd8abe"},
    {file = "clickhouse_connect-0.5.25-cp310-cp310-musllinux_1_1_aarch64.whl", hash = "sha256:182d5f28a24e2d91921dff6d6fedb51f3622088f340847e46ded93c23b10d8c5"},
    {file = "clickhouse_connect-0.5.25-cp310-cp310-musllinux_1_1_i686.whl", hash = "sha256:f6d4d536daf5c0730350cfe1c51dbf0379d07c8272ae288b82fe9a9c47978879"},
    {file = "clickhouse_connect-0.5.25-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:8918fcd0d14b1ea7d8159a0cef815ec707ec039689f4d4db956b8f4627a48aea"},
    {file = "clickhouse_connect-0.5.25-cp310-cp310-win32.whl", hash = "sha256:62819da829bdce30fac58f2266a134b50983f2a9f5808acdde70b0d59e3ed1e1"},
    {file = "clickhouse_connect-0.5.25-cp310-cp310-win_amd64.whl", hash = "sha256:219501ab1180475cbb5fbe604344fd13650507e0bc2618a876f209903dd6738d"},
    {file = "clickhouse_connect-0.5.25-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:ffe69f2242398845111987672552e2af76a13c0770ce00f82ce84d52f5dd5391"},
    {file = "clickhouse_connect-0.5.25-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:c701cfc1ae4c9f32aefc9b331224b232b01178ec5692297a827563012b29e2bc"},
    {file = "clickhouse_connect-0.5.25-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:4d8ecae52e5f4d93b7460fb66c61108b77afc28a39bdd6c31dded22865584ec3"},
    {file = "clickhouse_connect-0.5.25-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:95340f5054a697b36fc9d32f34516583d9a1d4b9c6784860a7454f7d27802d4e"},
    {file = "clickhouse_connect-0.5.25-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:657aa2624c532dcc465ef517880823d9c4f2732e792ff51bb306cee1abc4c6a6"},
    {file = "clickhouse_connect-0.5.25-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:964cae0de1badc2430927398f172da70c6f322266f8ae2509e7cf83f305a38f5"},
    {file = "clickhouse_connect-0.5.25-cp311-cp311-musllinux_1_1_i686.whl", hash = "sha256:0cf33dcb201500cce86c9550f55e0505fa22567ce5314aca01037cf88d139b21"},
    {file = "clickhouse_connect-0.5.25-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:ff5e4f2098b5363116ec99c79a8f78a81af95eb80086c3df86713dcebb47a36c"},
    {file = "clickhouse_connect-0.5.25-cp311-cp311-win32.whl", hash = "sha256:4792f129593f931609e623c64627b2a6b265fc55083e121e3c4cc800ea65bbb3"},
    {file = "clickhouse_connect-0.5.25-cp311-cp311-win_amd64.whl", hash = "sha256:e69421e03ac40c8a5c9f70aca110945b0a7e33843dc415f2305142db9b819941"},
    {file = "clickhouse_connect-0.5.25-cp37-cp37m-macosx_10_9_x86_64.whl", hash = "sha256:e1c285c3452564e99098cce044ef7f6e2041f70f5557022d0f07886d0c17284a"},
    {file = "clickhouse_connect-0.5.25-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:f501c4f8d92625b0028f828a317beda621bbd6fd26bddada756f2971b0808618"},
    {file = "clickhouse_connect-0.5.25-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:0d2bc8636e262a83f9ee8faf0de6562f463f6b431c6a543be6628006640c0065"},
    {file = "clickhouse_connect-0.5.25-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:b853b146b3557484946c93b4af22834c83ad30908850dc418dd6085b9367bf59"},
    {file = "clickhouse_connect-0.5.25-cp37-cp37m-musllinux_1_1_aarch64.whl", hash = "sha256:4851bb77eba7bbf494b3ee16f71a63cb890947ceddd3d71c2cf5a6635d482987"},
    {file = "clickhouse_connect-0.5.25-cp37-cp37m-musllinux_1_1_i686.whl", hash = "sha256:75884207d0e09a9018be29ebe38c0e26be8d0ba96053cc181ee85c15b4ccd18d"},
    {file = "clickhouse_connect-0.5.25-cp37-cp37m-musllinux_1_1_x86_64.whl", hash = "sha256:3d435e98126754ba82e23d20154ff427117227915ee84c7fea43a8d4444daed2"},
    {file = "clickhouse_connect-0.5.25-cp37-cp37m-win32.whl", hash = "sha256:d900614c8a85c635b45c30d5de37d287cd0b20e44ef1f7f4b83b392bc82696c7"},
    {file = "clickhouse_connect-0.5.25-cp37-cp37m-win_amd64.whl", hash = "sha256:1c47b203278df80ebd3eccb9087194f35dd666c2d19bca8148dc70d80b94502b"},
    {file = "clickhouse_connect-0.5.25-cp38-cp38-macosx_10_9_x86_64.whl", hash = "sha256:6d60ec792e72b26184082ec86d4a32d1503acd6725b02bcb56c2980340129837"},
    {file = "clickhouse_connect-0.5.25-cp38-cp38-macosx_11_0_arm64.whl", hash = "sha256:e050e67b9b2ce12ec3e7ce5c27d772e54d06dab578393c0760fd2fd8ea9eae57"},
    {file = "clickhouse_connect-0.5.25-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:e78c9b398abc683f003ed5d3013f2b35d692b8d1a9f1a40dc41fc9fa29304b58"},
    {file = "clickhouse_connect-0.5.25-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:805a0494c8a3f4b37f38b33bdf6daeb43ea4165c3d5916e0467a4811f7a1efc6"},
    {file = "clickhouse_connect-0.5.25-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:62e2da6c3971bc3cfec3dc0430318f38d061ffbdd4eb122e616a2f1aafc6e5c9"},
    {file = "clickhouse_connect-0.5.25-cp38-cp38-musllinux_1_1_aarch64.whl", hash = "sha256:b7dbccfc5a7b238c7e927fb159b95f8ab2970ca0fd4ea39c813be4d10d2799cd"},
    {file = "clickhouse_connect-0.5.25-cp38-cp38-musllinux_1_1_i686.whl", hash = "sha256:6b9e4d0367f99471e865af55865fe300ccbf6e1d9fa070e1e0048c0f33d1ac2c"},
    {file = "clickhouse_connect-0.5.25-cp38-cp38-musllinux_1_1_x86_64.whl", hash = "sha256:f5bf565a45fb52c5b7e9a96913cda4012c1de1407bbab165378e32c6c946bf0d"},
    {file = "clickhouse_connect-0.5.25-cp38-cp38-win32.whl", hash = "sha256:a08ead36c61ac28ce44a0f202acbd594e818be7640d6c972a33a1ebae72e6770"},
    {file = "clickhouse_connect-0.5.25-cp38-cp38-win_amd64.whl", hash = "sha256:0db04a7433d1616f88eaa33b5c5884f7d367d087774a058712a2a6075ac1b4fb"},
    {file = "clickhouse_connect-0.5.25-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:dd7ffbf42a97a4344b82b934d27749fd8296bb18b29a295c249b5d9a774ad122"},
    {file = "clickhouse_connect-0.5.25-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:b51c174621c1de9ae03acd31fbd258e51a1760ae39b4c9ffbaec4a38e19e1545"},
    {file = "clickhouse_connect-0.5.25-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:a59d3d84fccfb831b19a96503fcaf1f6387b49f561d38bf3549fe917a372cc68"},
    {file = "clickhouse_connect-0.5.25-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8335abab878fb166494bb9e68bb7d14a7325f96fb656d3f77d7a23668fb67a2f"},
    {file = "clickhouse_connect-0.5.25-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:a0a53d30cc50350efd82ad492a5b5597dedd6b79b19cfd2fe4331eac756f4aeb"},
    {file = "clickhouse_connect-0.5.25-cp39-cp39-musllinux_1_1_aarch64.whl", hash = "sha256:234579a907e43522c08f1ab9a199d44f7177d7a3755a43669143b237daa026a1"},
    {file = "clickhouse_connect-0.5.25-cp39-cp39-musllinux_1_1_i686.whl", hash = "sha256:93486291893605a0c8884db98e6306f61720fdbe4b1bed5b57cc0daa69cb18c9"},
    {file = "clickhouse_connect-0.5.25-cp39-cp39-musllinux_1_1_x86_64.whl", hash = "sha256:538025c3e10a387c0fe41b687c3421b98070a7f07d07ca88d7cc0d8aed7650f1"},
    {file = "clickhouse_connect-0.5.25-cp39-cp39-win32.whl", hash = "sha256:38051bf7bd6003c0763561214530eef49dc194b062d6bf7faca708f42a5dbf63"},
    {file = "clickhouse_connect-0.5.25-cp39-cp39-win_amd64.whl", hash = "sha256:3618a75a1f2c286e808b1d003ee3956bbf2a762ed36fee5f2a3e2e2096fb37ba"},
    {file = "clickhouse_connect-0.5.25-pp37-pypy37_pp73-macosx_10_9_x86_64.whl", hash = "sha256:2170ba71ad154e9af1f09efd6acaf257b8c1346aeaaf57ae9cac7aa5778bff2c"},
    {file = "clickhouse_connect-0.5.25-pp37-pypy37_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:c846104f4e399e50008324c6ae66c3ef45ac4137a67ccaacdd3afe7f2667b05a"},
    {file = "clickhouse_connect-0.5.25-pp37-pypy37_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:697e7c6ea239666aa2695a03787e4fff0c18cb829eb50086f929cf22cc455c7a"},
    {file = "clickhouse_connect-0.5.25-pp37-pypy37_pp73-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:7b093fc58104b1afe5f968394acaa9b665746564e1ed0c7d8ee5aea7a7a2331b"},
    {file = "clickhouse_connect-0.5.25-pp37-pypy37_pp73-win_amd64.whl", hash = "sha256:4b80e15bff634c4d6a895316b73843f41208d9e22e7e0039e417c79ead5ec906"},
    {file = "clickhouse_connect-0.5.25-pp38-pypy38_pp73-macosx_10_9_x86_64.whl", hash = "sha256:ce9cd94d0d9e022e71cd121555f07c28ad2dbda431e1caf2174ce89a9d792151"},
    {file = "clickhouse_connect-0.5.25-pp38-pypy38_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:9fec43568dd0424deb9dcc74a804addd91f7119367a4ae77796c59656ba22be9"},
    {file = "clickhouse_connect-0.5.25-pp38-pypy38_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:5c5bdc7895b05f0a64956b6b4a29da3882a9b805e1d9e0025a061c46791674f3"},
    {file = "clickhouse_connect-0.5.25-pp38-pypy38_pp73-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:00b54674d9af736f828c438776b7f11e80aecc95a3b763971df20d1537942408"},
    {file = "clickhouse_connect-0.5.25-pp38-pypy38_pp73-win_amd64.whl", hash = "sha256:401aa544255f15d6350934db59e6e0b9f1ddc866ccea41803973579725223aea"},
    {file = "clickhouse_connect-0.5.25-pp39-pypy39_pp73-macosx_10_9_x86_64.whl", hash = "sha256:c0a91e08f8563b8710b03c4a3696ba91fa3b0e475aa964a3169f201243f45d76"},
    {file = "clickhouse_connect-0.5.25-pp39-pypy39_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:af191fc8ecaa544e065257c99cd1d7f49d62c191d23adb78fd34182525ea2f8f"},
    {file = "clickhouse_connect-0.5.25-pp39-pypy39_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:ef6cb5437c18e7588d6c3d7f4df6c8cdd883c30f82f8ec4f199cdcea63d189e4"},
    {file = "clickhouse_connect-0.5.25-pp39-pypy39_pp73-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:d5f84058209fbab8119835d9dd475ca4c3a246263d1965f0e7c624bae020cfad"},
    {file = "clickhouse_connect-0.5.25-pp39-pypy39_pp73-win_amd64.whl", hash = "sha256:b0a12c57233c85edd1a03d7bd153ef68b5392d500d8a1cf21de8cb5698c57481"},
]

[package.dependencies]
certifi = "*"
lz4 = "*"
pytz = "*"
urllib3 = ">=1.26"
zstandard = "*"

[package.extras]
arrow = ["pyarrow"]
numpy = ["numpy"]
orjson = ["orjson"]
pandas = ["pandas"]
sqlalchemy = ["sqlalchemy (>1.3.21,<1.4)"]
superset = ["apache-superset (>=1.4.1)"]

[[package]]
name = "colorama"
version = "0.4.6"
description = "Cross-platform colored terminal text."
category = "main"
optional = false
python-versions = "!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*,!=3.5.*,!=3.6.*,>=2.7"
files = [
    {file = "colorama-0.4.6-py2.py3-none-any.whl", hash = "sha256:4f1d9991f5acc0ca119f9d443620b77f9d6b33703e51011c16baf57afb285fc6"},
    {file = "colorama-0.4.6.tar.gz", hash = "sha256:08695f5cb7ed6e0531a20572697297273c47b8cae5a63ffc6d6ed5c201be6e44"},
]

[[package]]
name = "coloredlogs"
version = "15.0.1"
description = "Colored terminal output for Python's logging module"
category = "main"
optional = false
python-versions = ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*"
files = [
    {file = "coloredlogs-15.0.1-py2.py3-none-any.whl", hash = "sha256:612ee75c546f53e92e70049c9dbfcc18c935a2b9a53b66085ce9ef6a6e5c0934"},
    {file = "coloredlogs-15.0.1.tar.gz", hash = "sha256:7c991aa71a4577af2f82600d8f8f3a89f936baeaf9b50a9c197da014e5bf16b0"},
]

[package.dependencies]
humanfriendly = ">=9.1"

[package.extras]
cron = ["capturer (>=2.4)"]

[[package]]
name = "coverage"
version = "7.2.7"
description = "Code coverage measurement for Python"
category = "dev"
optional = false
python-versions = ">=3.7"
files = [
    {file = "coverage-7.2.7-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:d39b5b4f2a66ccae8b7263ac3c8170994b65266797fb96cbbfd3fb5b23921db8"},
    {file = "coverage-7.2.7-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:6d040ef7c9859bb11dfeb056ff5b3872436e3b5e401817d87a31e1750b9ae2fb"},
    {file = "coverage-7.2.7-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:ba90a9563ba44a72fda2e85302c3abc71c5589cea608ca16c22b9804262aaeb6"},
    {file = "coverage-7.2.7-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:e7d9405291c6928619403db1d10bd07888888ec1abcbd9748fdaa971d7d661b2"},
    {file = "coverage-7.2.7-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:31563e97dae5598556600466ad9beea39fb04e0229e61c12eaa206e0aa202063"},
    {file = "coverage-7.2.7-cp310-cp310-musllinux_1_1_aarch64.whl", hash = "sha256:ebba1cd308ef115925421d3e6a586e655ca5a77b5bf41e02eb0e4562a111f2d1"},
    {file = "coverage-7.2.7-cp310-cp310-musllinux_1_1_i686.whl", hash = "sha256:cb017fd1b2603ef59e374ba2063f593abe0fc45f2ad9abdde5b4d83bd922a353"},
    {file = "coverage-7.2.7-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:d62a5c7dad11015c66fbb9d881bc4caa5b12f16292f857842d9d1871595f4495"},
    {file = "coverage-7.2.7-cp310-cp310-win32.whl", hash = "sha256:ee57190f24fba796e36bb6d3aa8a8783c643d8fa9760c89f7a98ab5455fbf818"},
    {file = "coverage-7.2.7-cp310-cp310-win_amd64.whl", hash = "sha256:f75f7168ab25dd93110c8a8117a22450c19976afbc44234cbf71481094c1b850"},
    {file = "coverage-7.2.7-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:06a9a2be0b5b576c3f18f1a241f0473575c4a26021b52b2a85263a00f034d51f"},
    {file = "coverage-7.2.7-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:5baa06420f837184130752b7c5ea0808762083bf3487b5038d68b012e5937dbe"},
    {file = "coverage-7.2.7-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:fdec9e8cbf13a5bf63290fc6013d216a4c7232efb51548594ca3631a7f13c3a3"},
    {file = "coverage-7.2.7-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:52edc1a60c0d34afa421c9c37078817b2e67a392cab17d97283b64c5833f427f"},
    {file = "coverage-7.2.7-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:63426706118b7f5cf6bb6c895dc215d8a418d5952544042c8a2d9fe87fcf09cb"},
    {file = "coverage-7.2.7-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:afb17f84d56068a7c29f5fa37bfd38d5aba69e3304af08ee94da8ed5b0865833"},
    {file = "coverage-7.2.7-cp311-cp311-musllinux_1_1_i686.whl", hash = "sha256:48c19d2159d433ccc99e729ceae7d5293fbffa0bdb94952d3579983d1c8c9d97"},
    {file = "coverage-7.2.7-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:0e1f928eaf5469c11e886fe0885ad2bf1ec606434e79842a879277895a50942a"},
    {file = "coverage-7.2.7-cp311-cp311-win32.whl", hash = "sha256:33d6d3ea29d5b3a1a632b3c4e4f4ecae24ef170b0b9ee493883f2df10039959a"},
    {file = "coverage-7.2.7-cp311-cp311-win_amd64.whl", hash = "sha256:5b7540161790b2f28143191f5f8ec02fb132660ff175b7747b95dcb77ac26562"},
    {file = "coverage-7.2.7-cp312-cp312-macosx_10_9_x86_64.whl", hash = "sha256:f2f67fe12b22cd130d34d0ef79206061bfb5eda52feb6ce0dba0644e20a03cf4"},
    {file = "coverage-7.2.7-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:a342242fe22407f3c17f4b499276a02b01e80f861f1682ad1d95b04018e0c0d4"},
    {file = "coverage-7.2.7-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:171717c7cb6b453aebac9a2ef603699da237f341b38eebfee9be75d27dc38e01"},
    {file = "coverage-7.2.7-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:49969a9f7ffa086d973d91cec8d2e31080436ef0fb4a359cae927e742abfaaa6"},
    {file = "coverage-7.2.7-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:b46517c02ccd08092f4fa99f24c3b83d8f92f739b4657b0f146246a0ca6a831d"},
    {file = "coverage-7.2.7-cp312-cp312-musllinux_1_1_i686.whl", hash = "sha256:a3d33a6b3eae87ceaefa91ffdc130b5e8536182cd6dfdbfc1aa56b46ff8c86de"},
    {file = "coverage-7.2.7-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:976b9c42fb2a43ebf304fa7d4a310e5f16cc99992f33eced91ef6f908bd8f33d"},
    {file = "coverage-7.2.7-cp312-cp312-win32.whl", hash = "sha256:8de8bb0e5ad103888d65abef8bca41ab93721647590a3f740100cd65c3b00511"},
    {file = "coverage-7.2.7-cp312-cp312-win_amd64.whl", hash = "sha256:9e31cb64d7de6b6f09702bb27c02d1904b3aebfca610c12772452c4e6c21a0d3"},
    {file = "coverage-7.2.7-cp37-cp37m-macosx_10_9_x86_64.whl", hash = "sha256:58c2ccc2f00ecb51253cbe5d8d7122a34590fac9646a960d1430d5b15321d95f"},
    {file = "coverage-7.2.7-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d22656368f0e6189e24722214ed8d66b8022db19d182927b9a248a2a8a2f67eb"},
    {file = "coverage-7.2.7-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:a895fcc7b15c3fc72beb43cdcbdf0ddb7d2ebc959edac9cef390b0d14f39f8a9"},
    {file = "coverage-7.2.7-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:e84606b74eb7de6ff581a7915e2dab7a28a0517fbe1c9239eb227e1354064dcd"},
    {file = "coverage-7.2.7-cp37-cp37m-musllinux_1_1_aarch64.whl", hash = "sha256:0a5f9e1dbd7fbe30196578ca36f3fba75376fb99888c395c5880b355e2875f8a"},
    {file = "coverage-7.2.7-cp37-cp37m-musllinux_1_1_i686.whl", hash = "sha256:419bfd2caae268623dd469eff96d510a920c90928b60f2073d79f8fe2bbc5959"},
    {file = "coverage-7.2.7-cp37-cp37m-musllinux_1_1_x86_64.whl", hash = "sha256:2aee274c46590717f38ae5e4650988d1af340fe06167546cc32fe2f58ed05b02"},
    {file = "coverage-7.2.7-cp37-cp37m-win32.whl", hash = "sha256:61b9a528fb348373c433e8966535074b802c7a5d7f23c4f421e6c6e2f1697a6f"},
    {file = "coverage-7.2.7-cp37-cp37m-win_amd64.whl", hash = "sha256:b1c546aca0ca4d028901d825015dc8e4d56aac4b541877690eb76490f1dc8ed0"},
    {file = "coverage-7.2.7-cp38-cp38-macosx_10_9_x86_64.whl", hash = "sha256:54b896376ab563bd38453cecb813c295cf347cf5906e8b41d340b0321a5433e5"},
    {file = "coverage-7.2.7-cp38-cp38-macosx_11_0_arm64.whl", hash = "sha256:3d376df58cc111dc8e21e3b6e24606b5bb5dee6024f46a5abca99124b2229ef5"},
    {file = "coverage-7.2.7-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:5e330fc79bd7207e46c7d7fd2bb4af2963f5f635703925543a70b99574b0fea9"},
    {file = "coverage-7.2.7-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:1e9d683426464e4a252bf70c3498756055016f99ddaec3774bf368e76bbe02b6"},
    {file = "coverage-7.2.7-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8d13c64ee2d33eccf7437961b6ea7ad8673e2be040b4f7fd4fd4d4d28d9ccb1e"},
    {file = "coverage-7.2.7-cp38-cp38-musllinux_1_1_aarch64.whl", hash = "sha256:b7aa5f8a41217360e600da646004f878250a0d6738bcdc11a0a39928d7dc2050"},
    {file = "coverage-7.2.7-cp38-cp38-musllinux_1_1_i686.whl", hash = "sha256:8fa03bce9bfbeeef9f3b160a8bed39a221d82308b4152b27d82d8daa7041fee5"},
    {file = "coverage-7.2.7-cp38-cp38-musllinux_1_1_x86_64.whl", hash = "sha256:245167dd26180ab4c91d5e1496a30be4cd721a5cf2abf52974f965f10f11419f"},
    {file = "coverage-7.2.7-cp38-cp38-win32.whl", hash = "sha256:d2c2db7fd82e9b72937969bceac4d6ca89660db0a0967614ce2481e81a0b771e"},
    {file = "coverage-7.2.7-cp38-cp38-win_amd64.whl", hash = "sha256:2e07b54284e381531c87f785f613b833569c14ecacdcb85d56b25c4622c16c3c"},
    {file = "coverage-7.2.7-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:537891ae8ce59ef63d0123f7ac9e2ae0fc8b72c7ccbe5296fec45fd68967b6c9"},
    {file = "coverage-7.2.7-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:06fb182e69f33f6cd1d39a6c597294cff3143554b64b9825d1dc69d18cc2fff2"},
    {file = "coverage-7.2.7-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:201e7389591af40950a6480bd9edfa8ed04346ff80002cec1a66cac4549c1ad7"},
    {file = "coverage-7.2.7-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:f6951407391b639504e3b3be51b7ba5f3528adbf1a8ac3302b687ecababf929e"},
    {file = "coverage-7.2.7-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6f48351d66575f535669306aa7d6d6f71bc43372473b54a832222803eb956fd1"},
    {file = "coverage-7.2.7-cp39-cp39-musllinux_1_1_aarch64.whl", hash = "sha256:b29019c76039dc3c0fd815c41392a044ce555d9bcdd38b0fb60fb4cd8e475ba9"},
    {file = "coverage-7.2.7-cp39-cp39-musllinux_1_1_i686.whl", hash = "sha256:81c13a1fc7468c40f13420732805a4c38a105d89848b7c10af65a90beff25250"},
    {file = "coverage-7.2.7-cp39-cp39-musllinux_1_1_x86_64.whl", hash = "sha256:975d70ab7e3c80a3fe86001d8751f6778905ec723f5b110aed1e450da9d4b7f2"},
    {file = "coverage-7.2.7-cp39-cp39-win32.whl", hash = "sha256:7ee7d9d4822c8acc74a5e26c50604dff824710bc8de424904c0982e25c39c6cb"},
    {file = "coverage-7.2.7-cp39-cp39-win_amd64.whl", hash = "sha256:eb393e5ebc85245347950143969b241d08b52b88a3dc39479822e073a1a8eb27"},
    {file = "coverage-7.2.7-pp37.pp38.pp39-none-any.whl", hash = "sha256:b7b4c971f05e6ae490fef852c218b0e79d4e52f79ef0c8475566584a8fb3e01d"},
    {file = "coverage-7.2.7.tar.gz", hash = "sha256:924d94291ca674905fe9481f12294eb11f2d3d3fd1adb20314ba89e94f44ed59"},
]

[package.dependencies]
tomli = {version = "*", optional = true, markers = "python_full_version <= \"3.11.0a6\" and extra == \"toml\""}

[package.extras]
toml = ["tomli"]

[[package]]
name = "cryptography"
version = "41.0.0"
description = "cryptography is a package which provides cryptographic recipes and primitives to Python developers."
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "cryptography-41.0.0-cp37-abi3-macosx_10_12_universal2.whl", hash = "sha256:3c5ef25d060c80d6d9f7f9892e1d41bb1c79b78ce74805b8cb4aa373cb7d5ec8"},
    {file = "cryptography-41.0.0-cp37-abi3-macosx_10_12_x86_64.whl", hash = "sha256:8362565b3835ceacf4dc8f3b56471a2289cf51ac80946f9087e66dc283a810e0"},
    {file = "cryptography-41.0.0-cp37-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:3680248309d340fda9611498a5319b0193a8dbdb73586a1acf8109d06f25b92d"},
    {file = "cryptography-41.0.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:84a165379cb9d411d58ed739e4af3396e544eac190805a54ba2e0322feb55c46"},
    {file = "cryptography-41.0.0-cp37-abi3-manylinux_2_28_aarch64.whl", hash = "sha256:4ab14d567f7bbe7f1cdff1c53d5324ed4d3fc8bd17c481b395db224fb405c237"},
    {file = "cryptography-41.0.0-cp37-abi3-manylinux_2_28_x86_64.whl", hash = "sha256:9f65e842cb02550fac96536edb1d17f24c0a338fd84eaf582be25926e993dde4"},
    {file = "cryptography-41.0.0-cp37-abi3-musllinux_1_1_aarch64.whl", hash = "sha256:b7f2f5c525a642cecad24ee8670443ba27ac1fab81bba4cc24c7b6b41f2d0c75"},
    {file = "cryptography-41.0.0-cp37-abi3-musllinux_1_1_x86_64.whl", hash = "sha256:7d92f0248d38faa411d17f4107fc0bce0c42cae0b0ba5415505df72d751bf62d"},
    {file = "cryptography-41.0.0-cp37-abi3-win32.whl", hash = "sha256:34d405ea69a8b34566ba3dfb0521379b210ea5d560fafedf9f800a9a94a41928"},
    {file = "cryptography-41.0.0-cp37-abi3-win_amd64.whl", hash = "sha256:344c6de9f8bda3c425b3a41b319522ba3208551b70c2ae00099c205f0d9fd3be"},
    {file = "cryptography-41.0.0-pp38-pypy38_pp73-macosx_10_12_x86_64.whl", hash = "sha256:88ff107f211ea696455ea8d911389f6d2b276aabf3231bf72c8853d22db755c5"},
    {file = "cryptography-41.0.0-pp38-pypy38_pp73-manylinux_2_28_aarch64.whl", hash = "sha256:b846d59a8d5a9ba87e2c3d757ca019fa576793e8758174d3868aecb88d6fc8eb"},
    {file = "cryptography-41.0.0-pp38-pypy38_pp73-manylinux_2_28_x86_64.whl", hash = "sha256:f5d0bf9b252f30a31664b6f64432b4730bb7038339bd18b1fafe129cfc2be9be"},
    {file = "cryptography-41.0.0-pp38-pypy38_pp73-win_amd64.whl", hash = "sha256:5c1f7293c31ebc72163a9a0df246f890d65f66b4a40d9ec80081969ba8c78cc9"},
    {file = "cryptography-41.0.0-pp39-pypy39_pp73-macosx_10_12_x86_64.whl", hash = "sha256:bf8fc66012ca857d62f6a347007e166ed59c0bc150cefa49f28376ebe7d992a2"},
    {file = "cryptography-41.0.0-pp39-pypy39_pp73-manylinux_2_28_aarch64.whl", hash = "sha256:a4fc68d1c5b951cfb72dfd54702afdbbf0fb7acdc9b7dc4301bbf2225a27714d"},
    {file = "cryptography-41.0.0-pp39-pypy39_pp73-manylinux_2_28_x86_64.whl", hash = "sha256:14754bcdae909d66ff24b7b5f166d69340ccc6cb15731670435efd5719294895"},
    {file = "cryptography-41.0.0-pp39-pypy39_pp73-win_amd64.whl", hash = "sha256:0ddaee209d1cf1f180f1efa338a68c4621154de0afaef92b89486f5f96047c55"},
    {file = "cryptography-41.0.0.tar.gz", hash = "sha256:6b71f64beeea341c9b4f963b48ee3b62d62d57ba93eb120e1196b31dc1025e78"},
]

[package.dependencies]
cffi = ">=1.12"

[package.extras]
docs = ["sphinx (>=5.3.0)", "sphinx-rtd-theme (>=1.1.1)"]
docstest = ["pyenchant (>=1.6.11)", "sphinxcontrib-spelling (>=4.0.1)", "twine (>=1.12.0)"]
nox = ["nox"]
pep8test = ["black", "check-sdist", "mypy", "ruff"]
sdist = ["build"]
ssh = ["bcrypt (>=3.1.5)"]
test = ["pretend", "pytest (>=6.2.0)", "pytest-benchmark", "pytest-cov", "pytest-xdist"]
test-randomorder = ["pytest-randomly"]

[[package]]
name = "dataclasses-json"
version = "0.5.7"
description = "Easily serialize dataclasses to and from JSON"
category = "main"
optional = false
python-versions = ">=3.6"
files = [
    {file = "dataclasses-json-0.5.7.tar.gz", hash = "sha256:c2c11bc8214fbf709ffc369d11446ff6945254a7f09128154a7620613d8fda90"},
    {file = "dataclasses_json-0.5.7-py3-none-any.whl", hash = "sha256:bc285b5f892094c3a53d558858a88553dd6a61a11ab1a8128a0e554385dcc5dd"},
]

[package.dependencies]
marshmallow = ">=3.3.0,<4.0.0"
marshmallow-enum = ">=1.5.1,<2.0.0"
typing-inspect = ">=0.4.0"

[package.extras]
dev = ["flake8", "hypothesis", "ipython", "mypy (>=0.710)", "portray", "pytest (>=6.2.3)", "simplejson", "types-dataclasses"]

[[package]]
name = "decorator"
version = "5.1.1"
description = "Decorators for Humans"
category = "main"
optional = false
python-versions = ">=3.5"
files = [
    {file = "decorator-5.1.1-py3-none-any.whl", hash = "sha256:b8c3f85900b9dc423225913c5aace94729fe1fa9763b38939a95226f02d37186"},
    {file = "decorator-5.1.1.tar.gz", hash = "sha256:637996211036b6385ef91435e4fae22989472f9d571faba8927ba8253acbc330"},
]

[[package]]
name = "deprecation"
version = "2.1.0"
description = "A library to handle automated deprecations"
category = "main"
optional = false
python-versions = "*"
files = [
    {file = "deprecation-2.1.0-py2.py3-none-any.whl", hash = "sha256:a10811591210e1fb0e768a8c25517cabeabcba6f0bf96564f8ff45189f90b14a"},
    {file = "deprecation-2.1.0.tar.gz", hash = "sha256:72b3bde64e5d778694b0cf68178aed03d15e15477116add3fb773e581f9518ff"},
]

[package.dependencies]
packaging = "*"

[[package]]
name = "dnspython"
version = "2.3.0"
description = "DNS toolkit"
category = "main"
optional = false
python-versions = ">=3.7,<4.0"
files = [
    {file = "dnspython-2.3.0-py3-none-any.whl", hash = "sha256:89141536394f909066cabd112e3e1a37e4e654db00a25308b0f130bc3152eb46"},
    {file = "dnspython-2.3.0.tar.gz", hash = "sha256:224e32b03eb46be70e12ef6d64e0be123a64e621ab4c0822ff6d450d52a540b9"},
]

[package.extras]
curio = ["curio (>=1.2,<2.0)", "sniffio (>=1.1,<2.0)"]
dnssec = ["cryptography (>=2.6,<40.0)"]
doh = ["h2 (>=4.1.0)", "httpx (>=0.21.1)", "requests (>=2.23.0,<3.0.0)", "requests-toolbelt (>=0.9.1,<0.11.0)"]
doq = ["aioquic (>=0.9.20)"]
idna = ["idna (>=2.1,<4.0)"]
trio = ["trio (>=0.14,<0.23)"]
wmi = ["wmi (>=1.5.1,<2.0.0)"]

[[package]]
name = "docutils"
version = "0.20.1"
description = "Docutils -- Python Documentation Utilities"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "docutils-0.20.1-py3-none-any.whl", hash = "sha256:96f387a2c5562db4476f09f13bbab2192e764cac08ebbf3a34a95d9b1e4a59d6"},
    {file = "docutils-0.20.1.tar.gz", hash = "sha256:f08a4e276c3a1583a86dce3e34aba3fe04d02bba2dd51ed16106244e8a923e3b"},
]

[[package]]
name = "docx2txt"
version = "0.8"
description = "A pure python-based utility to extract text and images from docx files."
category = "main"
optional = false
python-versions = "*"
files = [
    {file = "docx2txt-0.8.tar.gz", hash = "sha256:2c06d98d7cfe2d3947e5760a57d924e3ff07745b379c8737723922e7009236e5"},
]

[[package]]
name = "dotty-dict"
version = "1.3.1"
description = "Dictionary wrapper for quick access to deeply nested keys."
category = "main"
optional = false
python-versions = ">=3.5,<4.0"
files = [
    {file = "dotty_dict-1.3.1-py3-none-any.whl", hash = "sha256:5022d234d9922f13aa711b4950372a06a6d64cb6d6db9ba43d0ba133ebfce31f"},
    {file = "dotty_dict-1.3.1.tar.gz", hash = "sha256:4b016e03b8ae265539757a53eba24b9bfda506fb94fbce0bee843c6f05541a15"},
]

[[package]]
name = "duckdb"
version = "0.8.0"
description = "DuckDB embedded database"
category = "main"
optional = false
python-versions = "*"
files = [
    {file = "duckdb-0.8.0-cp310-cp310-macosx_10_9_universal2.whl", hash = "sha256:6455aee00af30770c20f4a8c5e4347918cf59b578f49ee996a13807b12911871"},
    {file = "duckdb-0.8.0-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:b8cf0622ae7f86d4ce72791f8928af4357a46824aadf1b6879c7936b3db65344"},
    {file = "duckdb-0.8.0-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:6132e8183ca3ae08a593e43c97cb189794077dedd48546e27ce43bd6a51a9c33"},
    {file = "duckdb-0.8.0-cp310-cp310-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:fe29e5343fa2a95f2cde4519a4f4533f4fd551a48d2d9a8ab5220d40ebf53610"},
    {file = "duckdb-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:945165987ca87c097dc0e578dcf47a100cad77e1c29f5dd8443d53ce159dc22e"},
    {file = "duckdb-0.8.0-cp310-cp310-musllinux_1_1_i686.whl", hash = "sha256:673c60daf7ada1d9a8518286a6893ec45efabb64602954af5f3d98f42912fda6"},
    {file = "duckdb-0.8.0-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:d5075fe1ff97ae62331ca5c61e3597e6e9f7682a6fdd418c23ba5c4873ed5cd1"},
    {file = "duckdb-0.8.0-cp310-cp310-win32.whl", hash = "sha256:001f5102f45d3d67f389fa8520046c8f55a99e2c6d43b8e68b38ea93261c5395"},
    {file = "duckdb-0.8.0-cp310-cp310-win_amd64.whl", hash = "sha256:cb00800f2e1e865584b13221e0121fce9341bb3a39a93e569d563eaed281f528"},
    {file = "duckdb-0.8.0-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:b2707096d6df4321044fcde2c9f04da632d11a8be60957fd09d49a42fae71a29"},
    {file = "duckdb-0.8.0-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:b27df1b70ae74d2c88efb5ffca8490954fdc678099509a9c4404ca30acc53426"},
    {file = "duckdb-0.8.0-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:75a97c800271b52dd0f37696d074c50576dcb4b2750b6115932a98696a268070"},
    {file = "duckdb-0.8.0-cp311-cp311-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:804cac261a5e016506a6d67838a65d19b06a237f7949f1704f0e800eb708286a"},
    {file = "duckdb-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:c6b9abca7fa6713e1d031c18485343b4de99742c7e1b85c10718aa2f31a4e2c6"},
    {file = "duckdb-0.8.0-cp311-cp311-musllinux_1_1_i686.whl", hash = "sha256:51aa6d606d49072abcfeb3be209eb559ac94c1b5e70f58ac3adbb94aca9cd69f"},
    {file = "duckdb-0.8.0-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:7c8dc769aaf2be0a1c57995ca657e5b92c1c56fc8437edb720ca6cab571adf14"},
    {file = "duckdb-0.8.0-cp311-cp311-win32.whl", hash = "sha256:c4207d18b42387c4a035846d8878eb967070198be8ac26fd77797ce320d1a400"},
    {file = "duckdb-0.8.0-cp311-cp311-win_amd64.whl", hash = "sha256:0c392257547c20794c3072fcbca99a49ef0a49974005d755e93893e2b4875267"},
    {file = "duckdb-0.8.0-cp36-cp36m-macosx_10_9_x86_64.whl", hash = "sha256:2832379e122020814dbe869af7b9ddf3c9f21474cf345531145b099c63ffe17e"},
    {file = "duckdb-0.8.0-cp36-cp36m-win32.whl", hash = "sha256:914896526f7caba86b170f2c4f17f11fd06540325deeb0000cb4fb24ec732966"},
    {file = "duckdb-0.8.0-cp36-cp36m-win_amd64.whl", hash = "sha256:022ebda86d0e3204cdc206e4af45aa9f0ae0668b34c2c68cf88e08355af4a372"},
    {file = "duckdb-0.8.0-cp37-cp37m-macosx_10_9_x86_64.whl", hash = "sha256:96a31c0f3f4ccbf0f5b18f94319f37691205d82f80aae48c6fe04860d743eb2c"},
    {file = "duckdb-0.8.0-cp37-cp37m-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:a07c73c6e6a8cf4ce1a634625e0d1b17e5b817242a8a530d26ed84508dfbdc26"},
    {file = "duckdb-0.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:424acbd6e857531b06448d757d7c2557938dbddbff0632092090efbf413b4699"},
    {file = "duckdb-0.8.0-cp37-cp37m-musllinux_1_1_i686.whl", hash = "sha256:c83cfd2a868f1acb0692b9c3fd5ef1d7da8faa1348c6eabf421fbf5d8c2f3eb8"},
    {file = "duckdb-0.8.0-cp37-cp37m-musllinux_1_1_x86_64.whl", hash = "sha256:5c6f6b2d8db56936f662c649539df81856b5a8cb769a31f9544edf18af2a11ff"},
    {file = "duckdb-0.8.0-cp37-cp37m-win32.whl", hash = "sha256:0bd6376b40a512172eaf4aa816813b1b9d68994292ca436ce626ccd5f77f8184"},
    {file = "duckdb-0.8.0-cp37-cp37m-win_amd64.whl", hash = "sha256:931221885bcf1e7dfce2400f11fd048a7beef566b775f1453bb1db89b828e810"},
    {file = "duckdb-0.8.0-cp38-cp38-macosx_10_9_universal2.whl", hash = "sha256:42e7853d963d68e72403ea208bcf806b0f28c7b44db0aa85ce49bb124d56c133"},
    {file = "duckdb-0.8.0-cp38-cp38-macosx_10_9_x86_64.whl", hash = "sha256:fcc338399175be3d43366576600aef7d72e82114d415992a7a95aded98a0f3fd"},
    {file = "duckdb-0.8.0-cp38-cp38-macosx_11_0_arm64.whl", hash = "sha256:03dd08a4624d6b581a59f9f9dbfd34902416398d16795ad19f92361cf21fd9b5"},
    {file = "duckdb-0.8.0-cp38-cp38-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:0c7c24ea0c9d8563dbd5ad49ccb54b7a9a3c7b8c2833d35e5d32a08549cacea5"},
    {file = "duckdb-0.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:cb58f6505cc0f34b4e976154302d26563d2e5d16b206758daaa04b65e55d9dd8"},
    {file = "duckdb-0.8.0-cp38-cp38-musllinux_1_1_i686.whl", hash = "sha256:ef37ac7880100c4b3f913c8483a29a13f8289313b9a07df019fadfa8e7427544"},
    {file = "duckdb-0.8.0-cp38-cp38-musllinux_1_1_x86_64.whl", hash = "sha256:c2a4f5ee913ca8a6a069c78f8944b9934ffdbc71fd935f9576fdcea2a6f476f1"},
    {file = "duckdb-0.8.0-cp38-cp38-win32.whl", hash = "sha256:73831c6d7aefcb5f4072cd677b9efebecbf6c578946d21710791e10a1fc41b9a"},
    {file = "duckdb-0.8.0-cp38-cp38-win_amd64.whl", hash = "sha256:faa36d2854734364d234f37d7ef4f3d763b73cd6b0f799cbc2a0e3b7e2575450"},
    {file = "duckdb-0.8.0-cp39-cp39-macosx_10_9_universal2.whl", hash = "sha256:50a31ec237ed619e50f9ab79eb0ec5111eb9697d4475da6e0ab22c08495ce26b"},
    {file = "duckdb-0.8.0-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:351abb4cc2d229d043920c4bc2a4c29ca31a79fef7d7ef8f6011cf4331f297bf"},
    {file = "duckdb-0.8.0-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:568550a163aca6a787bef8313e358590254de3f4019025a8d68c3a61253fedc1"},
    {file = "duckdb-0.8.0-cp39-cp39-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:2b82617f0e7f9fc080eda217090d82b42d4fad083bc9f6d58dfda9cecb7e3b29"},
    {file = "duckdb-0.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:d01c9be34d272532b75e8faedda0ff77fa76d1034cde60b8f5768ae85680d6d3"},
    {file = "duckdb-0.8.0-cp39-cp39-musllinux_1_1_i686.whl", hash = "sha256:8549d6a6bf5f00c012b6916f605416226507e733a3ffc57451682afd6e674d1b"},
    {file = "duckdb-0.8.0-cp39-cp39-musllinux_1_1_x86_64.whl", hash = "sha256:8d145c6d51e55743c3ed1a74cffa109d9e72f82b07e203b436cfa453c925313a"},
    {file = "duckdb-0.8.0-cp39-cp39-win32.whl", hash = "sha256:f8610dfd21e90d7b04e8598b244bf3ad68599fd6ba0daad3428c03cbfd74dced"},
    {file = "duckdb-0.8.0-cp39-cp39-win_amd64.whl", hash = "sha256:d0f0f104d30418808bafbe9bccdcd238588a07bd246b3cff13842d60bfd8e8ba"},
    {file = "duckdb-0.8.0.tar.gz", hash = "sha256:c68da35bab5072a64ada2646a5b343da620ddc75a7a6e84aa4a1e0628a7ec18f"},
]

[[package]]
name = "elastic-transport"
version = "8.4.0"
description = "Transport classes and utilities shared among Python Elastic client libraries"
category = "main"
optional = false
python-versions = ">=3.6"
files = [
    {file = "elastic-transport-8.4.0.tar.gz", hash = "sha256:b9ad708ceb7fcdbc6b30a96f886609a109f042c0b9d9f2e44403b3133ba7ff10"},
    {file = "elastic_transport-8.4.0-py3-none-any.whl", hash = "sha256:19db271ab79c9f70f8c43f8f5b5111408781a6176b54ab2e54d713b6d9ceb815"},
]

[package.dependencies]
certifi = "*"
urllib3 = ">=1.26.2,<2"

[package.extras]
develop = ["aiohttp", "mock", "pytest", "pytest-asyncio", "pytest-cov", "pytest-httpserver", "pytest-mock", "requests", "trustme"]

[[package]]
name = "elasticsearch"
version = "8.8.2"
description = "Python client for Elasticsearch"
category = "main"
optional = false
python-versions = ">=3.6, <4"
files = [
    {file = "elasticsearch-8.8.2-py3-none-any.whl", hash = "sha256:bffd6ce4faaacf90e6f617241773b3da8fb94e2e83554f5508e2fab92ca79643"},
    {file = "elasticsearch-8.8.2.tar.gz", hash = "sha256:bed8cf8fcc6c3be7c254b579de4c29afab021f373c832246f912d37aef3c6bd5"},
]

[package.dependencies]
elastic-transport = ">=8,<9"

[package.extras]
async = ["aiohttp (>=3,<4)"]
requests = ["requests (>=2.4.0,<3.0.0)"]

[[package]]
name = "environs"
version = "9.5.0"
description = "simplified environment variable parsing"
category = "main"
optional = false
python-versions = ">=3.6"
files = [
    {file = "environs-9.5.0-py2.py3-none-any.whl", hash = "sha256:1e549569a3de49c05f856f40bce86979e7d5ffbbc4398e7f338574c220189124"},
    {file = "environs-9.5.0.tar.gz", hash = "sha256:a76307b36fbe856bdca7ee9161e6c466fd7fcffc297109a118c59b54e27e30c9"},
]

[package.dependencies]
marshmallow = ">=3.0.0"
python-dotenv = "*"

[package.extras]
dev = ["dj-database-url", "dj-email-url", "django-cache-url", "flake8 (==4.0.1)", "flake8-bugbear (==21.9.2)", "mypy (==0.910)", "pre-commit (>=2.4,<3.0)", "pytest", "tox"]
django = ["dj-database-url", "dj-email-url", "django-cache-url"]
lint = ["flake8 (==4.0.1)", "flake8-bugbear (==21.9.2)", "mypy (==0.910)", "pre-commit (>=2.4,<3.0)"]
tests = ["dj-database-url", "dj-email-url", "django-cache-url", "pytest"]

[[package]]
name = "exceptiongroup"
version = "1.1.1"
description = "Backport of PEP 654 (exception groups)"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "exceptiongroup-1.1.1-py3-none-any.whl", hash = "sha256:232c37c63e4f682982c8b6459f33a8981039e5fb8756b2074364e5055c498c9e"},
    {file = "exceptiongroup-1.1.1.tar.gz", hash = "sha256:d484c3090ba2889ae2928419117447a14daf3c1231d5e30d0aae34f354f01785"},
]

[package.extras]
test = ["pytest (>=6)"]

[[package]]
name = "fastapi"
version = "0.92.0"
description = "FastAPI framework, high performance, easy to learn, fast to code, ready for production"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "fastapi-0.92.0-py3-none-any.whl", hash = "sha256:ae7b97c778e2f2ec3fb3cb4fb14162129411d99907fb71920f6d69a524340ebf"},
    {file = "fastapi-0.92.0.tar.gz", hash = "sha256:023a0f5bd2c8b2609014d3bba1e14a1d7df96c6abea0a73070621c9862b9a4de"},
]

[package.dependencies]
pydantic = ">=1.6.2,<1.7 || >1.7,<1.7.1 || >1.7.1,<1.7.2 || >1.7.2,<1.7.3 || >1.7.3,<1.8 || >1.8,<1.8.1 || >1.8.1,<2.0.0"
starlette = ">=0.25.0,<0.26.0"

[package.extras]
all = ["email-validator (>=1.1.1)", "httpx (>=0.23.0)", "itsdangerous (>=1.1.0)", "jinja2 (>=2.11.2)", "orjson (>=3.2.1)", "python-multipart (>=0.0.5)", "pyyaml (>=5.3.1)", "ujson (>=4.0.1,!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0)", "uvicorn[standard] (>=0.12.0)"]
dev = ["pre-commit (>=2.17.0,<3.0.0)", "ruff (==0.0.138)", "uvicorn[standard] (>=0.12.0,<0.21.0)"]
doc = ["mdx-include (>=1.4.1,<2.0.0)", "mkdocs (>=1.1.2,<2.0.0)", "mkdocs-markdownextradata-plugin (>=0.1.7,<0.3.0)", "mkdocs-material (>=8.1.4,<9.0.0)", "pyyaml (>=5.3.1,<7.0.0)", "typer[all] (>=0.6.1,<0.8.0)"]
test = ["anyio[trio] (>=3.2.1,<4.0.0)", "black (==22.10.0)", "coverage[toml] (>=6.5.0,<8.0)", "databases[sqlite] (>=0.3.2,<0.7.0)", "email-validator (>=1.1.1,<2.0.0)", "flask (>=1.1.2,<3.0.0)", "httpx (>=0.23.0,<0.24.0)", "isort (>=5.0.6,<6.0.0)", "mypy (==0.982)", "orjson (>=3.2.1,<4.0.0)", "passlib[bcrypt] (>=1.7.2,<2.0.0)", "peewee (>=3.13.3,<4.0.0)", "pytest (>=7.1.3,<8.0.0)", "python-jose[cryptography] (>=3.3.0,<4.0.0)", "python-multipart (>=0.0.5,<0.0.6)", "pyyaml (>=5.3.1,<7.0.0)", "ruff (==0.0.138)", "sqlalchemy (>=1.3.18,<1.4.43)", "types-orjson (==3.6.2)", "types-ujson (==5.6.0.0)", "ujson (>=4.0.1,!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,<6.0.0)"]

[[package]]
name = "filelock"
version = "3.12.0"
description = "A platform independent file lock."
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "filelock-3.12.0-py3-none-any.whl", hash = "sha256:ad98852315c2ab702aeb628412cbf7e95b7ce8c3bf9565670b4eaecf1db370a9"},
    {file = "filelock-3.12.0.tar.gz", hash = "sha256:fc03ae43288c013d2ea83c8597001b1129db351aad9c57fe2409327916b8e718"},
]

[package.extras]
docs = ["furo (>=2023.3.27)", "sphinx (>=6.1.3)", "sphinx-autodoc-typehints (>=1.23,!=1.23.4)"]
testing = ["covdefaults (>=2.3)", "coverage (>=7.2.3)", "diff-cover (>=7.5)", "pytest (>=7.3.1)", "pytest-cov (>=4)", "pytest-mock (>=3.10)", "pytest-timeout (>=2.1)"]

[[package]]
name = "flatbuffers"
version = "23.5.26"
description = "The FlatBuffers serialization format for Python"
category = "main"
optional = false
python-versions = "*"
files = [
    {file = "flatbuffers-23.5.26-py2.py3-none-any.whl", hash = "sha256:c0ff356da363087b915fde4b8b45bdda73432fc17cddb3c8157472eab1422ad1"},
    {file = "flatbuffers-23.5.26.tar.gz", hash = "sha256:9ea1144cac05ce5d86e2859f431c6cd5e66cd9c78c558317c7955fb8d4c78d89"},
]

[[package]]
name = "frozenlist"
version = "1.3.3"
description = "A list-like structure which implements collections.abc.MutableSequence"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "frozenlist-1.3.3-cp310-cp310-macosx_10_9_universal2.whl", hash = "sha256:ff8bf625fe85e119553b5383ba0fb6aa3d0ec2ae980295aaefa552374926b3f4"},
    {file = "frozenlist-1.3.3-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:dfbac4c2dfcc082fcf8d942d1e49b6aa0766c19d3358bd86e2000bf0fa4a9cf0"},
    {file = "frozenlist-1.3.3-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:b1c63e8d377d039ac769cd0926558bb7068a1f7abb0f003e3717ee003ad85530"},
    {file = "frozenlist-1.3.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:7fdfc24dcfce5b48109867c13b4cb15e4660e7bd7661741a391f821f23dfdca7"},
    {file = "frozenlist-1.3.3-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:2c926450857408e42f0bbc295e84395722ce74bae69a3b2aa2a65fe22cb14b99"},
    {file = "frozenlist-1.3.3-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:1841e200fdafc3d51f974d9d377c079a0694a8f06de2e67b48150328d66d5483"},
    {file = "frozenlist-1.3.3-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:f470c92737afa7d4c3aacc001e335062d582053d4dbe73cda126f2d7031068dd"},
    {file = "frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:783263a4eaad7c49983fe4b2e7b53fa9770c136c270d2d4bbb6d2192bf4d9caf"},
    {file = "frozenlist-1.3.3-cp310-cp310-musllinux_1_1_aarch64.whl", hash = "sha256:924620eef691990dfb56dc4709f280f40baee568c794b5c1885800c3ecc69816"},
    {file = "frozenlist-1.3.3-cp310-cp310-musllinux_1_1_i686.whl", hash = "sha256:ae4dc05c465a08a866b7a1baf360747078b362e6a6dbeb0c57f234db0ef88ae0"},
    {file = "frozenlist-1.3.3-cp310-cp310-musllinux_1_1_ppc64le.whl", hash = "sha256:bed331fe18f58d844d39ceb398b77d6ac0b010d571cba8267c2e7165806b00ce"},
    {file = "frozenlist-1.3.3-cp310-cp310-musllinux_1_1_s390x.whl", hash = "sha256:02c9ac843e3390826a265e331105efeab489ffaf4dd86384595ee8ce6d35ae7f"},
    {file = "frozenlist-1.3.3-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:9545a33965d0d377b0bc823dcabf26980e77f1b6a7caa368a365a9497fb09420"},
    {file = "frozenlist-1.3.3-cp310-cp310-win32.whl", hash = "sha256:d5cd3ab21acbdb414bb6c31958d7b06b85eeb40f66463c264a9b343a4e238642"},
    {file = "frozenlist-1.3.3-cp310-cp310-win_amd64.whl", hash = "sha256:b756072364347cb6aa5b60f9bc18e94b2f79632de3b0190253ad770c5df17db1"},
    {file = "frozenlist-1.3.3-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:b4395e2f8d83fbe0c627b2b696acce67868793d7d9750e90e39592b3626691b7"},
    {file = "frozenlist-1.3.3-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:14143ae966a6229350021384870458e4777d1eae4c28d1a7aa47f24d030e6678"},
    {file = "frozenlist-1.3.3-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:5d8860749e813a6f65bad8285a0520607c9500caa23fea6ee407e63debcdbef6"},
    {file = "frozenlist-1.3.3-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:23d16d9f477bb55b6154654e0e74557040575d9d19fe78a161bd33d7d76808e8"},
    {file = "frozenlist-1.3.3-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:eb82dbba47a8318e75f679690190c10a5e1f447fbf9df41cbc4c3afd726d88cb"},
    {file = "frozenlist-1.3.3-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:9309869032abb23d196cb4e4db574232abe8b8be1339026f489eeb34a4acfd91"},
    {file = "frozenlist-1.3.3-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:a97b4fe50b5890d36300820abd305694cb865ddb7885049587a5678215782a6b"},
    {file = "frozenlist-1.3.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:c188512b43542b1e91cadc3c6c915a82a5eb95929134faf7fd109f14f9892ce4"},
    {file = "frozenlist-1.3.3-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:303e04d422e9b911a09ad499b0368dc551e8c3cd15293c99160c7f1f07b59a48"},
    {file = "frozenlist-1.3.3-cp311-cp311-musllinux_1_1_i686.whl", hash = "sha256:0771aed7f596c7d73444c847a1c16288937ef988dc04fb9f7be4b2aa91db609d"},
    {file = "frozenlist-1.3.3-cp311-cp311-musllinux_1_1_ppc64le.whl", hash = "sha256:66080ec69883597e4d026f2f71a231a1ee9887835902dbe6b6467d5a89216cf6"},
    {file = "frozenlist-1.3.3-cp311-cp311-musllinux_1_1_s390x.whl", hash = "sha256:41fe21dc74ad3a779c3d73a2786bdf622ea81234bdd4faf90b8b03cad0c2c0b4"},
    {file = "frozenlist-1.3.3-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:f20380df709d91525e4bee04746ba612a4df0972c1b8f8e1e8af997e678c7b81"},
    {file = "frozenlist-1.3.3-cp311-cp311-win32.whl", hash = "sha256:f30f1928162e189091cf4d9da2eac617bfe78ef907a761614ff577ef4edfb3c8"},
    {file = "frozenlist-1.3.3-cp311-cp311-win_amd64.whl", hash = "sha256:a6394d7dadd3cfe3f4b3b186e54d5d8504d44f2d58dcc89d693698e8b7132b32"},
    {file = "frozenlist-1.3.3-cp37-cp37m-macosx_10_9_x86_64.whl", hash = "sha256:8df3de3a9ab8325f94f646609a66cbeeede263910c5c0de0101079ad541af332"},
    {file = "frozenlist-1.3.3-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0693c609e9742c66ba4870bcee1ad5ff35462d5ffec18710b4ac89337ff16e27"},
    {file = "frozenlist-1.3.3-cp37-cp37m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:cd4210baef299717db0a600d7a3cac81d46ef0e007f88c9335db79f8979c0d3d"},
    {file = "frozenlist-1.3.3-cp37-cp37m-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:394c9c242113bfb4b9aa36e2b80a05ffa163a30691c7b5a29eba82e937895d5e"},
    {file = "frozenlist-1.3.3-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:6327eb8e419f7d9c38f333cde41b9ae348bec26d840927332f17e887a8dcb70d"},
    {file = "frozenlist-1.3.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:2e24900aa13212e75e5b366cb9065e78bbf3893d4baab6052d1aca10d46d944c"},
    {file = "frozenlist-1.3.3-cp37-cp37m-musllinux_1_1_aarch64.whl", hash = "sha256:3843f84a6c465a36559161e6c59dce2f2ac10943040c2fd021cfb70d58c4ad56"},
    {file = "frozenlist-1.3.3-cp37-cp37m-musllinux_1_1_i686.whl", hash = "sha256:84610c1502b2461255b4c9b7d5e9c48052601a8957cd0aea6ec7a7a1e1fb9420"},
    {file = "frozenlist-1.3.3-cp37-cp37m-musllinux_1_1_ppc64le.whl", hash = "sha256:c21b9aa40e08e4f63a2f92ff3748e6b6c84d717d033c7b3438dd3123ee18f70e"},
    {file = "frozenlist-1.3.3-cp37-cp37m-musllinux_1_1_s390x.whl", hash = "sha256:efce6ae830831ab6a22b9b4091d411698145cb9b8fc869e1397ccf4b4b6455cb"},
    {file = "frozenlist-1.3.3-cp37-cp37m-musllinux_1_1_x86_64.whl", hash = "sha256:40de71985e9042ca00b7953c4f41eabc3dc514a2d1ff534027f091bc74416401"},
    {file = "frozenlist-1.3.3-cp37-cp37m-win32.whl", hash = "sha256:180c00c66bde6146a860cbb81b54ee0df350d2daf13ca85b275123bbf85de18a"},
    {file = "frozenlist-1.3.3-cp37-cp37m-win_amd64.whl", hash = "sha256:9bbbcedd75acdfecf2159663b87f1bb5cfc80e7cd99f7ddd9d66eb98b14a8411"},
    {file = "frozenlist-1.3.3-cp38-cp38-macosx_10_9_universal2.whl", hash = "sha256:034a5c08d36649591be1cbb10e09da9f531034acfe29275fc5454a3b101ce41a"},
    {file = "frozenlist-1.3.3-cp38-cp38-macosx_10_9_x86_64.whl", hash = "sha256:ba64dc2b3b7b158c6660d49cdb1d872d1d0bf4e42043ad8d5006099479a194e5"},
    {file = "frozenlist-1.3.3-cp38-cp38-macosx_11_0_arm64.whl", hash = "sha256:47df36a9fe24054b950bbc2db630d508cca3aa27ed0566c0baf661225e52c18e"},
    {file = "frozenlist-1.3.3-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:008a054b75d77c995ea26629ab3a0c0d7281341f2fa7e1e85fa6153ae29ae99c"},
    {file = "frozenlist-1.3.3-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:841ea19b43d438a80b4de62ac6ab21cfe6827bb8a9dc62b896acc88eaf9cecba"},
    {file = "frozenlist-1.3.3-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:e235688f42b36be2b6b06fc37ac2126a73b75fb8d6bc66dd632aa35286238703"},
    {file = "frozenlist-1.3.3-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:ca713d4af15bae6e5d79b15c10c8522859a9a89d3b361a50b817c98c2fb402a2"},
    {file = "frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9ac5995f2b408017b0be26d4a1d7c61bce106ff3d9e3324374d66b5964325448"},
    {file = "frozenlist-1.3.3-cp38-cp38-musllinux_1_1_aarch64.whl", hash = "sha256:a4ae8135b11652b08a8baf07631d3ebfe65a4c87909dbef5fa0cdde440444ee4"},
    {file = "frozenlist-1.3.3-cp38-cp38-musllinux_1_1_i686.whl", hash = "sha256:4ea42116ceb6bb16dbb7d526e242cb6747b08b7710d9782aa3d6732bd8d27649"},
    {file = "frozenlist-1.3.3-cp38-cp38-musllinux_1_1_ppc64le.whl", hash = "sha256:810860bb4bdce7557bc0febb84bbd88198b9dbc2022d8eebe5b3590b2ad6c842"},
    {file = "frozenlist-1.3.3-cp38-cp38-musllinux_1_1_s390x.whl", hash = "sha256:ee78feb9d293c323b59a6f2dd441b63339a30edf35abcb51187d2fc26e696d13"},
    {file = "frozenlist-1.3.3-cp38-cp38-musllinux_1_1_x86_64.whl", hash = "sha256:0af2e7c87d35b38732e810befb9d797a99279cbb85374d42ea61c1e9d23094b3"},
    {file = "frozenlist-1.3.3-cp38-cp38-win32.whl", hash = "sha256:899c5e1928eec13fd6f6d8dc51be23f0d09c5281e40d9cf4273d188d9feeaf9b"},
    {file = "frozenlist-1.3.3-cp38-cp38-win_amd64.whl", hash = "sha256:7f44e24fa70f6fbc74aeec3e971f60a14dde85da364aa87f15d1be94ae75aeef"},
    {file = "frozenlist-1.3.3-cp39-cp39-macosx_10_9_universal2.whl", hash = "sha256:2b07ae0c1edaa0a36339ec6cce700f51b14a3fc6545fdd32930d2c83917332cf"},
    {file = "frozenlist-1.3.3-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:ebb86518203e12e96af765ee89034a1dbb0c3c65052d1b0c19bbbd6af8a145e1"},
    {file = "frozenlist-1.3.3-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:5cf820485f1b4c91e0417ea0afd41ce5cf5965011b3c22c400f6d144296ccbc0"},
    {file = "frozenlist-1.3.3-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:5c11e43016b9024240212d2a65043b70ed8dfd3b52678a1271972702d990ac6d"},
    {file = "frozenlist-1.3.3-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:8fa3c6e3305aa1146b59a09b32b2e04074945ffcfb2f0931836d103a2c38f936"},
    {file = "frozenlist-1.3.3-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:352bd4c8c72d508778cf05ab491f6ef36149f4d0cb3c56b1b4302852255d05d5"},
    {file = "frozenlist-1.3.3-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:65a5e4d3aa679610ac6e3569e865425b23b372277f89b5ef06cf2cdaf1ebf22b"},
    {file = "frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:b1e2c1185858d7e10ff045c496bbf90ae752c28b365fef2c09cf0fa309291669"},
    {file = "frozenlist-1.3.3-cp39-cp39-musllinux_1_1_aarch64.whl", hash = "sha256:f163d2fd041c630fed01bc48d28c3ed4a3b003c00acd396900e11ee5316b56bb"},
    {file = "frozenlist-1.3.3-cp39-cp39-musllinux_1_1_i686.whl", hash = "sha256:05cdb16d09a0832eedf770cb7bd1fe57d8cf4eaf5aced29c4e41e3f20b30a784"},
    {file = "frozenlist-1.3.3-cp39-cp39-musllinux_1_1_ppc64le.whl", hash = "sha256:8bae29d60768bfa8fb92244b74502b18fae55a80eac13c88eb0b496d4268fd2d"},
    {file = "frozenlist-1.3.3-cp39-cp39-musllinux_1_1_s390x.whl", hash = "sha256:eedab4c310c0299961ac285591acd53dc6723a1ebd90a57207c71f6e0c2153ab"},
    {file = "frozenlist-1.3.3-cp39-cp39-musllinux_1_1_x86_64.whl", hash = "sha256:3bbdf44855ed8f0fbcd102ef05ec3012d6a4fd7c7562403f76ce6a52aeffb2b1"},
    {file = "frozenlist-1.3.3-cp39-cp39-win32.whl", hash = "sha256:efa568b885bca461f7c7b9e032655c0c143d305bf01c30caf6db2854a4532b38"},
    {file = "frozenlist-1.3.3-cp39-cp39-win_amd64.whl", hash = "sha256:cfe33efc9cb900a4c46f91a5ceba26d6df370ffddd9ca386eb1d4f0ad97b9ea9"},
    {file = "frozenlist-1.3.3.tar.gz", hash = "sha256:58bcc55721e8a90b88332d6cd441261ebb22342e238296bb330968952fbb3a6a"},
]

[[package]]
name = "gitdb"
version = "4.0.10"
description = "Git Object Database"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "gitdb-4.0.10-py3-none-any.whl", hash = "sha256:c286cf298426064079ed96a9e4a9d39e7f3e9bf15ba60701e95f5492f28415c7"},
    {file = "gitdb-4.0.10.tar.gz", hash = "sha256:6eb990b69df4e15bad899ea868dc46572c3f75339735663b81de79b06f17eb9a"},
]

[package.dependencies]
smmap = ">=3.0.1,<6"

[[package]]
name = "gitpython"
version = "3.1.31"
description = "GitPython is a Python library used to interact with Git repositories"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "GitPython-3.1.31-py3-none-any.whl", hash = "sha256:f04893614f6aa713a60cbbe1e6a97403ef633103cdd0ef5eb6efe0deb98dbe8d"},
    {file = "GitPython-3.1.31.tar.gz", hash = "sha256:8ce3bcf69adfdf7c7d503e78fd3b1c492af782d58893b650adb2ac8912ddd573"},
]

[package.dependencies]
gitdb = ">=4.0.1,<5"

[[package]]
name = "gotrue"
version = "1.0.1"
description = "Python Client Library for GoTrue"
category = "main"
optional = false
python-versions = ">=3.8,<4.0"
files = [
    {file = "gotrue-1.0.1-py3-none-any.whl", hash = "sha256:005e8bc8d7f2da87606504c9c269f2943245843e2ddefb99e583f45a8612e715"},
    {file = "gotrue-1.0.1.tar.gz", hash = "sha256:9d7e01703beb3c017bcf0461f518f93bc5a400720df3ba8c082264d405cee4d0"},
]

[package.dependencies]
httpx = ">=0.23.0,<0.24.0"
pydantic = ">=1.10.0,<2.0.0"

[[package]]
name = "greenlet"
version = "2.0.2"
description = "Lightweight in-process concurrent programming"
category = "main"
optional = false
python-versions = ">=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*"
files = [
    {file = "greenlet-2.0.2-cp27-cp27m-macosx_10_14_x86_64.whl", hash = "sha256:bdfea8c661e80d3c1c99ad7c3ff74e6e87184895bbaca6ee8cc61209f8b9b85d"},
    {file = "greenlet-2.0.2-cp27-cp27m-manylinux2010_x86_64.whl", hash = "sha256:9d14b83fab60d5e8abe587d51c75b252bcc21683f24699ada8fb275d7712f5a9"},
    {file = "greenlet-2.0.2-cp27-cp27m-win32.whl", hash = "sha256:6c3acb79b0bfd4fe733dff8bc62695283b57949ebcca05ae5c129eb606ff2d74"},
    {file = "greenlet-2.0.2-cp27-cp27m-win_amd64.whl", hash = "sha256:283737e0da3f08bd637b5ad058507e578dd462db259f7f6e4c5c365ba4ee9343"},
    {file = "greenlet-2.0.2-cp27-cp27mu-manylinux2010_x86_64.whl", hash = "sha256:d27ec7509b9c18b6d73f2f5ede2622441de812e7b1a80bbd446cb0633bd3d5ae"},
    {file = "greenlet-2.0.2-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:d967650d3f56af314b72df7089d96cda1083a7fc2da05b375d2bc48c82ab3f3c"},
    {file = "greenlet-2.0.2-cp310-cp310-macosx_11_0_x86_64.whl", hash = "sha256:30bcf80dda7f15ac77ba5af2b961bdd9dbc77fd4ac6105cee85b0d0a5fcf74df"},
    {file = "greenlet-2.0.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:26fbfce90728d82bc9e6c38ea4d038cba20b7faf8a0ca53a9c07b67318d46088"},
    {file = "greenlet-2.0.2-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:9190f09060ea4debddd24665d6804b995a9c122ef5917ab26e1566dcc712ceeb"},
    {file = "greenlet-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:d75209eed723105f9596807495d58d10b3470fa6732dd6756595e89925ce2470"},
    {file = "greenlet-2.0.2-cp310-cp310-musllinux_1_1_aarch64.whl", hash = "sha256:3a51c9751078733d88e013587b108f1b7a1fb106d402fb390740f002b6f6551a"},
    {file = "greenlet-2.0.2-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:76ae285c8104046b3a7f06b42f29c7b73f77683df18c49ab5af7983994c2dd91"},
    {file = "greenlet-2.0.2-cp310-cp310-win_amd64.whl", hash = "sha256:2d4686f195e32d36b4d7cf2d166857dbd0ee9f3d20ae349b6bf8afc8485b3645"},
    {file = "greenlet-2.0.2-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:c4302695ad8027363e96311df24ee28978162cdcdd2006476c43970b384a244c"},
    {file = "greenlet-2.0.2-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:d4606a527e30548153be1a9f155f4e283d109ffba663a15856089fb55f933e47"},
    {file = "greenlet-2.0.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:c48f54ef8e05f04d6eff74b8233f6063cb1ed960243eacc474ee73a2ea8573ca"},
    {file = "greenlet-2.0.2-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:a1846f1b999e78e13837c93c778dcfc3365902cfb8d1bdb7dd73ead37059f0d0"},
    {file = "greenlet-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:3a06ad5312349fec0ab944664b01d26f8d1f05009566339ac6f63f56589bc1a2"},
    {file = "greenlet-2.0.2-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:eff4eb9b7eb3e4d0cae3d28c283dc16d9bed6b193c2e1ace3ed86ce48ea8df19"},
    {file = "greenlet-2.0.2-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:5454276c07d27a740c5892f4907c86327b632127dd9abec42ee62e12427ff7e3"},
    {file = "greenlet-2.0.2-cp311-cp311-win_amd64.whl", hash = "sha256:7cafd1208fdbe93b67c7086876f061f660cfddc44f404279c1585bbf3cdc64c5"},
    {file = "greenlet-2.0.2-cp35-cp35m-macosx_10_14_x86_64.whl", hash = "sha256:910841381caba4f744a44bf81bfd573c94e10b3045ee00de0cbf436fe50673a6"},
    {file = "greenlet-2.0.2-cp35-cp35m-manylinux2010_x86_64.whl", hash = "sha256:18a7f18b82b52ee85322d7a7874e676f34ab319b9f8cce5de06067384aa8ff43"},
    {file = "greenlet-2.0.2-cp35-cp35m-win32.whl", hash = "sha256:03a8f4f3430c3b3ff8d10a2a86028c660355ab637cee9333d63d66b56f09d52a"},
    {file = "greenlet-2.0.2-cp35-cp35m-win_amd64.whl", hash = "sha256:4b58adb399c4d61d912c4c331984d60eb66565175cdf4a34792cd9600f21b394"},
    {file = "greenlet-2.0.2-cp36-cp36m-macosx_10_14_x86_64.whl", hash = "sha256:703f18f3fda276b9a916f0934d2fb6d989bf0b4fb5a64825260eb9bfd52d78f0"},
    {file = "greenlet-2.0.2-cp36-cp36m-manylinux2010_x86_64.whl", hash = "sha256:32e5b64b148966d9cccc2c8d35a671409e45f195864560829f395a54226408d3"},
    {file = "greenlet-2.0.2-cp36-cp36m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:2dd11f291565a81d71dab10b7033395b7a3a5456e637cf997a6f33ebdf06f8db"},
    {file = "greenlet-2.0.2-cp36-cp36m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:e0f72c9ddb8cd28532185f54cc1453f2c16fb417a08b53a855c4e6a418edd099"},
    {file = "greenlet-2.0.2-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:cd021c754b162c0fb55ad5d6b9d960db667faad0fa2ff25bb6e1301b0b6e6a75"},
    {file = "greenlet-2.0.2-cp36-cp36m-musllinux_1_1_aarch64.whl", hash = "sha256:3c9b12575734155d0c09d6c3e10dbd81665d5c18e1a7c6597df72fd05990c8cf"},
    {file = "greenlet-2.0.2-cp36-cp36m-musllinux_1_1_x86_64.whl", hash = "sha256:b9ec052b06a0524f0e35bd8790686a1da006bd911dd1ef7d50b77bfbad74e292"},
    {file = "greenlet-2.0.2-cp36-cp36m-win32.whl", hash = "sha256:dbfcfc0218093a19c252ca8eb9aee3d29cfdcb586df21049b9d777fd32c14fd9"},
    {file = "greenlet-2.0.2-cp36-cp36m-win_amd64.whl", hash = "sha256:9f35ec95538f50292f6d8f2c9c9f8a3c6540bbfec21c9e5b4b751e0a7c20864f"},
    {file = "greenlet-2.0.2-cp37-cp37m-macosx_10_15_x86_64.whl", hash = "sha256:d5508f0b173e6aa47273bdc0a0b5ba055b59662ba7c7ee5119528f466585526b"},
    {file = "greenlet-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl", hash = "sha256:f82d4d717d8ef19188687aa32b8363e96062911e63ba22a0cff7802a8e58e5f1"},
    {file = "greenlet-2.0.2-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:c9c59a2120b55788e800d82dfa99b9e156ff8f2227f07c5e3012a45a399620b7"},
    {file = "greenlet-2.0.2-cp37-cp37m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:2780572ec463d44c1d3ae850239508dbeb9fed38e294c68d19a24d925d9223ca"},
    {file = "greenlet-2.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:937e9020b514ceedb9c830c55d5c9872abc90f4b5862f89c0887033ae33c6f73"},
    {file = "greenlet-2.0.2-cp37-cp37m-musllinux_1_1_aarch64.whl", hash = "sha256:36abbf031e1c0f79dd5d596bfaf8e921c41df2bdf54ee1eed921ce1f52999a86"},
    {file = "greenlet-2.0.2-cp37-cp37m-musllinux_1_1_x86_64.whl", hash = "sha256:18e98fb3de7dba1c0a852731c3070cf022d14f0d68b4c87a19cc1016f3bb8b33"},
    {file = "greenlet-2.0.2-cp37-cp37m-win32.whl", hash = "sha256:3f6ea9bd35eb450837a3d80e77b517ea5bc56b4647f5502cd28de13675ee12f7"},
    {file = "greenlet-2.0.2-cp37-cp37m-win_amd64.whl", hash = "sha256:7492e2b7bd7c9b9916388d9df23fa49d9b88ac0640db0a5b4ecc2b653bf451e3"},
    {file = "greenlet-2.0.2-cp38-cp38-macosx_10_15_x86_64.whl", hash = "sha256:b864ba53912b6c3ab6bcb2beb19f19edd01a6bfcbdfe1f37ddd1778abfe75a30"},
    {file = "greenlet-2.0.2-cp38-cp38-macosx_11_0_arm64.whl", hash = "sha256:1087300cf9700bbf455b1b97e24db18f2f77b55302a68272c56209d5587c12d1"},
    {file = "greenlet-2.0.2-cp38-cp38-manylinux2010_x86_64.whl", hash = "sha256:ba2956617f1c42598a308a84c6cf021a90ff3862eddafd20c3333d50f0edb45b"},
    {file = "greenlet-2.0.2-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:fc3a569657468b6f3fb60587e48356fe512c1754ca05a564f11366ac9e306526"},
    {file = "greenlet-2.0.2-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:8eab883b3b2a38cc1e050819ef06a7e6344d4a990d24d45bc6f2cf959045a45b"},
    {file = "greenlet-2.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:acd2162a36d3de67ee896c43effcd5ee3de247eb00354db411feb025aa319857"},
    {file = "greenlet-2.0.2-cp38-cp38-musllinux_1_1_aarch64.whl", hash = "sha256:0bf60faf0bc2468089bdc5edd10555bab6e85152191df713e2ab1fcc86382b5a"},
    {file = "greenlet-2.0.2-cp38-cp38-musllinux_1_1_x86_64.whl", hash = "sha256:b0ef99cdbe2b682b9ccbb964743a6aca37905fda5e0452e5ee239b1654d37f2a"},
    {file = "greenlet-2.0.2-cp38-cp38-win32.whl", hash = "sha256:b80f600eddddce72320dbbc8e3784d16bd3fb7b517e82476d8da921f27d4b249"},
    {file = "greenlet-2.0.2-cp38-cp38-win_amd64.whl", hash = "sha256:4d2e11331fc0c02b6e84b0d28ece3a36e0548ee1a1ce9ddde03752d9b79bba40"},
    {file = "greenlet-2.0.2-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:8512a0c38cfd4e66a858ddd1b17705587900dd760c6003998e9472b77b56d417"},
    {file = "greenlet-2.0.2-cp39-cp39-macosx_11_0_x86_64.whl", hash = "sha256:88d9ab96491d38a5ab7c56dd7a3cc37d83336ecc564e4e8816dbed12e5aaefc8"},
    {file = "greenlet-2.0.2-cp39-cp39-manylinux2010_x86_64.whl", hash = "sha256:561091a7be172ab497a3527602d467e2b3fbe75f9e783d8b8ce403fa414f71a6"},
    {file = "greenlet-2.0.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:971ce5e14dc5e73715755d0ca2975ac88cfdaefcaab078a284fea6cfabf866df"},
    {file = "greenlet-2.0.2-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:be4ed120b52ae4d974aa40215fcdfde9194d63541c7ded40ee12eb4dda57b76b"},
    {file = "greenlet-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:94c817e84245513926588caf1152e3b559ff794d505555211ca041f032abbb6b"},
    {file = "greenlet-2.0.2-cp39-cp39-musllinux_1_1_aarch64.whl", hash = "sha256:1a819eef4b0e0b96bb0d98d797bef17dc1b4a10e8d7446be32d1da33e095dbb8"},
    {file = "greenlet-2.0.2-cp39-cp39-musllinux_1_1_x86_64.whl", hash = "sha256:7efde645ca1cc441d6dc4b48c0f7101e8d86b54c8530141b09fd31cef5149ec9"},
    {file = "greenlet-2.0.2-cp39-cp39-win32.whl", hash = "sha256:ea9872c80c132f4663822dd2a08d404073a5a9b5ba6155bea72fb2a79d1093b5"},
    {file = "greenlet-2.0.2-cp39-cp39-win_amd64.whl", hash = "sha256:db1a39669102a1d8d12b57de2bb7e2ec9066a6f2b3da35ae511ff93b01b5d564"},
    {file = "greenlet-2.0.2.tar.gz", hash = "sha256:e7c8dc13af7db097bed64a051d2dd49e9f0af495c26995c00a9ee842690d34c0"},
]

[package.extras]
docs = ["Sphinx", "docutils (<0.18)"]
test = ["objgraph", "psutil"]

[[package]]
name = "grpcio"
version = "1.53.0"
description = "HTTP/2-based RPC framework"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "grpcio-1.53.0-cp310-cp310-linux_armv7l.whl", hash = "sha256:752d2949b40e12e6ad3ed8cc552a65b54d226504f6b1fb67cab2ccee502cc06f"},
    {file = "grpcio-1.53.0-cp310-cp310-macosx_12_0_universal2.whl", hash = "sha256:8a48fd3a7222be226bb86b7b413ad248f17f3101a524018cdc4562eeae1eb2a3"},
    {file = "grpcio-1.53.0-cp310-cp310-manylinux_2_17_aarch64.whl", hash = "sha256:f3e837d29f0e1b9d6e7b29d569e2e9b0da61889e41879832ea15569c251c303a"},
    {file = "grpcio-1.53.0-cp310-cp310-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:aef7d30242409c3aa5839b501e877e453a2c8d3759ca8230dd5a21cda029f046"},
    {file = "grpcio-1.53.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:e6f90698b5d1c5dd7b3236cd1fa959d7b80e17923f918d5be020b65f1c78b173"},
    {file = "grpcio-1.53.0-cp310-cp310-musllinux_1_1_i686.whl", hash = "sha256:a96c3c7f564b263c5d7c0e49a337166c8611e89c4c919f66dba7b9a84abad137"},
    {file = "grpcio-1.53.0-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:ee81349411648d1abc94095c68cd25e3c2812e4e0367f9a9355be1e804a5135c"},
    {file = "grpcio-1.53.0-cp310-cp310-win32.whl", hash = "sha256:fdc6191587de410a184550d4143e2b24a14df495c86ca15e59508710681690ac"},
    {file = "grpcio-1.53.0-cp310-cp310-win_amd64.whl", hash = "sha256:658ffe1e39171be00490db5bd3b966f79634ac4215a1eb9a85c6cd6783bf7f6e"},
    {file = "grpcio-1.53.0-cp311-cp311-linux_armv7l.whl", hash = "sha256:1b172e6d497191940c4b8d75b53de82dc252e15b61de2951d577ec5b43316b29"},
    {file = "grpcio-1.53.0-cp311-cp311-macosx_10_10_universal2.whl", hash = "sha256:82434ba3a5935e47908bc861ce1ebc43c2edfc1001d235d6e31e5d3ed55815f7"},
    {file = "grpcio-1.53.0-cp311-cp311-manylinux_2_17_aarch64.whl", hash = "sha256:1c734a2d4843e4e14ececf5600c3c4750990ec319e1299db7e4f0d02c25c1467"},
    {file = "grpcio-1.53.0-cp311-cp311-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:b6a2ead3de3b2d53119d473aa2f224030257ef33af1e4ddabd4afee1dea5f04c"},
    {file = "grpcio-1.53.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:a34d6e905f071f9b945cabbcc776e2055de1fdb59cd13683d9aa0a8f265b5bf9"},
    {file = "grpcio-1.53.0-cp311-cp311-musllinux_1_1_i686.whl", hash = "sha256:eaf8e3b97caaf9415227a3c6ca5aa8d800fecadd526538d2bf8f11af783f1550"},
    {file = "grpcio-1.53.0-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:da95778d37be8e4e9afca771a83424f892296f5dfb2a100eda2571a1d8bbc0dc"},
    {file = "grpcio-1.53.0-cp311-cp311-win32.whl", hash = "sha256:e4f513d63df6336fd84b74b701f17d1bb3b64e9d78a6ed5b5e8a198bbbe8bbfa"},
    {file = "grpcio-1.53.0-cp311-cp311-win_amd64.whl", hash = "sha256:ddb2511fbbb440ed9e5c9a4b9b870f2ed649b7715859fd6f2ebc585ee85c0364"},
    {file = "grpcio-1.53.0-cp37-cp37m-linux_armv7l.whl", hash = "sha256:2a912397eb8d23c177d6d64e3c8bc46b8a1c7680b090d9f13a640b104aaec77c"},
    {file = "grpcio-1.53.0-cp37-cp37m-macosx_10_10_universal2.whl", hash = "sha256:55930c56b8f5b347d6c8c609cc341949a97e176c90f5cbb01d148d778f3bbd23"},
    {file = "grpcio-1.53.0-cp37-cp37m-manylinux_2_17_aarch64.whl", hash = "sha256:6601d812105583948ab9c6e403a7e2dba6e387cc678c010e74f2d6d589d1d1b3"},
    {file = "grpcio-1.53.0-cp37-cp37m-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:c705e0c21acb0e8478a00e7e773ad0ecdb34bd0e4adc282d3d2f51ba3961aac7"},
    {file = "grpcio-1.53.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:ba074af9ca268ad7b05d3fc2b920b5fb3c083da94ab63637aaf67f4f71ecb755"},
    {file = "grpcio-1.53.0-cp37-cp37m-musllinux_1_1_i686.whl", hash = "sha256:14817de09317dd7d3fbc8272864288320739973ef0f4b56bf2c0032349da8cdf"},
    {file = "grpcio-1.53.0-cp37-cp37m-musllinux_1_1_x86_64.whl", hash = "sha256:c7ad9fbedb93f331c2e9054e202e95cf825b885811f1bcbbdfdc301e451442db"},
    {file = "grpcio-1.53.0-cp37-cp37m-win_amd64.whl", hash = "sha256:dad5b302a4c21c604d88a5d441973f320134e6ff6a84ecef9c1139e5ffd466f6"},
    {file = "grpcio-1.53.0-cp38-cp38-linux_armv7l.whl", hash = "sha256:fa8eaac75d3107e3f5465f2c9e3bbd13db21790c6e45b7de1756eba16b050aca"},
    {file = "grpcio-1.53.0-cp38-cp38-macosx_10_10_universal2.whl", hash = "sha256:104a2210edd3776c38448b4f76c2f16e527adafbde171fc72a8a32976c20abc7"},
    {file = "grpcio-1.53.0-cp38-cp38-manylinux_2_17_aarch64.whl", hash = "sha256:dbc1ba968639c1d23476f75c356e549e7bbf2d8d6688717dcab5290e88e8482b"},
    {file = "grpcio-1.53.0-cp38-cp38-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:95952d3fe795b06af29bb8ec7bbf3342cdd867fc17b77cc25e6733d23fa6c519"},
    {file = "grpcio-1.53.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:f144a790f14c51b8a8e591eb5af40507ffee45ea6b818c2482f0457fec2e1a2e"},
    {file = "grpcio-1.53.0-cp38-cp38-musllinux_1_1_i686.whl", hash = "sha256:0698c094688a2dd4c7c2f2c0e3e142cac439a64d1cef6904c97f6cde38ba422f"},
    {file = "grpcio-1.53.0-cp38-cp38-musllinux_1_1_x86_64.whl", hash = "sha256:6b6d60b0958be711bab047e9f4df5dbbc40367955f8651232bfdcdd21450b9ab"},
    {file = "grpcio-1.53.0-cp38-cp38-win32.whl", hash = "sha256:1948539ce78805d4e6256ab0e048ec793956d54787dc9d6777df71c1d19c7f81"},
    {file = "grpcio-1.53.0-cp38-cp38-win_amd64.whl", hash = "sha256:df9ba1183b3f649210788cf80c239041dddcb375d6142d8bccafcfdf549522cd"},
    {file = "grpcio-1.53.0-cp39-cp39-linux_armv7l.whl", hash = "sha256:19caa5b7282a89b799e63776ff602bb39604f7ca98db6df27e2de06756ae86c3"},
    {file = "grpcio-1.53.0-cp39-cp39-macosx_10_10_universal2.whl", hash = "sha256:b5bd026ac928c96cc23149e6ef79183125542062eb6d1ccec34c0a37e02255e7"},
    {file = "grpcio-1.53.0-cp39-cp39-manylinux_2_17_aarch64.whl", hash = "sha256:7dc8584ca6c015ad82e186e82f4c0fe977394588f66b8ecfc4ec873285314619"},
    {file = "grpcio-1.53.0-cp39-cp39-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:2eddaae8af625e45b5c8500dcca1043264d751a6872cde2eda5022df8a336959"},
    {file = "grpcio-1.53.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:c5fb6f3d7824696c1c9f2ad36ddb080ba5a86f2d929ef712d511b4d9972d3d27"},
    {file = "grpcio-1.53.0-cp39-cp39-musllinux_1_1_i686.whl", hash = "sha256:8270d1dc2c98ab57e6dbf36fa187db8df4c036f04a398e5d5e25b4e01a766d70"},
    {file = "grpcio-1.53.0-cp39-cp39-musllinux_1_1_x86_64.whl", hash = "sha256:976a7f24eb213e8429cab78d5e120500dfcdeb01041f1f5a77b17b9101902615"},
    {file = "grpcio-1.53.0-cp39-cp39-win32.whl", hash = "sha256:9c84a481451e7174f3a764a44150f93b041ab51045aa33d7b5b68b6979114e48"},
    {file = "grpcio-1.53.0-cp39-cp39-win_amd64.whl", hash = "sha256:6beb84f83360ff29a3654f43f251ec11b809dcb5524b698d711550243debd289"},
    {file = "grpcio-1.53.0.tar.gz", hash = "sha256:a4952899b4931a6ba12951f9a141ef3e74ff8a6ec9aa2dc602afa40f63595e33"},
]

[package.extras]
protobuf = ["grpcio-tools (>=1.53.0)"]

[[package]]
name = "grpcio-tools"
version = "1.53.0"
description = "Protobuf code generator for gRPC"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "grpcio-tools-1.53.0.tar.gz", hash = "sha256:925efff2d63ca3266f93c924ffeba5d496f16a8ccbe125fa0d18acf47cc5fa88"},
    {file = "grpcio_tools-1.53.0-cp310-cp310-linux_armv7l.whl", hash = "sha256:41b859cf943256debba1e7b921e3689c89f95495b65f7ad226c4f0e38edf8ee4"},
    {file = "grpcio_tools-1.53.0-cp310-cp310-macosx_12_0_universal2.whl", hash = "sha256:17c557240f7fbe1886dcfb5f3ba79740ecb65fe3b93061e64b8f4dfc6a6a5dc5"},
    {file = "grpcio_tools-1.53.0-cp310-cp310-manylinux_2_17_aarch64.whl", hash = "sha256:6afffd7e97e5bddc63b3ce9abe912b9adb704a36ba86d4406be94426734b97c2"},
    {file = "grpcio_tools-1.53.0-cp310-cp310-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:f55e2c13620271b7f5a81a489a188d6e34a24da8885d46f1566f0e798cb59e6f"},
    {file = "grpcio_tools-1.53.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6bd4c732d8d7a736e787b5d0963d4195267fc856e1d313d4532d1625e19a0e4a"},
    {file = "grpcio_tools-1.53.0-cp310-cp310-musllinux_1_1_i686.whl", hash = "sha256:99ecefb6b66e9fe41468a70ee2f05da2eb9c7bf63867fb9ff07f7dd90ea813ae"},
    {file = "grpcio_tools-1.53.0-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:7754d6466191d327a0eef364ad5b863477a8fcc12953adc06b30b8e470c70e4a"},
    {file = "grpcio_tools-1.53.0-cp310-cp310-win32.whl", hash = "sha256:f31c549d793a0e72c044f724b3373141d2aa9970fe97b1c2cfaa7ea44002b9aa"},
    {file = "grpcio_tools-1.53.0-cp310-cp310-win_amd64.whl", hash = "sha256:b4173b95e2c29a5145c806d16945ce1e5b38a11c7eb6ab1a6d74afc0a2ce47d9"},
    {file = "grpcio_tools-1.53.0-cp311-cp311-linux_armv7l.whl", hash = "sha256:613a84ebd1881635370c12503f2b15b37332a53fbac32904c94ac4c0c10f0a2a"},
    {file = "grpcio_tools-1.53.0-cp311-cp311-macosx_10_10_universal2.whl", hash = "sha256:af686b83bc6b5c1f1591c9f49183717974047de9546adcf5e09a18781b550c96"},
    {file = "grpcio_tools-1.53.0-cp311-cp311-manylinux_2_17_aarch64.whl", hash = "sha256:3cc832e8297e9437bc2b137fe815c8ba1d9af6ffdd76c5c6d7f911bf8e1b0f45"},
    {file = "grpcio_tools-1.53.0-cp311-cp311-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:39d0a254de49d852f5fe9f9df0a45b2ae66bc04e2d9ee1d6d2c0ba1e70fac91a"},
    {file = "grpcio_tools-1.53.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:7062109553ec1873c5c09cc379b8ae0aa76a2d6d6aae97759b97787b93fa9786"},
    {file = "grpcio_tools-1.53.0-cp311-cp311-musllinux_1_1_i686.whl", hash = "sha256:7728407b1e89fb1473b86152fc33be00f1a25a5aa3264245521f05cbbef9d817"},
    {file = "grpcio_tools-1.53.0-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:2758ea125442bc81251267fc9c28f65555a571f6a0afda4d71a6e7d669347095"},
    {file = "grpcio_tools-1.53.0-cp311-cp311-win32.whl", hash = "sha256:8940d59fca790f1bd45785d0661c3a8c081231c9f8049d7fbf6c6c00737e43da"},
    {file = "grpcio_tools-1.53.0-cp311-cp311-win_amd64.whl", hash = "sha256:c2cff79be5a06d63e9a6a7e38f8f160ade21517386eabe27afacef65a8531358"},
    {file = "grpcio_tools-1.53.0-cp37-cp37m-linux_armv7l.whl", hash = "sha256:d646d65fafbf70a57416493e719a0df7ffa0772133266cfe1b2b72e072ae64a2"},
    {file = "grpcio_tools-1.53.0-cp37-cp37m-macosx_10_10_universal2.whl", hash = "sha256:7da0fc185735050d8240b1d74c4667a02baf1b4fa379a5fc05d1fc067eeba596"},
    {file = "grpcio_tools-1.53.0-cp37-cp37m-manylinux_2_17_aarch64.whl", hash = "sha256:2be17265c0f070efd625683cef986e07dbc495103fcc719009ff2f6988003166"},
    {file = "grpcio_tools-1.53.0-cp37-cp37m-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:4701d48f649443f1101a24d85e9d5ac13346ccac7781e243f49491328e172266"},
    {file = "grpcio_tools-1.53.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:b54c64d85bea5c3a3d895454878c7d6bed5cbb80dc3cafcd75dc1e78300d8c95"},
    {file = "grpcio_tools-1.53.0-cp37-cp37m-musllinux_1_1_i686.whl", hash = "sha256:7152045190e9bd665d1feaeaef931d82c75cacce2b116ab150befa90855de3d0"},
    {file = "grpcio_tools-1.53.0-cp37-cp37m-musllinux_1_1_x86_64.whl", hash = "sha256:e18292123c86975d0aa47f1bcb176393640dcc23912e9f3a2247f1eff81ac8e8"},
    {file = "grpcio_tools-1.53.0-cp37-cp37m-win_amd64.whl", hash = "sha256:b1b76b6ab5c24e44b15d6a7df6c1b81c3099a54b82d41a3ce96e73a2e6a5081c"},
    {file = "grpcio_tools-1.53.0-cp38-cp38-linux_armv7l.whl", hash = "sha256:e76e8dfe6fe4e61ce3049e9d56c0d806d0d3edc28aa32117d1b17f387469c52e"},
    {file = "grpcio_tools-1.53.0-cp38-cp38-macosx_10_10_universal2.whl", hash = "sha256:4c6acaca09cfcd59850e27bd138df9d01c0686c42a5412aa6a92141c15316b1e"},
    {file = "grpcio_tools-1.53.0-cp38-cp38-manylinux_2_17_aarch64.whl", hash = "sha256:76898c1dadf8630a75a40b5a89ab38e326f1288dcfde3413cdfa7a58e149c987"},
    {file = "grpcio_tools-1.53.0-cp38-cp38-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:2b47f8b1bd3af2fb25548b625ad9c3659da30fe83c06f462f357c754f49b71ae"},
    {file = "grpcio_tools-1.53.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:a2faad4b6362e7ff3ae43ef2d51dfce0a3bc32cf52469e88568c3f65cae377d5"},
    {file = "grpcio_tools-1.53.0-cp38-cp38-musllinux_1_1_i686.whl", hash = "sha256:830261fe08541f0fd2dd5035264df2b91012988f37aa1d80a0b4ee6404dc25ae"},
    {file = "grpcio_tools-1.53.0-cp38-cp38-musllinux_1_1_x86_64.whl", hash = "sha256:4be32c694c760f3281555089f7aed7d48ca7ea4094115a08b5fc895e17d7e62e"},
    {file = "grpcio_tools-1.53.0-cp38-cp38-win32.whl", hash = "sha256:4605db5a5828205d7fa33a5de9e00723bd037709e74e15c028b9dcec2339b7bc"},
    {file = "grpcio_tools-1.53.0-cp38-cp38-win_amd64.whl", hash = "sha256:0229e6cd442915192b8f8ee2e7e1c8b9986c878bc4dd8be3539f3be35f1b8282"},
    {file = "grpcio_tools-1.53.0-cp39-cp39-linux_armv7l.whl", hash = "sha256:ad0c20688a650e731e8328a7a08899c433a59bfc995a7afcf715b5ad9eca9e7b"},
    {file = "grpcio_tools-1.53.0-cp39-cp39-macosx_10_10_universal2.whl", hash = "sha256:a8c3e30c531969c62a5a219be414277b269c1be9a76bcd6948571868894e19b2"},
    {file = "grpcio_tools-1.53.0-cp39-cp39-manylinux_2_17_aarch64.whl", hash = "sha256:326c67b35be69409a88632e6145032d53b8b8141634e9cbcd27fa8f9015a112c"},
    {file = "grpcio_tools-1.53.0-cp39-cp39-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:102b6d323d7cef7ac29683f949ec66885b417c06df6059f6a88d07c5556c2592"},
    {file = "grpcio_tools-1.53.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:861f8634cca3ca5bb5336ba16cc78291dba3e7fcadedff195bfdeb433f2c29f2"},
    {file = "grpcio_tools-1.53.0-cp39-cp39-musllinux_1_1_i686.whl", hash = "sha256:c9a9e1da1868349eba401e9648eac19132700942c475adcc97b6938bf4bf0182"},
    {file = "grpcio_tools-1.53.0-cp39-cp39-musllinux_1_1_x86_64.whl", hash = "sha256:ccf7313e5bee13f2f86d12741489f3ed8c901d6b463dff2604191cd4ff518abb"},
    {file = "grpcio_tools-1.53.0-cp39-cp39-win32.whl", hash = "sha256:65b77532bb8f6ab1bfbdd2ac0788626a6c05b227f4722d3bbc2c54258e49c3e5"},
    {file = "grpcio_tools-1.53.0-cp39-cp39-win_amd64.whl", hash = "sha256:7c0ede22796259e83aa1f108038513e86672b2892d3654f94415e3930b74b871"},
]

[package.dependencies]
grpcio = ">=1.53.0"
protobuf = ">=4.21.6,<5.0dev"
setuptools = "*"

[[package]]
name = "h11"
version = "0.14.0"
description = "A pure-Python, bring-your-own-I/O implementation of HTTP/1.1"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "h11-0.14.0-py3-none-any.whl", hash = "sha256:e3fe4ac4b851c468cc8363d500db52c2ead036020723024a109d37346efaa761"},
    {file = "h11-0.14.0.tar.gz", hash = "sha256:8f19fbbe99e72420ff35c00b27a34cb9937e902a8b810e2c88300c6f0a3b699d"},
]

[[package]]
name = "h2"
version = "4.1.0"
description = "HTTP/2 State-Machine based protocol implementation"
category = "main"
optional = false
python-versions = ">=3.6.1"
files = [
    {file = "h2-4.1.0-py3-none-any.whl", hash = "sha256:03a46bcf682256c95b5fd9e9a99c1323584c3eec6440d379b9903d709476bc6d"},
    {file = "h2-4.1.0.tar.gz", hash = "sha256:a83aca08fbe7aacb79fec788c9c0bac936343560ed9ec18b82a13a12c28d2abb"},
]

[package.dependencies]
hpack = ">=4.0,<5"
hyperframe = ">=6.0,<7"

[[package]]
name = "hnswlib"
version = "0.7.0"
description = "hnswlib"
category = "main"
optional = false
python-versions = "*"
files = [
    {file = "hnswlib-0.7.0.tar.gz", hash = "sha256:bc459668e7e44bb7454b256b90c98c5af750653919d9a91698dafcf416cf64c4"},
]

[package.dependencies]
numpy = "*"

[[package]]
name = "hpack"
version = "4.0.0"
description = "Pure-Python HPACK header compression"
category = "main"
optional = false
python-versions = ">=3.6.1"
files = [
    {file = "hpack-4.0.0-py3-none-any.whl", hash = "sha256:84a076fad3dc9a9f8063ccb8041ef100867b1878b25ef0ee63847a5d53818a6c"},
    {file = "hpack-4.0.0.tar.gz", hash = "sha256:fc41de0c63e687ebffde81187a948221294896f6bdc0ae2312708df339430095"},
]

[[package]]
name = "httpcore"
version = "0.16.3"
description = "A minimal low-level HTTP client."
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "httpcore-0.16.3-py3-none-any.whl", hash = "sha256:da1fb708784a938aa084bde4feb8317056c55037247c787bd7e19eb2c2949dc0"},
    {file = "httpcore-0.16.3.tar.gz", hash = "sha256:c5d6f04e2fc530f39e0c077e6a30caa53f1451096120f1f38b954afd0b17c0cb"},
]

[package.dependencies]
anyio = ">=3.0,<5.0"
certifi = "*"
h11 = ">=0.13,<0.15"
sniffio = ">=1.0.0,<2.0.0"

[package.extras]
http2 = ["h2 (>=3,<5)"]
socks = ["socksio (>=1.0.0,<2.0.0)"]

[[package]]
name = "httptools"
version = "0.5.0"
description = "A collection of framework independent HTTP protocol utils."
category = "main"
optional = false
python-versions = ">=3.5.0"
files = [
    {file = "httptools-0.5.0-cp310-cp310-macosx_10_9_universal2.whl", hash = "sha256:8f470c79061599a126d74385623ff4744c4e0f4a0997a353a44923c0b561ee51"},
    {file = "httptools-0.5.0-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:e90491a4d77d0cb82e0e7a9cb35d86284c677402e4ce7ba6b448ccc7325c5421"},
    {file = "httptools-0.5.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:c1d2357f791b12d86faced7b5736dea9ef4f5ecdc6c3f253e445ee82da579449"},
    {file = "httptools-0.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:1f90cd6fd97c9a1b7fe9215e60c3bd97336742a0857f00a4cb31547bc22560c2"},
    {file = "httptools-0.5.0-cp310-cp310-musllinux_1_1_aarch64.whl", hash = "sha256:5230a99e724a1bdbbf236a1b58d6e8504b912b0552721c7c6b8570925ee0ccde"},
    {file = "httptools-0.5.0-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:3a47a34f6015dd52c9eb629c0f5a8a5193e47bf2a12d9a3194d231eaf1bc451a"},
    {file = "httptools-0.5.0-cp310-cp310-win_amd64.whl", hash = "sha256:24bb4bb8ac3882f90aa95403a1cb48465de877e2d5298ad6ddcfdebec060787d"},
    {file = "httptools-0.5.0-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:e67d4f8734f8054d2c4858570cc4b233bf753f56e85217de4dfb2495904cf02e"},
    {file = "httptools-0.5.0-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:7e5eefc58d20e4c2da82c78d91b2906f1a947ef42bd668db05f4ab4201a99f49"},
    {file = "httptools-0.5.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0297822cea9f90a38df29f48e40b42ac3d48a28637368f3ec6d15eebefd182f9"},
    {file = "httptools-0.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:557be7fbf2bfa4a2ec65192c254e151684545ebab45eca5d50477d562c40f986"},
    {file = "httptools-0.5.0-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:54465401dbbec9a6a42cf737627fb0f014d50dc7365a6b6cd57753f151a86ff0"},
    {file = "httptools-0.5.0-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:4d9ebac23d2de960726ce45f49d70eb5466725c0087a078866043dad115f850f"},
    {file = "httptools-0.5.0-cp311-cp311-win_amd64.whl", hash = "sha256:e8a34e4c0ab7b1ca17b8763613783e2458e77938092c18ac919420ab8655c8c1"},
    {file = "httptools-0.5.0-cp36-cp36m-macosx_10_9_x86_64.whl", hash = "sha256:f659d7a48401158c59933904040085c200b4be631cb5f23a7d561fbae593ec1f"},
    {file = "httptools-0.5.0-cp36-cp36m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:ef1616b3ba965cd68e6f759eeb5d34fbf596a79e84215eeceebf34ba3f61fdc7"},
    {file = "httptools-0.5.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:3625a55886257755cb15194efbf209584754e31d336e09e2ffe0685a76cb4b60"},
    {file = "httptools-0.5.0-cp36-cp36m-musllinux_1_1_aarch64.whl", hash = "sha256:72ad589ba5e4a87e1d404cc1cb1b5780bfcb16e2aec957b88ce15fe879cc08ca"},
    {file = "httptools-0.5.0-cp36-cp36m-musllinux_1_1_x86_64.whl", hash = "sha256:850fec36c48df5a790aa735417dca8ce7d4b48d59b3ebd6f83e88a8125cde324"},
    {file = "httptools-0.5.0-cp36-cp36m-win_amd64.whl", hash = "sha256:f222e1e9d3f13b68ff8a835574eda02e67277d51631d69d7cf7f8e07df678c86"},
    {file = "httptools-0.5.0-cp37-cp37m-macosx_10_9_x86_64.whl", hash = "sha256:3cb8acf8f951363b617a8420768a9f249099b92e703c052f9a51b66342eea89b"},
    {file = "httptools-0.5.0-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:550059885dc9c19a072ca6d6735739d879be3b5959ec218ba3e013fd2255a11b"},
    {file = "httptools-0.5.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:a04fe458a4597aa559b79c7f48fe3dceabef0f69f562daf5c5e926b153817281"},
    {file = "httptools-0.5.0-cp37-cp37m-musllinux_1_1_aarch64.whl", hash = "sha256:7d0c1044bce274ec6711f0770fd2d5544fe392591d204c68328e60a46f88843b"},
    {file = "httptools-0.5.0-cp37-cp37m-musllinux_1_1_x86_64.whl", hash = "sha256:c6eeefd4435055a8ebb6c5cc36111b8591c192c56a95b45fe2af22d9881eee25"},
    {file = "httptools-0.5.0-cp37-cp37m-win_amd64.whl", hash = "sha256:5b65be160adcd9de7a7e6413a4966665756e263f0d5ddeffde277ffeee0576a5"},
    {file = "httptools-0.5.0-cp38-cp38-macosx_10_9_universal2.whl", hash = "sha256:fe9c766a0c35b7e3d6b6939393c8dfdd5da3ac5dec7f971ec9134f284c6c36d6"},
    {file = "httptools-0.5.0-cp38-cp38-macosx_10_9_x86_64.whl", hash = "sha256:85b392aba273566c3d5596a0a490978c085b79700814fb22bfd537d381dd230c"},
    {file = "httptools-0.5.0-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:f5e3088f4ed33947e16fd865b8200f9cfae1144f41b64a8cf19b599508e096bc"},
    {file = "httptools-0.5.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8c2a56b6aad7cc8f5551d8e04ff5a319d203f9d870398b94702300de50190f63"},
    {file = "httptools-0.5.0-cp38-cp38-musllinux_1_1_aarch64.whl", hash = "sha256:9b571b281a19762adb3f48a7731f6842f920fa71108aff9be49888320ac3e24d"},
    {file = "httptools-0.5.0-cp38-cp38-musllinux_1_1_x86_64.whl", hash = "sha256:aa47ffcf70ba6f7848349b8a6f9b481ee0f7637931d91a9860a1838bfc586901"},
    {file = "httptools-0.5.0-cp38-cp38-win_amd64.whl", hash = "sha256:bede7ee075e54b9a5bde695b4fc8f569f30185891796b2e4e09e2226801d09bd"},
    {file = "httptools-0.5.0-cp39-cp39-macosx_10_9_universal2.whl", hash = "sha256:64eba6f168803a7469866a9c9b5263a7463fa8b7a25b35e547492aa7322036b6"},
    {file = "httptools-0.5.0-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:4b098e4bb1174096a93f48f6193e7d9aa7071506a5877da09a783509ca5fff42"},
    {file = "httptools-0.5.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:9423a2de923820c7e82e18980b937893f4aa8251c43684fa1772e341f6e06887"},
    {file = "httptools-0.5.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:ca1b7becf7d9d3ccdbb2f038f665c0f4857e08e1d8481cbcc1a86a0afcfb62b2"},
    {file = "httptools-0.5.0-cp39-cp39-musllinux_1_1_aarch64.whl", hash = "sha256:50d4613025f15f4b11f1c54bbed4761c0020f7f921b95143ad6d58c151198142"},
    {file = "httptools-0.5.0-cp39-cp39-musllinux_1_1_x86_64.whl", hash = "sha256:8ffce9d81c825ac1deaa13bc9694c0562e2840a48ba21cfc9f3b4c922c16f372"},
    {file = "httptools-0.5.0-cp39-cp39-win_amd64.whl", hash = "sha256:1af91b3650ce518d226466f30bbba5b6376dbd3ddb1b2be8b0658c6799dd450b"},
    {file = "httptools-0.5.0.tar.gz", hash = "sha256:295874861c173f9101960bba332429bb77ed4dcd8cdf5cee9922eb00e4f6bc09"},
]

[package.extras]
test = ["Cython (>=0.29.24,<0.30.0)"]

[[package]]
name = "httpx"
version = "0.23.3"
description = "The next generation HTTP client."
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "httpx-0.23.3-py3-none-any.whl", hash = "sha256:a211fcce9b1254ea24f0cd6af9869b3d29aba40154e947d2a07bb499b3e310d6"},
    {file = "httpx-0.23.3.tar.gz", hash = "sha256:9818458eb565bb54898ccb9b8b251a28785dd4a55afbc23d0eb410754fe7d0f9"},
]

[package.dependencies]
certifi = "*"
h2 = {version = ">=3,<5", optional = true, markers = "extra == \"http2\""}
httpcore = ">=0.15.0,<0.17.0"
rfc3986 = {version = ">=1.3,<2", extras = ["idna2008"]}
sniffio = "*"

[package.extras]
brotli = ["brotli", "brotlicffi"]
cli = ["click (>=8.0.0,<9.0.0)", "pygments (>=2.0.0,<3.0.0)", "rich (>=10,<13)"]
http2 = ["h2 (>=3,<5)"]
socks = ["socksio (>=1.0.0,<2.0.0)"]

[[package]]
name = "humanfriendly"
version = "10.0"
description = "Human friendly output for text interfaces using Python"
category = "main"
optional = false
python-versions = ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*"
files = [
    {file = "humanfriendly-10.0-py2.py3-none-any.whl", hash = "sha256:1697e1a8a8f550fd43c2865cd84542fc175a61dcb779b6fee18cf6b6ccba1477"},
    {file = "humanfriendly-10.0.tar.gz", hash = "sha256:6b0b831ce8f15f7300721aa49829fc4e83921a9a301cc7f606be6686a2288ddc"},
]

[package.dependencies]
pyreadline3 = {version = "*", markers = "sys_platform == \"win32\" and python_version >= \"3.8\""}

[[package]]
name = "hyperframe"
version = "6.0.1"
description = "HTTP/2 framing layer for Python"
category = "main"
optional = false
python-versions = ">=3.6.1"
files = [
    {file = "hyperframe-6.0.1-py3-none-any.whl", hash = "sha256:0ec6bafd80d8ad2195c4f03aacba3a8265e57bc4cff261e802bf39970ed02a15"},
    {file = "hyperframe-6.0.1.tar.gz", hash = "sha256:ae510046231dc8e9ecb1a6586f63d2347bf4c8905914aa84ba585ae85f28a914"},
]

[[package]]
name = "idna"
version = "3.4"
description = "Internationalized Domain Names in Applications (IDNA)"
category = "main"
optional = false
python-versions = ">=3.5"
files = [
    {file = "idna-3.4-py3-none-any.whl", hash = "sha256:90b77e79eaa3eba6de819a0c442c0b4ceefc341a7a2ab77d7562bf49f425c5c2"},
    {file = "idna-3.4.tar.gz", hash = "sha256:814f528e8dead7d329833b91c5faa87d60bf71824cd12a7530b5526063d02cb4"},
]

[[package]]
name = "importlib-metadata"
version = "6.6.0"
description = "Read metadata from Python packages"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "importlib_metadata-6.6.0-py3-none-any.whl", hash = "sha256:43dd286a2cd8995d5eaef7fee2066340423b818ed3fd70adf0bad5f1fac53fed"},
    {file = "importlib_metadata-6.6.0.tar.gz", hash = "sha256:92501cdf9cc66ebd3e612f1b4f0c0765dfa42f0fa38ffb319b6bd84dd675d705"},
]

[package.dependencies]
zipp = ">=0.5"

[package.extras]
docs = ["furo", "jaraco.packaging (>=9)", "jaraco.tidelift (>=1.4)", "rst.linker (>=1.9)", "sphinx (>=3.5)", "sphinx-lint"]
perf = ["ipython"]
testing = ["flake8 (<5)", "flufl.flake8", "importlib-resources (>=1.3)", "packaging", "pyfakefs", "pytest (>=6)", "pytest-black (>=0.3.7)", "pytest-checkdocs (>=2.4)", "pytest-cov", "pytest-enabler (>=1.3)", "pytest-flake8", "pytest-mypy (>=0.9.1)", "pytest-perf (>=0.9.2)"]

[[package]]
name = "iniconfig"
version = "2.0.0"
description = "brain-dead simple config-ini parsing"
category = "dev"
optional = false
python-versions = ">=3.7"
files = [
    {file = "iniconfig-2.0.0-py3-none-any.whl", hash = "sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374"},
    {file = "iniconfig-2.0.0.tar.gz", hash = "sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3"},
]

[[package]]
name = "invoke"
version = "1.7.3"
description = "Pythonic task execution"
category = "main"
optional = false
python-versions = "*"
files = [
    {file = "invoke-1.7.3-py3-none-any.whl", hash = "sha256:d9694a865764dd3fd91f25f7e9a97fb41666e822bbb00e670091e3f43933574d"},
    {file = "invoke-1.7.3.tar.gz", hash = "sha256:41b428342d466a82135d5ab37119685a989713742be46e42a3a399d685579314"},
]

[[package]]
name = "isodate"
version = "0.6.1"
description = "An ISO 8601 date/time/duration parser and formatter"
category = "main"
optional = false
python-versions = "*"
files = [
    {file = "isodate-0.6.1-py2.py3-none-any.whl", hash = "sha256:0751eece944162659049d35f4f549ed815792b38793f07cf73381c1c87cbed96"},
    {file = "isodate-0.6.1.tar.gz", hash = "sha256:48c5881de7e8b0a0d648cb024c8062dc84e7b840ed81e864c7614fd3c127bde9"},
]

[package.dependencies]
six = "*"

[[package]]
name = "jaraco-classes"
version = "3.2.3"
description = "Utility functions for Python class constructs"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "jaraco.classes-3.2.3-py3-none-any.whl", hash = "sha256:2353de3288bc6b82120752201c6b1c1a14b058267fa424ed5ce5984e3b922158"},
    {file = "jaraco.classes-3.2.3.tar.gz", hash = "sha256:89559fa5c1d3c34eff6f631ad80bb21f378dbcbb35dd161fd2c6b93f5be2f98a"},
]

[package.dependencies]
more-itertools = "*"

[package.extras]
docs = ["jaraco.packaging (>=9)", "jaraco.tidelift (>=1.4)", "rst.linker (>=1.9)", "sphinx (>=3.5)"]
testing = ["flake8 (<5)", "pytest (>=6)", "pytest-black (>=0.3.7)", "pytest-checkdocs (>=2.4)", "pytest-cov", "pytest-enabler (>=1.3)", "pytest-flake8", "pytest-mypy (>=0.9.1)"]

[[package]]
name = "jeepney"
version = "0.8.0"
description = "Low-level, pure Python DBus protocol wrapper."
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "jeepney-0.8.0-py3-none-any.whl", hash = "sha256:c0a454ad016ca575060802ee4d590dd912e35c122fa04e70306de3d076cce755"},
    {file = "jeepney-0.8.0.tar.gz", hash = "sha256:5efe48d255973902f6badc3ce55e2aa6c5c3b3bc642059ef3a91247bcfcc5806"},
]

[package.extras]
test = ["async-timeout", "pytest", "pytest-asyncio (>=0.17)", "pytest-trio", "testpath", "trio"]
trio = ["async_generator", "trio"]

[[package]]
name = "keyring"
version = "23.13.1"
description = "Store and access your passwords safely."
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "keyring-23.13.1-py3-none-any.whl", hash = "sha256:771ed2a91909389ed6148631de678f82ddc73737d85a927f382a8a1b157898cd"},
    {file = "keyring-23.13.1.tar.gz", hash = "sha256:ba2e15a9b35e21908d0aaf4e0a47acc52d6ae33444df0da2b49d41a46ef6d678"},
]

[package.dependencies]
importlib-metadata = {version = ">=4.11.4", markers = "python_version < \"3.12\""}
"jaraco.classes" = "*"
jeepney = {version = ">=0.4.2", markers = "sys_platform == \"linux\""}
pywin32-ctypes = {version = ">=0.2.0", markers = "sys_platform == \"win32\""}
SecretStorage = {version = ">=3.2", markers = "sys_platform == \"linux\""}

[package.extras]
completion = ["shtab"]
docs = ["furo", "jaraco.packaging (>=9)", "jaraco.tidelift (>=1.4)", "rst.linker (>=1.9)", "sphinx (>=3.5)"]
testing = ["flake8 (<5)", "pytest (>=6)", "pytest-black (>=0.3.7)", "pytest-checkdocs (>=2.4)", "pytest-cov", "pytest-enabler (>=1.3)", "pytest-flake8", "pytest-mypy (>=0.9.1)"]

[[package]]
name = "langchain"
version = "0.0.187"
description = "Building applications with LLMs through composability"
category = "main"
optional = false
python-versions = ">=3.8.1,<4.0"
files = [
    {file = "langchain-0.0.187-py3-none-any.whl", hash = "sha256:33c6cdbc99e6dda6f1cfc3bb4a8b099a7631e1a4e4c2f3121e0d6fac880d9b3b"},
    {file = "langchain-0.0.187.tar.gz", hash = "sha256:acd840358d0b47c60291c517a7affe9c2db70b70a5c8b2e5edc744889facb672"},
]

[package.dependencies]
aiohttp = ">=3.8.3,<4.0.0"
async-timeout = {version = ">=4.0.0,<5.0.0", markers = "python_version < \"3.11\""}
dataclasses-json = ">=0.5.7,<0.6.0"
numexpr = ">=2.8.4,<3.0.0"
numpy = ">=1,<2"
openapi-schema-pydantic = ">=1.2,<2.0"
pydantic = ">=1,<2"
PyYAML = ">=5.4.1"
requests = ">=2,<3"
SQLAlchemy = ">=1.4,<3"
tenacity = ">=8.1.0,<9.0.0"

[package.extras]
all = ["O365 (>=2.0.26,<3.0.0)", "aleph-alpha-client (>=2.15.0,<3.0.0)", "anthropic (>=0.2.6,<0.3.0)", "arxiv (>=1.4,<2.0)", "atlassian-python-api (>=3.36.0,<4.0.0)", "azure-ai-formrecognizer (>=3.2.1,<4.0.0)", "azure-ai-vision (>=0.11.1b1,<0.12.0)", "azure-cognitiveservices-speech (>=1.28.0,<2.0.0)", "azure-cosmos (>=4.4.0b1,<5.0.0)", "azure-identity (>=1.12.0,<2.0.0)", "beautifulsoup4 (>=4,<5)", "clickhouse-connect (>=0.5.14,<0.6.0)", "cohere (>=3,<4)", "deeplake (>=3.3.0,<4.0.0)", "docarray[hnswlib] (>=0.32.0,<0.33.0)", "duckduckgo-search (>=2.8.6,<3.0.0)", "elasticsearch (>=8,<9)", "faiss-cpu (>=1,<2)", "google-api-python-client (==2.70.0)", "google-auth (>=2.18.1,<3.0.0)", "google-search-results (>=2,<3)", "gptcache (>=0.1.7)", "html2text (>=2020.1.16,<2021.0.0)", "huggingface_hub (>=0,<1)", "jina (>=3.14,<4.0)", "jinja2 (>=3,<4)", "jq (>=1.4.1,<2.0.0)", "lancedb (>=0.1,<0.2)", "langkit (>=0.0.1.dev3,<0.1.0)", "lark (>=1.1.5,<2.0.0)", "lxml (>=4.9.2,<5.0.0)", "manifest-ml (>=0.0.1,<0.0.2)", "momento (>=1.5.0,<2.0.0)", "neo4j (>=5.8.1,<6.0.0)", "networkx (>=2.6.3,<3.0.0)", "nlpcloud (>=1,<2)", "nltk (>=3,<4)", "nomic (>=1.0.43,<2.0.0)", "openai (>=0,<1)", "openlm (>=0.0.5,<0.0.6)", "opensearch-py (>=2.0.0,<3.0.0)", "pdfminer-six (>=20221105,<20221106)", "pexpect (>=4.8.0,<5.0.0)", "pgvector (>=0.1.6,<0.2.0)", "pinecone-client (>=2,<3)", "pinecone-text (>=0.4.2,<0.5.0)", "psycopg2-binary (>=2.9.5,<3.0.0)", "pymongo (>=4.3.3,<5.0.0)", "pyowm (>=3.3.0,<4.0.0)", "pypdf (>=3.4.0,<4.0.0)", "pytesseract (>=0.3.10,<0.4.0)", "pyvespa (>=0.33.0,<0.34.0)", "qdrant-client (>=1.1.2,<2.0.0)", "redis (>=4,<5)", "requests-toolbelt (>=1.0.0,<2.0.0)", "sentence-transformers (>=2,<3)", "spacy (>=3,<4)", "steamship (>=2.16.9,<3.0.0)", "tensorflow-text (>=2.11.0,<3.0.0)", "tiktoken (>=0.3.2,<0.4.0)", "torch (>=1,<3)", "transformers (>=4,<5)", "weaviate-client (>=3,<4)", "wikipedia (>=1,<2)", "wolframalpha (==5.0.0)"]
azure = ["azure-ai-formrecognizer (>=3.2.1,<4.0.0)", "azure-ai-vision (>=0.11.1b1,<0.12.0)", "azure-cognitiveservices-speech (>=1.28.0,<2.0.0)", "azure-core (>=1.26.4,<2.0.0)", "azure-cosmos (>=4.4.0b1,<5.0.0)", "azure-identity (>=1.12.0,<2.0.0)", "openai (>=0,<1)"]
cohere = ["cohere (>=3,<4)"]
docarray = ["docarray[hnswlib] (>=0.32.0,<0.33.0)"]
embeddings = ["sentence-transformers (>=2,<3)"]
extended-testing = ["atlassian-python-api (>=3.36.0,<4.0.0)", "beautifulsoup4 (>=4,<5)", "bibtexparser (>=1.4.0,<2.0.0)", "chardet (>=5.1.0,<6.0.0)", "gql (>=3.4.1,<4.0.0)", "html2text (>=2020.1.16,<2021.0.0)", "jq (>=1.4.1,<2.0.0)", "lxml (>=4.9.2,<5.0.0)", "pandas (>=2.0.1,<3.0.0)", "pdfminer-six (>=20221105,<20221106)", "psychicapi (>=0.5,<0.6)", "py-trello (>=0.19.0,<0.20.0)", "pymupdf (>=1.22.3,<2.0.0)", "pypdf (>=3.4.0,<4.0.0)", "pypdfium2 (>=4.10.0,<5.0.0)", "pyspark (>=3.4.0,<4.0.0)", "requests-toolbelt (>=1.0.0,<2.0.0)", "scikit-learn (>=1.2.2,<2.0.0)", "telethon (>=1.28.5,<2.0.0)", "tqdm (>=4.48.0)", "zep-python (>=0.30,<0.31)"]
llms = ["anthropic (>=0.2.6,<0.3.0)", "cohere (>=3,<4)", "huggingface_hub (>=0,<1)", "manifest-ml (>=0.0.1,<0.0.2)", "nlpcloud (>=1,<2)", "openai (>=0,<1)", "openlm (>=0.0.5,<0.0.6)", "torch (>=1,<3)", "transformers (>=4,<5)"]
openai = ["openai (>=0,<1)", "tiktoken (>=0.3.2,<0.4.0)"]
qdrant = ["qdrant-client (>=1.1.2,<2.0.0)"]
text-helpers = ["chardet (>=5.1.0,<6.0.0)"]

[[package]]
name = "llama-index"
version = "0.5.4"
description = "Interface between LLMs and your data."
category = "main"
optional = false
python-versions = "*"
files = [
    {file = "llama_index-0.5.4.tar.gz", hash = "sha256:f6c556370f7f03039bc19be4ca594b00f38d7cea18399f6045bc776fa3a90d15"},
]

[package.dependencies]
dataclasses_json = "*"
langchain = "*"
numpy = "*"
openai = ">=0.26.4"
pandas = "*"
tenacity = ">=8.2.0,<9.0.0"
tiktoken = "*"

[[package]]
name = "loguru"
version = "0.7.0"
description = "Python logging made (stupidly) simple"
category = "main"
optional = false
python-versions = ">=3.5"
files = [
    {file = "loguru-0.7.0-py3-none-any.whl", hash = "sha256:b93aa30099fa6860d4727f1b81f8718e965bb96253fa190fab2077aaad6d15d3"},
    {file = "loguru-0.7.0.tar.gz", hash = "sha256:1612053ced6ae84d7959dd7d5e431a0532642237ec21f7fd83ac73fe539e03e1"},
]

[package.dependencies]
colorama = {version = ">=0.3.4", markers = "sys_platform == \"win32\""}
win32-setctime = {version = ">=1.0.0", markers = "sys_platform == \"win32\""}

[package.extras]
dev = ["Sphinx (==5.3.0)", "colorama (==0.4.5)", "colorama (==0.4.6)", "freezegun (==1.1.0)", "freezegun (==1.2.2)", "mypy (==v0.910)", "mypy (==v0.971)", "mypy (==v0.990)", "pre-commit (==3.2.1)", "pytest (==6.1.2)", "pytest (==7.2.1)", "pytest-cov (==2.12.1)", "pytest-cov (==4.0.0)", "pytest-mypy-plugins (==1.10.1)", "pytest-mypy-plugins (==1.9.3)", "sphinx-autobuild (==2021.3.14)", "sphinx-rtd-theme (==1.2.0)", "tox (==3.27.1)", "tox (==4.4.6)"]

[[package]]
name = "lxml"
version = "4.9.2"
description = "Powerful and Pythonic XML processing library combining libxml2/libxslt with the ElementTree API."
category = "main"
optional = false
python-versions = ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, != 3.4.*"
files = [
    {file = "lxml-4.9.2-cp27-cp27m-macosx_10_15_x86_64.whl", hash = "sha256:76cf573e5a365e790396a5cc2b909812633409306c6531a6877c59061e42c4f2"},
    {file = "lxml-4.9.2-cp27-cp27m-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:b1f42b6921d0e81b1bcb5e395bc091a70f41c4d4e55ba99c6da2b31626c44892"},
    {file = "lxml-4.9.2-cp27-cp27m-manylinux_2_5_x86_64.manylinux1_x86_64.whl", hash = "sha256:9f102706d0ca011de571de32c3247c6476b55bb6bc65a20f682f000b07a4852a"},
    {file = "lxml-4.9.2-cp27-cp27m-win32.whl", hash = "sha256:8d0b4612b66ff5d62d03bcaa043bb018f74dfea51184e53f067e6fdcba4bd8de"},
    {file = "lxml-4.9.2-cp27-cp27m-win_amd64.whl", hash = "sha256:4c8f293f14abc8fd3e8e01c5bd86e6ed0b6ef71936ded5bf10fe7a5efefbaca3"},
    {file = "lxml-4.9.2-cp27-cp27mu-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:2899456259589aa38bfb018c364d6ae7b53c5c22d8e27d0ec7609c2a1ff78b50"},
    {file = "lxml-4.9.2-cp27-cp27mu-manylinux_2_5_x86_64.manylinux1_x86_64.whl", hash = "sha256:6749649eecd6a9871cae297bffa4ee76f90b4504a2a2ab528d9ebe912b101975"},
    {file = "lxml-4.9.2-cp310-cp310-macosx_10_15_x86_64.whl", hash = "sha256:a08cff61517ee26cb56f1e949cca38caabe9ea9fbb4b1e10a805dc39844b7d5c"},
    {file = "lxml-4.9.2-cp310-cp310-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_24_i686.whl", hash = "sha256:85cabf64adec449132e55616e7ca3e1000ab449d1d0f9d7f83146ed5bdcb6d8a"},
    {file = "lxml-4.9.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.manylinux_2_24_aarch64.whl", hash = "sha256:8340225bd5e7a701c0fa98284c849c9b9fc9238abf53a0ebd90900f25d39a4e4"},
    {file = "lxml-4.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl", hash = "sha256:1ab8f1f932e8f82355e75dda5413a57612c6ea448069d4fb2e217e9a4bed13d4"},
    {file = "lxml-4.9.2-cp310-cp310-musllinux_1_1_aarch64.whl", hash = "sha256:699a9af7dffaf67deeae27b2112aa06b41c370d5e7633e0ee0aea2e0b6c211f7"},
    {file = "lxml-4.9.2-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:b9cc34af337a97d470040f99ba4282f6e6bac88407d021688a5d585e44a23184"},
    {file = "lxml-4.9.2-cp310-cp310-win32.whl", hash = "sha256:d02a5399126a53492415d4906ab0ad0375a5456cc05c3fc0fc4ca11771745cda"},
    {file = "lxml-4.9.2-cp310-cp310-win_amd64.whl", hash = "sha256:a38486985ca49cfa574a507e7a2215c0c780fd1778bb6290c21193b7211702ab"},
    {file = "lxml-4.9.2-cp311-cp311-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_24_i686.whl", hash = "sha256:c83203addf554215463b59f6399835201999b5e48019dc17f182ed5ad87205c9"},
    {file = "lxml-4.9.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.manylinux_2_24_aarch64.whl", hash = "sha256:2a87fa548561d2f4643c99cd13131acb607ddabb70682dcf1dff5f71f781a4bf"},
    {file = "lxml-4.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl", hash = "sha256:d6b430a9938a5a5d85fc107d852262ddcd48602c120e3dbb02137c83d212b380"},
    {file = "lxml-4.9.2-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:3efea981d956a6f7173b4659849f55081867cf897e719f57383698af6f618a92"},
    {file = "lxml-4.9.2-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:df0623dcf9668ad0445e0558a21211d4e9a149ea8f5666917c8eeec515f0a6d1"},
    {file = "lxml-4.9.2-cp311-cp311-win32.whl", hash = "sha256:da248f93f0418a9e9d94b0080d7ebc407a9a5e6d0b57bb30db9b5cc28de1ad33"},
    {file = "lxml-4.9.2-cp311-cp311-win_amd64.whl", hash = "sha256:3818b8e2c4b5148567e1b09ce739006acfaa44ce3156f8cbbc11062994b8e8dd"},
    {file = "lxml-4.9.2-cp35-cp35m-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:ca989b91cf3a3ba28930a9fc1e9aeafc2a395448641df1f387a2d394638943b0"},
    {file = "lxml-4.9.2-cp35-cp35m-manylinux_2_5_x86_64.manylinux1_x86_64.whl", hash = "sha256:822068f85e12a6e292803e112ab876bc03ed1f03dddb80154c395f891ca6b31e"},
    {file = "lxml-4.9.2-cp35-cp35m-win32.whl", hash = "sha256:be7292c55101e22f2a3d4d8913944cbea71eea90792bf914add27454a13905df"},
    {file = "lxml-4.9.2-cp35-cp35m-win_amd64.whl", hash = "sha256:998c7c41910666d2976928c38ea96a70d1aa43be6fe502f21a651e17483a43c5"},
    {file = "lxml-4.9.2-cp36-cp36m-macosx_10_15_x86_64.whl", hash = "sha256:b26a29f0b7fc6f0897f043ca366142d2b609dc60756ee6e4e90b5f762c6adc53"},
    {file = "lxml-4.9.2-cp36-cp36m-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_24_i686.whl", hash = "sha256:ab323679b8b3030000f2be63e22cdeea5b47ee0abd2d6a1dc0c8103ddaa56cd7"},
    {file = "lxml-4.9.2-cp36-cp36m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:689bb688a1db722485e4610a503e3e9210dcc20c520b45ac8f7533c837be76fe"},
    {file = "lxml-4.9.2-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl", hash = "sha256:f49e52d174375a7def9915c9f06ec4e569d235ad428f70751765f48d5926678c"},
    {file = "lxml-4.9.2-cp36-cp36m-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:36c3c175d34652a35475a73762b545f4527aec044910a651d2bf50de9c3352b1"},
    {file = "lxml-4.9.2-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.whl", hash = "sha256:a35f8b7fa99f90dd2f5dc5a9fa12332642f087a7641289ca6c40d6e1a2637d8e"},
    {file = "lxml-4.9.2-cp36-cp36m-musllinux_1_1_aarch64.whl", hash = "sha256:58bfa3aa19ca4c0f28c5dde0ff56c520fbac6f0daf4fac66ed4c8d2fb7f22e74"},
    {file = "lxml-4.9.2-cp36-cp36m-musllinux_1_1_x86_64.whl", hash = "sha256:bc718cd47b765e790eecb74d044cc8d37d58562f6c314ee9484df26276d36a38"},
    {file = "lxml-4.9.2-cp36-cp36m-win32.whl", hash = "sha256:d5bf6545cd27aaa8a13033ce56354ed9e25ab0e4ac3b5392b763d8d04b08e0c5"},
    {file = "lxml-4.9.2-cp36-cp36m-win_amd64.whl", hash = "sha256:3ab9fa9d6dc2a7f29d7affdf3edebf6ece6fb28a6d80b14c3b2fb9d39b9322c3"},
    {file = "lxml-4.9.2-cp37-cp37m-macosx_10_15_x86_64.whl", hash = "sha256:05ca3f6abf5cf78fe053da9b1166e062ade3fa5d4f92b4ed688127ea7d7b1d03"},
    {file = "lxml-4.9.2-cp37-cp37m-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_24_i686.whl", hash = "sha256:a5da296eb617d18e497bcf0a5c528f5d3b18dadb3619fbdadf4ed2356ef8d941"},
    {file = "lxml-4.9.2-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.manylinux_2_24_aarch64.whl", hash = "sha256:04876580c050a8c5341d706dd464ff04fd597095cc8c023252566a8826505726"},
    {file = "lxml-4.9.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl", hash = "sha256:c9ec3eaf616d67db0764b3bb983962b4f385a1f08304fd30c7283954e6a7869b"},
    {file = "lxml-4.9.2-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:2a29ba94d065945944016b6b74e538bdb1751a1db6ffb80c9d3c2e40d6fa9894"},
    {file = "lxml-4.9.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl", hash = "sha256:a82d05da00a58b8e4c0008edbc8a4b6ec5a4bc1e2ee0fb6ed157cf634ed7fa45"},
    {file = "lxml-4.9.2-cp37-cp37m-musllinux_1_1_aarch64.whl", hash = "sha256:223f4232855ade399bd409331e6ca70fb5578efef22cf4069a6090acc0f53c0e"},
    {file = "lxml-4.9.2-cp37-cp37m-musllinux_1_1_x86_64.whl", hash = "sha256:d17bc7c2ccf49c478c5bdd447594e82692c74222698cfc9b5daae7ae7e90743b"},
    {file = "lxml-4.9.2-cp37-cp37m-win32.whl", hash = "sha256:b64d891da92e232c36976c80ed7ebb383e3f148489796d8d31a5b6a677825efe"},
    {file = "lxml-4.9.2-cp37-cp37m-win_amd64.whl", hash = "sha256:a0a336d6d3e8b234a3aae3c674873d8f0e720b76bc1d9416866c41cd9500ffb9"},
    {file = "lxml-4.9.2-cp38-cp38-macosx_10_15_x86_64.whl", hash = "sha256:da4dd7c9c50c059aba52b3524f84d7de956f7fef88f0bafcf4ad7dde94a064e8"},
    {file = "lxml-4.9.2-cp38-cp38-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_24_i686.whl", hash = "sha256:821b7f59b99551c69c85a6039c65b75f5683bdc63270fec660f75da67469ca24"},
    {file = "lxml-4.9.2-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.manylinux_2_24_aarch64.whl", hash = "sha256:e5168986b90a8d1f2f9dc1b841467c74221bd752537b99761a93d2d981e04889"},
    {file = "lxml-4.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl", hash = "sha256:8e20cb5a47247e383cf4ff523205060991021233ebd6f924bca927fcf25cf86f"},
    {file = "lxml-4.9.2-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:13598ecfbd2e86ea7ae45ec28a2a54fb87ee9b9fdb0f6d343297d8e548392c03"},
    {file = "lxml-4.9.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl", hash = "sha256:880bbbcbe2fca64e2f4d8e04db47bcdf504936fa2b33933efd945e1b429bea8c"},
    {file = "lxml-4.9.2-cp38-cp38-musllinux_1_1_aarch64.whl", hash = "sha256:7d2278d59425777cfcb19735018d897ca8303abe67cc735f9f97177ceff8027f"},
    {file = "lxml-4.9.2-cp38-cp38-musllinux_1_1_x86_64.whl", hash = "sha256:5344a43228767f53a9df6e5b253f8cdca7dfc7b7aeae52551958192f56d98457"},
    {file = "lxml-4.9.2-cp38-cp38-win32.whl", hash = "sha256:925073b2fe14ab9b87e73f9a5fde6ce6392da430f3004d8b72cc86f746f5163b"},
    {file = "lxml-4.9.2-cp38-cp38-win_amd64.whl", hash = "sha256:9b22c5c66f67ae00c0199f6055705bc3eb3fcb08d03d2ec4059a2b1b25ed48d7"},
    {file = "lxml-4.9.2-cp39-cp39-macosx_10_15_x86_64.whl", hash = "sha256:5f50a1c177e2fa3ee0667a5ab79fdc6b23086bc8b589d90b93b4bd17eb0e64d1"},
    {file = "lxml-4.9.2-cp39-cp39-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_24_i686.whl", hash = "sha256:090c6543d3696cbe15b4ac6e175e576bcc3f1ccfbba970061b7300b0c15a2140"},
    {file = "lxml-4.9.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.manylinux_2_24_aarch64.whl", hash = "sha256:63da2ccc0857c311d764e7d3d90f429c252e83b52d1f8f1d1fe55be26827d1f4"},
    {file = "lxml-4.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl", hash = "sha256:5b4545b8a40478183ac06c073e81a5ce4cf01bf1734962577cf2bb569a5b3bbf"},
    {file = "lxml-4.9.2-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:2e430cd2824f05f2d4f687701144556646bae8f249fd60aa1e4c768ba7018947"},
    {file = "lxml-4.9.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl", hash = "sha256:6804daeb7ef69e7b36f76caddb85cccd63d0c56dedb47555d2fc969e2af6a1a5"},
    {file = "lxml-4.9.2-cp39-cp39-musllinux_1_1_aarch64.whl", hash = "sha256:a6e441a86553c310258aca15d1c05903aaf4965b23f3bc2d55f200804e005ee5"},
    {file = "lxml-4.9.2-cp39-cp39-musllinux_1_1_x86_64.whl", hash = "sha256:ca34efc80a29351897e18888c71c6aca4a359247c87e0b1c7ada14f0ab0c0fb2"},
    {file = "lxml-4.9.2-cp39-cp39-win32.whl", hash = "sha256:6b418afe5df18233fc6b6093deb82a32895b6bb0b1155c2cdb05203f583053f1"},
    {file = "lxml-4.9.2-cp39-cp39-win_amd64.whl", hash = "sha256:f1496ea22ca2c830cbcbd473de8f114a320da308438ae65abad6bab7867fe38f"},
    {file = "lxml-4.9.2-pp37-pypy37_pp73-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_24_i686.whl", hash = "sha256:b264171e3143d842ded311b7dccd46ff9ef34247129ff5bf5066123c55c2431c"},
    {file = "lxml-4.9.2-pp37-pypy37_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl", hash = "sha256:0dc313ef231edf866912e9d8f5a042ddab56c752619e92dfd3a2c277e6a7299a"},
    {file = "lxml-4.9.2-pp38-pypy38_pp73-macosx_10_15_x86_64.whl", hash = "sha256:16efd54337136e8cd72fb9485c368d91d77a47ee2d42b057564aae201257d419"},
    {file = "lxml-4.9.2-pp38-pypy38_pp73-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_24_i686.whl", hash = "sha256:0f2b1e0d79180f344ff9f321327b005ca043a50ece8713de61d1cb383fb8ac05"},
    {file = "lxml-4.9.2-pp38-pypy38_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl", hash = "sha256:7b770ed79542ed52c519119473898198761d78beb24b107acf3ad65deae61f1f"},
    {file = "lxml-4.9.2-pp38-pypy38_pp73-win_amd64.whl", hash = "sha256:efa29c2fe6b4fdd32e8ef81c1528506895eca86e1d8c4657fda04c9b3786ddf9"},
    {file = "lxml-4.9.2-pp39-pypy39_pp73-macosx_10_15_x86_64.whl", hash = "sha256:7e91ee82f4199af8c43d8158024cbdff3d931df350252288f0d4ce656df7f3b5"},
    {file = "lxml-4.9.2-pp39-pypy39_pp73-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_24_i686.whl", hash = "sha256:b23e19989c355ca854276178a0463951a653309fb8e57ce674497f2d9f208746"},
    {file = "lxml-4.9.2-pp39-pypy39_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl", hash = "sha256:01d36c05f4afb8f7c20fd9ed5badca32a2029b93b1750f571ccc0b142531caf7"},
    {file = "lxml-4.9.2-pp39-pypy39_pp73-win_amd64.whl", hash = "sha256:7b515674acfdcadb0eb5d00d8a709868173acece5cb0be3dd165950cbfdf5409"},
    {file = "lxml-4.9.2.tar.gz", hash = "sha256:2455cfaeb7ac70338b3257f41e21f0724f4b5b0c0e7702da67ee6c3640835b67"},
]

[package.extras]
cssselect = ["cssselect (>=0.7)"]
html5 = ["html5lib"]
htmlsoup = ["BeautifulSoup4"]
source = ["Cython (>=0.29.7)"]

[[package]]
name = "lz4"
version = "4.3.2"
description = "LZ4 Bindings for Python"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "lz4-4.3.2-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:1c4c100d99eed7c08d4e8852dd11e7d1ec47a3340f49e3a96f8dfbba17ffb300"},
    {file = "lz4-4.3.2-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:edd8987d8415b5dad25e797043936d91535017237f72fa456601be1479386c92"},
    {file = "lz4-4.3.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:f7c50542b4ddceb74ab4f8b3435327a0861f06257ca501d59067a6a482535a77"},
    {file = "lz4-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:0f5614d8229b33d4a97cb527db2a1ac81308c6e796e7bdb5d1309127289f69d5"},
    {file = "lz4-4.3.2-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:8f00a9ba98f6364cadda366ae6469b7b3568c0cced27e16a47ddf6b774169270"},
    {file = "lz4-4.3.2-cp310-cp310-win32.whl", hash = "sha256:b10b77dc2e6b1daa2f11e241141ab8285c42b4ed13a8642495620416279cc5b2"},
    {file = "lz4-4.3.2-cp310-cp310-win_amd64.whl", hash = "sha256:86480f14a188c37cb1416cdabacfb4e42f7a5eab20a737dac9c4b1c227f3b822"},
    {file = "lz4-4.3.2-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:7c2df117def1589fba1327dceee51c5c2176a2b5a7040b45e84185ce0c08b6a3"},
    {file = "lz4-4.3.2-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:1f25eb322eeb24068bb7647cae2b0732b71e5c639e4e4026db57618dcd8279f0"},
    {file = "lz4-4.3.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:8df16c9a2377bdc01e01e6de5a6e4bbc66ddf007a6b045688e285d7d9d61d1c9"},
    {file = "lz4-4.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:f571eab7fec554d3b1db0d666bdc2ad85c81f4b8cb08906c4c59a8cad75e6e22"},
    {file = "lz4-4.3.2-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:7211dc8f636ca625abc3d4fb9ab74e5444b92df4f8d58ec83c8868a2b0ff643d"},
    {file = "lz4-4.3.2-cp311-cp311-win32.whl", hash = "sha256:867664d9ca9bdfce840ac96d46cd8838c9ae891e859eb98ce82fcdf0e103a947"},
    {file = "lz4-4.3.2-cp311-cp311-win_amd64.whl", hash = "sha256:a6a46889325fd60b8a6b62ffc61588ec500a1883db32cddee9903edfba0b7584"},
    {file = "lz4-4.3.2-cp37-cp37m-macosx_10_9_x86_64.whl", hash = "sha256:3a85b430138882f82f354135b98c320dafb96fc8fe4656573d95ab05de9eb092"},
    {file = "lz4-4.3.2-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:65d5c93f8badacfa0456b660285e394e65023ef8071142e0dcbd4762166e1be0"},
    {file = "lz4-4.3.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6b50f096a6a25f3b2edca05aa626ce39979d63c3b160687c8c6d50ac3943d0ba"},
    {file = "lz4-4.3.2-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:200d05777d61ba1ff8d29cb51c534a162ea0b4fe6d3c28be3571a0a48ff36080"},
    {file = "lz4-4.3.2-cp37-cp37m-win32.whl", hash = "sha256:edc2fb3463d5d9338ccf13eb512aab61937be50aa70734bcf873f2f493801d3b"},
    {file = "lz4-4.3.2-cp37-cp37m-win_amd64.whl", hash = "sha256:83acfacab3a1a7ab9694333bcb7950fbeb0be21660d236fd09c8337a50817897"},
    {file = "lz4-4.3.2-cp38-cp38-macosx_10_9_x86_64.whl", hash = "sha256:7a9eec24ec7d8c99aab54de91b4a5a149559ed5b3097cf30249b665689b3d402"},
    {file = "lz4-4.3.2-cp38-cp38-macosx_11_0_arm64.whl", hash = "sha256:31d72731c4ac6ebdce57cd9a5cabe0aecba229c4f31ba3e2c64ae52eee3fdb1c"},
    {file = "lz4-4.3.2-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:83903fe6db92db0be101acedc677aa41a490b561567fe1b3fe68695b2110326c"},
    {file = "lz4-4.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:926b26db87ec8822cf1870efc3d04d06062730ec3279bbbd33ba47a6c0a5c673"},
    {file = "lz4-4.3.2-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:e05afefc4529e97c08e65ef92432e5f5225c0bb21ad89dee1e06a882f91d7f5e"},
    {file = "lz4-4.3.2-cp38-cp38-win32.whl", hash = "sha256:ad38dc6a7eea6f6b8b642aaa0683253288b0460b70cab3216838747163fb774d"},
    {file = "lz4-4.3.2-cp38-cp38-win_amd64.whl", hash = "sha256:7e2dc1bd88b60fa09b9b37f08553f45dc2b770c52a5996ea52b2b40f25445676"},
    {file = "lz4-4.3.2-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:edda4fb109439b7f3f58ed6bede59694bc631c4b69c041112b1b7dc727fffb23"},
    {file = "lz4-4.3.2-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:0ca83a623c449295bafad745dcd399cea4c55b16b13ed8cfea30963b004016c9"},
    {file = "lz4-4.3.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d5ea0e788dc7e2311989b78cae7accf75a580827b4d96bbaf06c7e5a03989bd5"},
    {file = "lz4-4.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:a98b61e504fb69f99117b188e60b71e3c94469295571492a6468c1acd63c37ba"},
    {file = "lz4-4.3.2-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:4931ab28a0d1c133104613e74eec1b8bb1f52403faabe4f47f93008785c0b929"},
    {file = "lz4-4.3.2-cp39-cp39-win32.whl", hash = "sha256:ec6755cacf83f0c5588d28abb40a1ac1643f2ff2115481089264c7630236618a"},
    {file = "lz4-4.3.2-cp39-cp39-win_amd64.whl", hash = "sha256:4caedeb19e3ede6c7a178968b800f910db6503cb4cb1e9cc9221157572139b49"},
    {file = "lz4-4.3.2.tar.gz", hash = "sha256:e1431d84a9cfb23e6773e72078ce8e65cad6745816d4cbf9ae67da5ea419acda"},
]

[package.extras]
docs = ["sphinx (>=1.6.0)", "sphinx-bootstrap-theme"]
flake8 = ["flake8"]
tests = ["psutil", "pytest (!=3.3.0)", "pytest-cov"]

[[package]]
name = "marshmallow"
version = "3.19.0"
description = "A lightweight library for converting complex datatypes to and from native Python datatypes."
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "marshmallow-3.19.0-py3-none-any.whl", hash = "sha256:93f0958568da045b0021ec6aeb7ac37c81bfcccbb9a0e7ed8559885070b3a19b"},
    {file = "marshmallow-3.19.0.tar.gz", hash = "sha256:90032c0fd650ce94b6ec6dc8dfeb0e3ff50c144586462c389b81a07205bedb78"},
]

[package.dependencies]
packaging = ">=17.0"

[package.extras]
dev = ["flake8 (==5.0.4)", "flake8-bugbear (==22.10.25)", "mypy (==0.990)", "pre-commit (>=2.4,<3.0)", "pytest", "pytz", "simplejson", "tox"]
docs = ["alabaster (==0.7.12)", "autodocsumm (==0.2.9)", "sphinx (==5.3.0)", "sphinx-issues (==3.0.1)", "sphinx-version-warning (==1.1.2)"]
lint = ["flake8 (==5.0.4)", "flake8-bugbear (==22.10.25)", "mypy (==0.990)", "pre-commit (>=2.4,<3.0)"]
tests = ["pytest", "pytz", "simplejson"]

[[package]]
name = "marshmallow-enum"
version = "1.5.1"
description = "Enum field for Marshmallow"
category = "main"
optional = false
python-versions = "*"
files = [
    {file = "marshmallow-enum-1.5.1.tar.gz", hash = "sha256:38e697e11f45a8e64b4a1e664000897c659b60aa57bfa18d44e226a9920b6e58"},
    {file = "marshmallow_enum-1.5.1-py2.py3-none-any.whl", hash = "sha256:57161ab3dbfde4f57adeb12090f39592e992b9c86d206d02f6bd03ebec60f072"},
]

[package.dependencies]
marshmallow = ">=2.0.0"

[[package]]
name = "monotonic"
version = "1.6"
description = "An implementation of time.monotonic() for Python 2 & < 3.3"
category = "main"
optional = false
python-versions = "*"
files = [
    {file = "monotonic-1.6-py2.py3-none-any.whl", hash = "sha256:68687e19a14f11f26d140dd5c86f3dba4bf5df58003000ed467e0e2a69bca96c"},
    {file = "monotonic-1.6.tar.gz", hash = "sha256:3a55207bcfed53ddd5c5bae174524062935efed17792e9de2ad0205ce9ad63f7"},
]

[[package]]
name = "more-itertools"
version = "9.1.0"
description = "More routines for operating on iterables, beyond itertools"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "more-itertools-9.1.0.tar.gz", hash = "sha256:cabaa341ad0389ea83c17a94566a53ae4c9d07349861ecb14dc6d0345cf9ac5d"},
    {file = "more_itertools-9.1.0-py3-none-any.whl", hash = "sha256:d2bc7f02446e86a68911e58ded76d6561eea00cddfb2a91e7019bbb586c799f3"},
]

[[package]]
name = "mpmath"
version = "1.3.0"
description = "Python library for arbitrary-precision floating-point arithmetic"
category = "main"
optional = false
python-versions = "*"
files = [
    {file = "mpmath-1.3.0-py3-none-any.whl", hash = "sha256:a0b2b9fe80bbcd81a6647ff13108738cfb482d481d826cc0e02f5b35e5c88d2c"},
    {file = "mpmath-1.3.0.tar.gz", hash = "sha256:7a28eb2a9774d00c7bc92411c19a89209d5da7c4c9a9e227be8330a23a25b91f"},
]

[package.extras]
develop = ["codecov", "pycodestyle", "pytest (>=4.6)", "pytest-cov", "wheel"]
docs = ["sphinx"]
gmpy = ["gmpy2 (>=2.1.0a4)"]
tests = ["pytest (>=4.6)"]

[[package]]
name = "msal"
version = "1.22.0"
description = "The Microsoft Authentication Library (MSAL) for Python library enables your app to access the Microsoft Cloud by supporting authentication of users with Microsoft Azure Active Directory accounts (AAD) and Microsoft Accounts (MSA) using industry standard OAuth2 and OpenID Connect."
category = "main"
optional = false
python-versions = "*"
files = [
    {file = "msal-1.22.0-py2.py3-none-any.whl", hash = "sha256:9120b7eafdf061c92f7b3d744e5f325fca35873445fa8ffebb40b1086a13dd58"},
    {file = "msal-1.22.0.tar.gz", hash = "sha256:8a82f5375642c1625c89058018430294c109440dce42ea667d466c2cab520acd"},
]

[package.dependencies]
cryptography = ">=0.6,<43"
PyJWT = {version = ">=1.0.0,<3", extras = ["crypto"]}
requests = ">=2.0.0,<3"

[package.extras]
broker = ["pymsalruntime (>=0.13.2,<0.14)"]

[[package]]
name = "msal-extensions"
version = "1.0.0"
description = "Microsoft Authentication Library extensions (MSAL EX) provides a persistence API that can save your data on disk, encrypted on Windows, macOS and Linux. Concurrent data access will be coordinated by a file lock mechanism."
category = "main"
optional = false
python-versions = "*"
files = [
    {file = "msal-extensions-1.0.0.tar.gz", hash = "sha256:c676aba56b0cce3783de1b5c5ecfe828db998167875126ca4b47dc6436451354"},
    {file = "msal_extensions-1.0.0-py2.py3-none-any.whl", hash = "sha256:91e3db9620b822d0ed2b4d1850056a0f133cba04455e62f11612e40f5502f2ee"},
]

[package.dependencies]
msal = ">=0.4.1,<2.0.0"
portalocker = [
    {version = ">=1.0,<3", markers = "python_version >= \"3.5\" and platform_system != \"Windows\""},
    {version = ">=1.6,<3", markers = "python_version >= \"3.5\" and platform_system == \"Windows\""},
]

[[package]]
name = "multidict"
version = "6.0.4"
description = "multidict implementation"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "multidict-6.0.4-cp310-cp310-macosx_10_9_universal2.whl", hash = "sha256:0b1a97283e0c85772d613878028fec909f003993e1007eafa715b24b377cb9b8"},
    {file = "multidict-6.0.4-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:eeb6dcc05e911516ae3d1f207d4b0520d07f54484c49dfc294d6e7d63b734171"},
    {file = "multidict-6.0.4-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:d6d635d5209b82a3492508cf5b365f3446afb65ae7ebd755e70e18f287b0adf7"},
    {file = "multidict-6.0.4-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:c048099e4c9e9d615545e2001d3d8a4380bd403e1a0578734e0d31703d1b0c0b"},
    {file = "multidict-6.0.4-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:ea20853c6dbbb53ed34cb4d080382169b6f4554d394015f1bef35e881bf83547"},
    {file = "multidict-6.0.4-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:16d232d4e5396c2efbbf4f6d4df89bfa905eb0d4dc5b3549d872ab898451f569"},
    {file = "multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:36c63aaa167f6c6b04ef2c85704e93af16c11d20de1d133e39de6a0e84582a93"},
    {file = "multidict-6.0.4-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:64bdf1086b6043bf519869678f5f2757f473dee970d7abf6da91ec00acb9cb98"},
    {file = "multidict-6.0.4-cp310-cp310-musllinux_1_1_aarch64.whl", hash = "sha256:43644e38f42e3af682690876cff722d301ac585c5b9e1eacc013b7a3f7b696a0"},
    {file = "multidict-6.0.4-cp310-cp310-musllinux_1_1_i686.whl", hash = "sha256:7582a1d1030e15422262de9f58711774e02fa80df0d1578995c76214f6954988"},
    {file = "multidict-6.0.4-cp310-cp310-musllinux_1_1_ppc64le.whl", hash = "sha256:ddff9c4e225a63a5afab9dd15590432c22e8057e1a9a13d28ed128ecf047bbdc"},
    {file = "multidict-6.0.4-cp310-cp310-musllinux_1_1_s390x.whl", hash = "sha256:ee2a1ece51b9b9e7752e742cfb661d2a29e7bcdba2d27e66e28a99f1890e4fa0"},
    {file = "multidict-6.0.4-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:a2e4369eb3d47d2034032a26c7a80fcb21a2cb22e1173d761a162f11e562caa5"},
    {file = "multidict-6.0.4-cp310-cp310-win32.whl", hash = "sha256:574b7eae1ab267e5f8285f0fe881f17efe4b98c39a40858247720935b893bba8"},
    {file = "multidict-6.0.4-cp310-cp310-win_amd64.whl", hash = "sha256:4dcbb0906e38440fa3e325df2359ac6cb043df8e58c965bb45f4e406ecb162cc"},
    {file = "multidict-6.0.4-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:0dfad7a5a1e39c53ed00d2dd0c2e36aed4650936dc18fd9a1826a5ae1cad6f03"},
    {file = "multidict-6.0.4-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:64da238a09d6039e3bd39bb3aee9c21a5e34f28bfa5aa22518581f910ff94af3"},
    {file = "multidict-6.0.4-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:ff959bee35038c4624250473988b24f846cbeb2c6639de3602c073f10410ceba"},
    {file = "multidict-6.0.4-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:01a3a55bd90018c9c080fbb0b9f4891db37d148a0a18722b42f94694f8b6d4c9"},
    {file = "multidict-6.0.4-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:c5cb09abb18c1ea940fb99360ea0396f34d46566f157122c92dfa069d3e0e982"},
    {file = "multidict-6.0.4-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:666daae833559deb2d609afa4490b85830ab0dfca811a98b70a205621a6109fe"},
    {file = "multidict-6.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:11bdf3f5e1518b24530b8241529d2050014c884cf18b6fc69c0c2b30ca248710"},
    {file = "multidict-6.0.4-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:7d18748f2d30f94f498e852c67d61261c643b349b9d2a581131725595c45ec6c"},
    {file = "multidict-6.0.4-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:458f37be2d9e4c95e2d8866a851663cbc76e865b78395090786f6cd9b3bbf4f4"},
    {file = "multidict-6.0.4-cp311-cp311-musllinux_1_1_i686.whl", hash = "sha256:b1a2eeedcead3a41694130495593a559a668f382eee0727352b9a41e1c45759a"},
    {file = "multidict-6.0.4-cp311-cp311-musllinux_1_1_ppc64le.whl", hash = "sha256:7d6ae9d593ef8641544d6263c7fa6408cc90370c8cb2bbb65f8d43e5b0351d9c"},
    {file = "multidict-6.0.4-cp311-cp311-musllinux_1_1_s390x.whl", hash = "sha256:5979b5632c3e3534e42ca6ff856bb24b2e3071b37861c2c727ce220d80eee9ed"},
    {file = "multidict-6.0.4-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:dcfe792765fab89c365123c81046ad4103fcabbc4f56d1c1997e6715e8015461"},
    {file = "multidict-6.0.4-cp311-cp311-win32.whl", hash = "sha256:3601a3cece3819534b11d4efc1eb76047488fddd0c85a3948099d5da4d504636"},
    {file = "multidict-6.0.4-cp311-cp311-win_amd64.whl", hash = "sha256:81a4f0b34bd92df3da93315c6a59034df95866014ac08535fc819f043bfd51f0"},
    {file = "multidict-6.0.4-cp37-cp37m-macosx_10_9_x86_64.whl", hash = "sha256:67040058f37a2a51ed8ea8f6b0e6ee5bd78ca67f169ce6122f3e2ec80dfe9b78"},
    {file = "multidict-6.0.4-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:853888594621e6604c978ce2a0444a1e6e70c8d253ab65ba11657659dcc9100f"},
    {file = "multidict-6.0.4-cp37-cp37m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:39ff62e7d0f26c248b15e364517a72932a611a9b75f35b45be078d81bdb86603"},
    {file = "multidict-6.0.4-cp37-cp37m-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:af048912e045a2dc732847d33821a9d84ba553f5c5f028adbd364dd4765092ac"},
    {file = "multidict-6.0.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:b1e8b901e607795ec06c9e42530788c45ac21ef3aaa11dbd0c69de543bfb79a9"},
    {file = "multidict-6.0.4-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:62501642008a8b9871ddfccbf83e4222cf8ac0d5aeedf73da36153ef2ec222d2"},
    {file = "multidict-6.0.4-cp37-cp37m-musllinux_1_1_aarch64.whl", hash = "sha256:99b76c052e9f1bc0721f7541e5e8c05db3941eb9ebe7b8553c625ef88d6eefde"},
    {file = "multidict-6.0.4-cp37-cp37m-musllinux_1_1_i686.whl", hash = "sha256:509eac6cf09c794aa27bcacfd4d62c885cce62bef7b2c3e8b2e49d365b5003fe"},
    {file = "multidict-6.0.4-cp37-cp37m-musllinux_1_1_ppc64le.whl", hash = "sha256:21a12c4eb6ddc9952c415f24eef97e3e55ba3af61f67c7bc388dcdec1404a067"},
    {file = "multidict-6.0.4-cp37-cp37m-musllinux_1_1_s390x.whl", hash = "sha256:5cad9430ab3e2e4fa4a2ef4450f548768400a2ac635841bc2a56a2052cdbeb87"},
    {file = "multidict-6.0.4-cp37-cp37m-musllinux_1_1_x86_64.whl", hash = "sha256:ab55edc2e84460694295f401215f4a58597f8f7c9466faec545093045476327d"},
    {file = "multidict-6.0.4-cp37-cp37m-win32.whl", hash = "sha256:5a4dcf02b908c3b8b17a45fb0f15b695bf117a67b76b7ad18b73cf8e92608775"},
    {file = "multidict-6.0.4-cp37-cp37m-win_amd64.whl", hash = "sha256:6ed5f161328b7df384d71b07317f4d8656434e34591f20552c7bcef27b0ab88e"},
    {file = "multidict-6.0.4-cp38-cp38-macosx_10_9_universal2.whl", hash = "sha256:5fc1b16f586f049820c5c5b17bb4ee7583092fa0d1c4e28b5239181ff9532e0c"},
    {file = "multidict-6.0.4-cp38-cp38-macosx_10_9_x86_64.whl", hash = "sha256:1502e24330eb681bdaa3eb70d6358e818e8e8f908a22a1851dfd4e15bc2f8161"},
    {file = "multidict-6.0.4-cp38-cp38-macosx_11_0_arm64.whl", hash = "sha256:b692f419760c0e65d060959df05f2a531945af31fda0c8a3b3195d4efd06de11"},
    {file = "multidict-6.0.4-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:45e1ecb0379bfaab5eef059f50115b54571acfbe422a14f668fc8c27ba410e7e"},
    {file = "multidict-6.0.4-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:ddd3915998d93fbcd2566ddf9cf62cdb35c9e093075f862935573d265cf8f65d"},
    {file = "multidict-6.0.4-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:59d43b61c59d82f2effb39a93c48b845efe23a3852d201ed2d24ba830d0b4cf2"},
    {file = "multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:cc8e1d0c705233c5dd0c5e6460fbad7827d5d36f310a0fadfd45cc3029762258"},
    {file = "multidict-6.0.4-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:d6aa0418fcc838522256761b3415822626f866758ee0bc6632c9486b179d0b52"},
    {file = "multidict-6.0.4-cp38-cp38-musllinux_1_1_aarch64.whl", hash = "sha256:6748717bb10339c4760c1e63da040f5f29f5ed6e59d76daee30305894069a660"},
    {file = "multidict-6.0.4-cp38-cp38-musllinux_1_1_i686.whl", hash = "sha256:4d1a3d7ef5e96b1c9e92f973e43aa5e5b96c659c9bc3124acbbd81b0b9c8a951"},
    {file = "multidict-6.0.4-cp38-cp38-musllinux_1_1_ppc64le.whl", hash = "sha256:4372381634485bec7e46718edc71528024fcdc6f835baefe517b34a33c731d60"},
    {file = "multidict-6.0.4-cp38-cp38-musllinux_1_1_s390x.whl", hash = "sha256:fc35cb4676846ef752816d5be2193a1e8367b4c1397b74a565a9d0389c433a1d"},
    {file = "multidict-6.0.4-cp38-cp38-musllinux_1_1_x86_64.whl", hash = "sha256:4b9d9e4e2b37daddb5c23ea33a3417901fa7c7b3dee2d855f63ee67a0b21e5b1"},
    {file = "multidict-6.0.4-cp38-cp38-win32.whl", hash = "sha256:e41b7e2b59679edfa309e8db64fdf22399eec4b0b24694e1b2104fb789207779"},
    {file = "multidict-6.0.4-cp38-cp38-win_amd64.whl", hash = "sha256:d6c254ba6e45d8e72739281ebc46ea5eb5f101234f3ce171f0e9f5cc86991480"},
    {file = "multidict-6.0.4-cp39-cp39-macosx_10_9_universal2.whl", hash = "sha256:16ab77bbeb596e14212e7bab8429f24c1579234a3a462105cda4a66904998664"},
    {file = "multidict-6.0.4-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:bc779e9e6f7fda81b3f9aa58e3a6091d49ad528b11ed19f6621408806204ad35"},
    {file = "multidict-6.0.4-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:4ceef517eca3e03c1cceb22030a3e39cb399ac86bff4e426d4fc6ae49052cc60"},
    {file = "multidict-6.0.4-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:281af09f488903fde97923c7744bb001a9b23b039a909460d0f14edc7bf59706"},
    {file = "multidict-6.0.4-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:52f2dffc8acaba9a2f27174c41c9e57f60b907bb9f096b36b1a1f3be71c6284d"},
    {file = "multidict-6.0.4-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:b41156839806aecb3641f3208c0dafd3ac7775b9c4c422d82ee2a45c34ba81ca"},
    {file = "multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:d5e3fc56f88cc98ef8139255cf8cd63eb2c586531e43310ff859d6bb3a6b51f1"},
    {file = "multidict-6.0.4-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:8316a77808c501004802f9beebde51c9f857054a0c871bd6da8280e718444449"},
    {file = "multidict-6.0.4-cp39-cp39-musllinux_1_1_aarch64.whl", hash = "sha256:f70b98cd94886b49d91170ef23ec5c0e8ebb6f242d734ed7ed677b24d50c82cf"},
    {file = "multidict-6.0.4-cp39-cp39-musllinux_1_1_i686.whl", hash = "sha256:bf6774e60d67a9efe02b3616fee22441d86fab4c6d335f9d2051d19d90a40063"},
    {file = "multidict-6.0.4-cp39-cp39-musllinux_1_1_ppc64le.whl", hash = "sha256:e69924bfcdda39b722ef4d9aa762b2dd38e4632b3641b1d9a57ca9cd18f2f83a"},
    {file = "multidict-6.0.4-cp39-cp39-musllinux_1_1_s390x.whl", hash = "sha256:6b181d8c23da913d4ff585afd1155a0e1194c0b50c54fcfe286f70cdaf2b7176"},
    {file = "multidict-6.0.4-cp39-cp39-musllinux_1_1_x86_64.whl", hash = "sha256:52509b5be062d9eafc8170e53026fbc54cf3b32759a23d07fd935fb04fc22d95"},
    {file = "multidict-6.0.4-cp39-cp39-win32.whl", hash = "sha256:27c523fbfbdfd19c6867af7346332b62b586eed663887392cff78d614f9ec313"},
    {file = "multidict-6.0.4-cp39-cp39-win_amd64.whl", hash = "sha256:33029f5734336aa0d4c0384525da0387ef89148dc7191aae00ca5fb23d7aafc2"},
    {file = "multidict-6.0.4.tar.gz", hash = "sha256:3666906492efb76453c0e7b97f2cf459b0682e7402c0489a95484965dbc1da49"},
]

[[package]]
name = "mypy-extensions"
version = "1.0.0"
description = "Type system extensions for programs checked with the mypy type checker."
category = "main"
optional = false
python-versions = ">=3.5"
files = [
    {file = "mypy_extensions-1.0.0-py3-none-any.whl", hash = "sha256:4392f6c0eb8a5668a69e23d168ffa70f0be9ccfd32b5cc2d26a34ae5b844552d"},
    {file = "mypy_extensions-1.0.0.tar.gz", hash = "sha256:75dbf8955dc00442a438fc4d0666508a9a97b6bd41aa2f0ffe9d2f2725af0782"},
]

[[package]]
name = "numexpr"
version = "2.8.4"
description = "Fast numerical expression evaluator for NumPy"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "numexpr-2.8.4-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:a75967d46b6bd56455dd32da6285e5ffabe155d0ee61eef685bbfb8dafb2e484"},
    {file = "numexpr-2.8.4-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:db93cf1842f068247de631bfc8af20118bf1f9447cd929b531595a5e0efc9346"},
    {file = "numexpr-2.8.4-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:7bca95f4473b444428061d4cda8e59ac564dc7dc6a1dea3015af9805c6bc2946"},
    {file = "numexpr-2.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9e34931089a6bafc77aaae21f37ad6594b98aa1085bb8b45d5b3cd038c3c17d9"},
    {file = "numexpr-2.8.4-cp310-cp310-win32.whl", hash = "sha256:f3a920bfac2645017110b87ddbe364c9c7a742870a4d2f6120b8786c25dc6db3"},
    {file = "numexpr-2.8.4-cp310-cp310-win_amd64.whl", hash = "sha256:6931b1e9d4f629f43c14b21d44f3f77997298bea43790cfcdb4dd98804f90783"},
    {file = "numexpr-2.8.4-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:9400781553541f414f82eac056f2b4c965373650df9694286b9bd7e8d413f8d8"},
    {file = "numexpr-2.8.4-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:6ee9db7598dd4001138b482342b96d78110dd77cefc051ec75af3295604dde6a"},
    {file = "numexpr-2.8.4-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:ff5835e8af9a212e8480003d731aad1727aaea909926fd009e8ae6a1cba7f141"},
    {file = "numexpr-2.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:655d84eb09adfee3c09ecf4a89a512225da153fdb7de13c447404b7d0523a9a7"},
    {file = "numexpr-2.8.4-cp311-cp311-win32.whl", hash = "sha256:5538b30199bfc68886d2be18fcef3abd11d9271767a7a69ff3688defe782800a"},
    {file = "numexpr-2.8.4-cp311-cp311-win_amd64.whl", hash = "sha256:3f039321d1c17962c33079987b675fb251b273dbec0f51aac0934e932446ccc3"},
    {file = "numexpr-2.8.4-cp37-cp37m-macosx_10_9_x86_64.whl", hash = "sha256:c867cc36cf815a3ec9122029874e00d8fbcef65035c4a5901e9b120dd5d626a2"},
    {file = "numexpr-2.8.4-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:059546e8f6283ccdb47c683101a890844f667fa6d56258d48ae2ecf1b3875957"},
    {file = "numexpr-2.8.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:845a6aa0ed3e2a53239b89c1ebfa8cf052d3cc6e053c72805e8153300078c0b1"},
    {file = "numexpr-2.8.4-cp37-cp37m-win32.whl", hash = "sha256:a38664e699526cb1687aefd9069e2b5b9387da7feac4545de446141f1ef86f46"},
    {file = "numexpr-2.8.4-cp37-cp37m-win_amd64.whl", hash = "sha256:eaec59e9bf70ff05615c34a8b8d6c7bd042bd9f55465d7b495ea5436f45319d0"},
    {file = "numexpr-2.8.4-cp38-cp38-macosx_10_9_x86_64.whl", hash = "sha256:b318541bf3d8326682ebada087ba0050549a16d8b3fa260dd2585d73a83d20a7"},
    {file = "numexpr-2.8.4-cp38-cp38-macosx_11_0_arm64.whl", hash = "sha256:b076db98ca65eeaf9bd224576e3ac84c05e451c0bd85b13664b7e5f7b62e2c70"},
    {file = "numexpr-2.8.4-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:90f12cc851240f7911a47c91aaf223dba753e98e46dff3017282e633602e76a7"},
    {file = "numexpr-2.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6c368aa35ae9b18840e78b05f929d3a7b3abccdba9630a878c7db74ca2368339"},
    {file = "numexpr-2.8.4-cp38-cp38-win32.whl", hash = "sha256:b96334fc1748e9ec4f93d5fadb1044089d73fb08208fdb8382ed77c893f0be01"},
    {file = "numexpr-2.8.4-cp38-cp38-win_amd64.whl", hash = "sha256:a6d2d7740ae83ba5f3531e83afc4b626daa71df1ef903970947903345c37bd03"},
    {file = "numexpr-2.8.4-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:77898fdf3da6bb96aa8a4759a8231d763a75d848b2f2e5c5279dad0b243c8dfe"},
    {file = "numexpr-2.8.4-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:df35324666b693f13a016bc7957de7cc4d8801b746b81060b671bf78a52b9037"},
    {file = "numexpr-2.8.4-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:17ac9cfe6d0078c5fc06ba1c1bbd20b8783f28c6f475bbabd3cad53683075cab"},
    {file = "numexpr-2.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:df3a1f6b24214a1ab826e9c1c99edf1686c8e307547a9aef33910d586f626d01"},
    {file = "numexpr-2.8.4-cp39-cp39-win32.whl", hash = "sha256:7d71add384adc9119568d7e9ffa8a35b195decae81e0abf54a2b7779852f0637"},
    {file = "numexpr-2.8.4-cp39-cp39-win_amd64.whl", hash = "sha256:9f096d707290a6a00b6ffdaf581ee37331109fb7b6c8744e9ded7c779a48e517"},
    {file = "numexpr-2.8.4.tar.gz", hash = "sha256:d5432537418d18691b9115d615d6daa17ee8275baef3edf1afbbf8bc69806147"},
]

[package.dependencies]
numpy = ">=1.13.3"

[[package]]
name = "numpy"
version = "1.24.3"
description = "Fundamental package for array computing in Python"
category = "main"
optional = false
python-versions = ">=3.8"
files = [
    {file = "numpy-1.24.3-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:3c1104d3c036fb81ab923f507536daedc718d0ad5a8707c6061cdfd6d184e570"},
    {file = "numpy-1.24.3-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:202de8f38fc4a45a3eea4b63e2f376e5f2dc64ef0fa692838e31a808520efaf7"},
    {file = "numpy-1.24.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:8535303847b89aa6b0f00aa1dc62867b5a32923e4d1681a35b5eef2d9591a463"},
    {file = "numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:2d926b52ba1367f9acb76b0df6ed21f0b16a1ad87c6720a1121674e5cf63e2b6"},
    {file = "numpy-1.24.3-cp310-cp310-win32.whl", hash = "sha256:f21c442fdd2805e91799fbe044a7b999b8571bb0ab0f7850d0cb9641a687092b"},
    {file = "numpy-1.24.3-cp310-cp310-win_amd64.whl", hash = "sha256:ab5f23af8c16022663a652d3b25dcdc272ac3f83c3af4c02eb8b824e6b3ab9d7"},
    {file = "numpy-1.24.3-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:9a7721ec204d3a237225db3e194c25268faf92e19338a35f3a224469cb6039a3"},
    {file = "numpy-1.24.3-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:d6cc757de514c00b24ae8cf5c876af2a7c3df189028d68c0cb4eaa9cd5afc2bf"},
    {file = "numpy-1.24.3-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:76e3f4e85fc5d4fd311f6e9b794d0c00e7002ec122be271f2019d63376f1d385"},
    {file = "numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:a1d3c026f57ceaad42f8231305d4653d5f05dc6332a730ae5c0bea3513de0950"},
    {file = "numpy-1.24.3-cp311-cp311-win32.whl", hash = "sha256:c91c4afd8abc3908e00a44b2672718905b8611503f7ff87390cc0ac3423fb096"},
    {file = "numpy-1.24.3-cp311-cp311-win_amd64.whl", hash = "sha256:5342cf6aad47943286afa6f1609cad9b4266a05e7f2ec408e2cf7aea7ff69d80"},
    {file = "numpy-1.24.3-cp38-cp38-macosx_10_9_x86_64.whl", hash = "sha256:7776ea65423ca6a15255ba1872d82d207bd1e09f6d0894ee4a64678dd2204078"},
    {file = "numpy-1.24.3-cp38-cp38-macosx_11_0_arm64.whl", hash = "sha256:ae8d0be48d1b6ed82588934aaaa179875e7dc4f3d84da18d7eae6eb3f06c242c"},
    {file = "numpy-1.24.3-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:ecde0f8adef7dfdec993fd54b0f78183051b6580f606111a6d789cd14c61ea0c"},
    {file = "numpy-1.24.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:4749e053a29364d3452c034827102ee100986903263e89884922ef01a0a6fd2f"},
    {file = "numpy-1.24.3-cp38-cp38-win32.whl", hash = "sha256:d933fabd8f6a319e8530d0de4fcc2e6a61917e0b0c271fded460032db42a0fe4"},
    {file = "numpy-1.24.3-cp38-cp38-win_amd64.whl", hash = "sha256:56e48aec79ae238f6e4395886b5eaed058abb7231fb3361ddd7bfdf4eed54289"},
    {file = "numpy-1.24.3-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:4719d5aefb5189f50887773699eaf94e7d1e02bf36c1a9d353d9f46703758ca4"},
    {file = "numpy-1.24.3-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:0ec87a7084caa559c36e0a2309e4ecb1baa03b687201d0a847c8b0ed476a7187"},
    {file = "numpy-1.24.3-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:ea8282b9bcfe2b5e7d491d0bf7f3e2da29700cec05b49e64d6246923329f2b02"},
    {file = "numpy-1.24.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:210461d87fb02a84ef243cac5e814aad2b7f4be953b32cb53327bb49fd77fbb4"},
    {file = "numpy-1.24.3-cp39-cp39-win32.whl", hash = "sha256:784c6da1a07818491b0ffd63c6bbe5a33deaa0e25a20e1b3ea20cf0e43f8046c"},
    {file = "numpy-1.24.3-cp39-cp39-win_amd64.whl", hash = "sha256:d5036197ecae68d7f491fcdb4df90082b0d4960ca6599ba2659957aafced7c17"},
    {file = "numpy-1.24.3-pp38-pypy38_pp73-macosx_10_9_x86_64.whl", hash = "sha256:352ee00c7f8387b44d19f4cada524586f07379c0d49270f87233983bc5087ca0"},
    {file = "numpy-1.24.3-pp38-pypy38_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:1a7d6acc2e7524c9955e5c903160aa4ea083736fde7e91276b0e5d98e6332812"},
    {file = "numpy-1.24.3-pp38-pypy38_pp73-win_amd64.whl", hash = "sha256:35400e6a8d102fd07c71ed7dcadd9eb62ee9a6e84ec159bd48c28235bbb0f8e4"},
    {file = "numpy-1.24.3.tar.gz", hash = "sha256:ab344f1bf21f140adab8e47fdbc7c35a477dc01408791f8ba00d018dd0bc5155"},
]

[[package]]
name = "onnxruntime"
version = "1.15.0"
description = "ONNX Runtime is a runtime accelerator for Machine Learning models"
category = "main"
optional = false
python-versions = "*"
files = [
    {file = "onnxruntime-1.15.0-cp310-cp310-macosx_10_15_x86_64.whl", hash = "sha256:a61c6ff118e01d0800b19cbed8aa64cf687edd60e6d0cc39f0e9900ad324c212"},
    {file = "onnxruntime-1.15.0-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:413495938e389f087c1dbc61168fbb4c5460610c0bac542c7ec3bcfba64b6bfd"},
    {file = "onnxruntime-1.15.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:bbcd117186826a7b81afa10a2a4bb3882302149b07351ab5add367daf8e245cf"},
    {file = "onnxruntime-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:34caf6e11b12d199c4029b6717fc679d12b41ef05a8d34badff3977eb459d5ac"},
    {file = "onnxruntime-1.15.0-cp310-cp310-win32.whl", hash = "sha256:3155fcfa546db5d31859235a4ab21c70d10a933a3df8354a1269d3a6def1b743"},
    {file = "onnxruntime-1.15.0-cp310-cp310-win_amd64.whl", hash = "sha256:1ffde3e22d2969aeb9ab8fea3adfd7a568d28e6a2f05c369e562056a52510857"},
    {file = "onnxruntime-1.15.0-cp311-cp311-macosx_10_15_x86_64.whl", hash = "sha256:3c89bceb1dd172ab8b288fca48eb0ac3d68796041abcd49537108581a14748ad"},
    {file = "onnxruntime-1.15.0-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:3216524f10b805831ac606718966c5623f001762c1fb70aa107691e03b20e8ae"},
    {file = "onnxruntime-1.15.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:99c34478cf34d64d72ed5fb0642c9e19d1bc1a42be9ffeda31420dc23a4d0af2"},
    {file = "onnxruntime-1.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:4ff833c740a5054e9eba734eb1c13d6925d5a84adf1102179e46b20effe5e1a6"},
    {file = "onnxruntime-1.15.0-cp311-cp311-win32.whl", hash = "sha256:cd3bbdb6cff207a3696c5a370717280bc04f0824de27dc03b51292b23fcff0ad"},
    {file = "onnxruntime-1.15.0-cp311-cp311-win_amd64.whl", hash = "sha256:335f26b663295f50921437f74620bc0ee8b25707a1a8987a8142452d5283d098"},
    {file = "onnxruntime-1.15.0-cp38-cp38-macosx_10_15_x86_64.whl", hash = "sha256:563c56bbf7331686cb72d93b47fa96f4fdf883dbdf7398dc07ba6011a4311c40"},
    {file = "onnxruntime-1.15.0-cp38-cp38-macosx_11_0_arm64.whl", hash = "sha256:de66cd57e8db8be3783ce478eaade75c07e9b1ce0f3c12500b34d1683d6c13e5"},
    {file = "onnxruntime-1.15.0-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:101310d7ed44b43f35d1158033266b9df4994b80691ded7517f389d7fc281353"},
    {file = "onnxruntime-1.15.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:f3df38690e495c57ae3d160210c3567a53f0b38dcc4526983af69e89a6f762d9"},
    {file = "onnxruntime-1.15.0-cp38-cp38-win32.whl", hash = "sha256:84ad51d1be998a250acd93e61f1db699628b523caf12c3bf863c3204e9e6ceda"},
    {file = "onnxruntime-1.15.0-cp38-cp38-win_amd64.whl", hash = "sha256:4d443c544370e28d2ad2d5e472ba32eb7cbc62ef7e315e66ec75c3292f47cee5"},
    {file = "onnxruntime-1.15.0-cp39-cp39-macosx_10_15_x86_64.whl", hash = "sha256:80861aa47817fa0db29f5e818e567afd2b0013373418413435cc3dd361458fe8"},
    {file = "onnxruntime-1.15.0-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:df402e46432fb038fca9bfe8d1a489396b54b6253ec26398f6d8b0322276fc99"},
    {file = "onnxruntime-1.15.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d5c5afc01e0109f26a06ea19e132f7b934878ec81fac4557d35c5f3d01ca2a17"},
    {file = "onnxruntime-1.15.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:67bbb583d7f6f98c831945f664a5d302a65c3cc9dc8da793d2acce0260650927"},
    {file = "onnxruntime-1.15.0-cp39-cp39-win32.whl", hash = "sha256:b21438216c0a9985c224938431259959ddf71c7949f9e5759d6ad68fb5f61717"},
    {file = "onnxruntime-1.15.0-cp39-cp39-win_amd64.whl", hash = "sha256:e83521ddbeef421c3e697a7cc803e826a0d1b7a40446a857572700b6ab5f4083"},
]

[package.dependencies]
coloredlogs = "*"
flatbuffers = "*"
numpy = ">=1.21.6"
packaging = "*"
protobuf = "*"
sympy = "*"

[[package]]
name = "openai"
version = "0.27.7"
description = "Python client library for the OpenAI API"
category = "main"
optional = false
python-versions = ">=3.7.1"
files = [
    {file = "openai-0.27.7-py3-none-any.whl", hash = "sha256:788fb7fa85bf7caac6c1ed7eea5984254a1bdaf09ef485acf0e5718c8b2dc25a"},
    {file = "openai-0.27.7.tar.gz", hash = "sha256:bca95fd4c3054ef38924def096396122130454442ec52005915ecf8269626b1d"},
]

[package.dependencies]
aiohttp = "*"
requests = ">=2.20"
tqdm = "*"

[package.extras]
datalib = ["numpy", "openpyxl (>=3.0.7)", "pandas (>=1.2.3)", "pandas-stubs (>=1.1.0.11)"]
dev = ["black (>=21.6b0,<22.0)", "pytest (>=6.0.0,<7.0.0)", "pytest-asyncio", "pytest-mock"]
embeddings = ["matplotlib", "numpy", "openpyxl (>=3.0.7)", "pandas (>=1.2.3)", "pandas-stubs (>=1.1.0.11)", "plotly", "scikit-learn (>=1.0.2)", "scipy", "tenacity (>=8.0.1)"]
wandb = ["numpy", "openpyxl (>=3.0.7)", "pandas (>=1.2.3)", "pandas-stubs (>=1.1.0.11)", "wandb"]

[[package]]
name = "openapi-schema-pydantic"
version = "1.2.4"
description = "OpenAPI (v3) specification schema as pydantic class"
category = "main"
optional = false
python-versions = ">=3.6.1"
files = [
    {file = "openapi-schema-pydantic-1.2.4.tar.gz", hash = "sha256:3e22cf58b74a69f752cc7e5f1537f6e44164282db2700cbbcd3bb99ddd065196"},
    {file = "openapi_schema_pydantic-1.2.4-py3-none-any.whl", hash = "sha256:a932ecc5dcbb308950282088956e94dea069c9823c84e507d64f6b622222098c"},
]

[package.dependencies]
pydantic = ">=1.8.2"

[[package]]
name = "overrides"
version = "7.3.1"
description = "A decorator to automatically detect mismatch when overriding a method."
category = "main"
optional = false
python-versions = ">=3.6"
files = [
    {file = "overrides-7.3.1-py3-none-any.whl", hash = "sha256:6187d8710a935d09b0bcef8238301d6ee2569d2ac1ae0ec39a8c7924e27f58ca"},
    {file = "overrides-7.3.1.tar.gz", hash = "sha256:8b97c6c1e1681b78cbc9424b138d880f0803c2254c5ebaabdde57bb6c62093f2"},
]

[[package]]
name = "packaging"
version = "23.1"
description = "Core utilities for Python packages"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "packaging-23.1-py3-none-any.whl", hash = "sha256:994793af429502c4ea2ebf6bf664629d07c1a9fe974af92966e4b8d2df7edc61"},
    {file = "packaging-23.1.tar.gz", hash = "sha256:a392980d2b6cffa644431898be54b0045151319d1e7ec34f0cfed48767dd334f"},
]

[[package]]
name = "pandas"
version = "2.0.2"
description = "Powerful data structures for data analysis, time series, and statistics"
category = "main"
optional = false
python-versions = ">=3.8"
files = [
    {file = "pandas-2.0.2-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:9ebb9f1c22ddb828e7fd017ea265a59d80461d5a79154b49a4207bd17514d122"},
    {file = "pandas-2.0.2-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:1eb09a242184092f424b2edd06eb2b99d06dc07eeddff9929e8667d4ed44e181"},
    {file = "pandas-2.0.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:c7319b6e68de14e6209460f72a8d1ef13c09fb3d3ef6c37c1e65b35d50b5c145"},
    {file = "pandas-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:dd46bde7309088481b1cf9c58e3f0e204b9ff9e3244f441accd220dd3365ce7c"},
    {file = "pandas-2.0.2-cp310-cp310-win32.whl", hash = "sha256:51a93d422fbb1bd04b67639ba4b5368dffc26923f3ea32a275d2cc450f1d1c86"},
    {file = "pandas-2.0.2-cp310-cp310-win_amd64.whl", hash = "sha256:66d00300f188fa5de73f92d5725ced162488f6dc6ad4cecfe4144ca29debe3b8"},
    {file = "pandas-2.0.2-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:02755de164da6827764ceb3bbc5f64b35cb12394b1024fdf88704d0fa06e0e2f"},
    {file = "pandas-2.0.2-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:0a1e0576611641acde15c2322228d138258f236d14b749ad9af498ab69089e2d"},
    {file = "pandas-2.0.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:a6b5f14cd24a2ed06e14255ff40fe2ea0cfaef79a8dd68069b7ace74bd6acbba"},
    {file = "pandas-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:50e451932b3011b61d2961b4185382c92cc8c6ee4658dcd4f320687bb2d000ee"},
    {file = "pandas-2.0.2-cp311-cp311-win32.whl", hash = "sha256:7b21cb72958fc49ad757685db1919021d99650d7aaba676576c9e88d3889d456"},
    {file = "pandas-2.0.2-cp311-cp311-win_amd64.whl", hash = "sha256:c4af689352c4fe3d75b2834933ee9d0ccdbf5d7a8a7264f0ce9524e877820c08"},
    {file = "pandas-2.0.2-cp38-cp38-macosx_10_9_x86_64.whl", hash = "sha256:69167693cb8f9b3fc060956a5d0a0a8dbfed5f980d9fd2c306fb5b9c855c814c"},
    {file = "pandas-2.0.2-cp38-cp38-macosx_11_0_arm64.whl", hash = "sha256:30a89d0fec4263ccbf96f68592fd668939481854d2ff9da709d32a047689393b"},
    {file = "pandas-2.0.2-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:a18e5c72b989ff0f7197707ceddc99828320d0ca22ab50dd1b9e37db45b010c0"},
    {file = "pandas-2.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:7376e13d28eb16752c398ca1d36ccfe52bf7e887067af9a0474de6331dd948d2"},
    {file = "pandas-2.0.2-cp38-cp38-win32.whl", hash = "sha256:6d6d10c2142d11d40d6e6c0a190b1f89f525bcf85564707e31b0a39e3b398e08"},
    {file = "pandas-2.0.2-cp38-cp38-win_amd64.whl", hash = "sha256:e69140bc2d29a8556f55445c15f5794490852af3de0f609a24003ef174528b79"},
    {file = "pandas-2.0.2-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:b42b120458636a981077cfcfa8568c031b3e8709701315e2bfa866324a83efa8"},
    {file = "pandas-2.0.2-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:f908a77cbeef9bbd646bd4b81214cbef9ac3dda4181d5092a4aa9797d1bc7774"},
    {file = "pandas-2.0.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:713f2f70abcdade1ddd68fc91577cb090b3544b07ceba78a12f799355a13ee44"},
    {file = "pandas-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:cf3f0c361a4270185baa89ec7ab92ecaa355fe783791457077473f974f654df5"},
    {file = "pandas-2.0.2-cp39-cp39-win32.whl", hash = "sha256:598e9020d85a8cdbaa1815eb325a91cfff2bb2b23c1442549b8a3668e36f0f77"},
    {file = "pandas-2.0.2-cp39-cp39-win_amd64.whl", hash = "sha256:77550c8909ebc23e56a89f91b40ad01b50c42cfbfab49b3393694a50549295ea"},
    {file = "pandas-2.0.2.tar.gz", hash = "sha256:dd5476b6c3fe410ee95926873f377b856dbc4e81a9c605a0dc05aaccc6a7c6c6"},
]

[package.dependencies]
numpy = [
    {version = ">=1.21.0", markers = "python_version >= \"3.10\""},
    {version = ">=1.23.2", markers = "python_version >= \"3.11\""},
]
python-dateutil = ">=2.8.2"
pytz = ">=2020.1"
tzdata = ">=2022.1"

[package.extras]
all = ["PyQt5 (>=5.15.1)", "SQLAlchemy (>=1.4.16)", "beautifulsoup4 (>=4.9.3)", "bottleneck (>=1.3.2)", "brotlipy (>=0.7.0)", "fastparquet (>=0.6.3)", "fsspec (>=2021.07.0)", "gcsfs (>=2021.07.0)", "html5lib (>=1.1)", "hypothesis (>=6.34.2)", "jinja2 (>=3.0.0)", "lxml (>=4.6.3)", "matplotlib (>=3.6.1)", "numba (>=0.53.1)", "numexpr (>=2.7.3)", "odfpy (>=1.4.1)", "openpyxl (>=3.0.7)", "pandas-gbq (>=0.15.0)", "psycopg2 (>=2.8.6)", "pyarrow (>=7.0.0)", "pymysql (>=1.0.2)", "pyreadstat (>=1.1.2)", "pytest (>=7.0.0)", "pytest-asyncio (>=0.17.0)", "pytest-xdist (>=2.2.0)", "python-snappy (>=0.6.0)", "pyxlsb (>=1.0.8)", "qtpy (>=2.2.0)", "s3fs (>=2021.08.0)", "scipy (>=1.7.1)", "tables (>=3.6.1)", "tabulate (>=0.8.9)", "xarray (>=0.21.0)", "xlrd (>=2.0.1)", "xlsxwriter (>=1.4.3)", "zstandard (>=0.15.2)"]
aws = ["s3fs (>=2021.08.0)"]
clipboard = ["PyQt5 (>=5.15.1)", "qtpy (>=2.2.0)"]
compression = ["brotlipy (>=0.7.0)", "python-snappy (>=0.6.0)", "zstandard (>=0.15.2)"]
computation = ["scipy (>=1.7.1)", "xarray (>=0.21.0)"]
excel = ["odfpy (>=1.4.1)", "openpyxl (>=3.0.7)", "pyxlsb (>=1.0.8)", "xlrd (>=2.0.1)", "xlsxwriter (>=1.4.3)"]
feather = ["pyarrow (>=7.0.0)"]
fss = ["fsspec (>=2021.07.0)"]
gcp = ["gcsfs (>=2021.07.0)", "pandas-gbq (>=0.15.0)"]
hdf5 = ["tables (>=3.6.1)"]
html = ["beautifulsoup4 (>=4.9.3)", "html5lib (>=1.1)", "lxml (>=4.6.3)"]
mysql = ["SQLAlchemy (>=1.4.16)", "pymysql (>=1.0.2)"]
output-formatting = ["jinja2 (>=3.0.0)", "tabulate (>=0.8.9)"]
parquet = ["pyarrow (>=7.0.0)"]
performance = ["bottleneck (>=1.3.2)", "numba (>=0.53.1)", "numexpr (>=2.7.1)"]
plot = ["matplotlib (>=3.6.1)"]
postgresql = ["SQLAlchemy (>=1.4.16)", "psycopg2 (>=2.8.6)"]
spss = ["pyreadstat (>=1.1.2)"]
sql-other = ["SQLAlchemy (>=1.4.16)"]
test = ["hypothesis (>=6.34.2)", "pytest (>=7.0.0)", "pytest-asyncio (>=0.17.0)", "pytest-xdist (>=2.2.0)"]
xml = ["lxml (>=4.6.3)"]

[[package]]
name = "pgvector"
version = "0.1.8"
description = "pgvector support for Python"
category = "main"
optional = false
python-versions = ">=3.6"
files = [
    {file = "pgvector-0.1.8-py2.py3-none-any.whl", hash = "sha256:99dce3a6580ef73863edb9b8441937671f4e1a09383826e6b0838176cd441a96"},
]

[package.dependencies]
numpy = "*"

[[package]]
name = "pillow"
version = "9.5.0"
description = "Python Imaging Library (Fork)"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "Pillow-9.5.0-cp310-cp310-macosx_10_10_x86_64.whl", hash = "sha256:ace6ca218308447b9077c14ea4ef381ba0b67ee78d64046b3f19cf4e1139ad16"},
    {file = "Pillow-9.5.0-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:d3d403753c9d5adc04d4694d35cf0391f0f3d57c8e0030aac09d7678fa8030aa"},
    {file = "Pillow-9.5.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:5ba1b81ee69573fe7124881762bb4cd2e4b6ed9dd28c9c60a632902fe8db8b38"},
    {file = "Pillow-9.5.0-cp310-cp310-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:fe7e1c262d3392afcf5071df9afa574544f28eac825284596ac6db56e6d11062"},
    {file = "Pillow-9.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8f36397bf3f7d7c6a3abdea815ecf6fd14e7fcd4418ab24bae01008d8d8ca15e"},
    {file = "Pillow-9.5.0-cp310-cp310-manylinux_2_28_aarch64.whl", hash = "sha256:252a03f1bdddce077eff2354c3861bf437c892fb1832f75ce813ee94347aa9b5"},
    {file = "Pillow-9.5.0-cp310-cp310-manylinux_2_28_x86_64.whl", hash = "sha256:85ec677246533e27770b0de5cf0f9d6e4ec0c212a1f89dfc941b64b21226009d"},
    {file = "Pillow-9.5.0-cp310-cp310-musllinux_1_1_aarch64.whl", hash = "sha256:b416f03d37d27290cb93597335a2f85ed446731200705b22bb927405320de903"},
    {file = "Pillow-9.5.0-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:1781a624c229cb35a2ac31cc4a77e28cafc8900733a864870c49bfeedacd106a"},
    {file = "Pillow-9.5.0-cp310-cp310-win32.whl", hash = "sha256:8507eda3cd0608a1f94f58c64817e83ec12fa93a9436938b191b80d9e4c0fc44"},
    {file = "Pillow-9.5.0-cp310-cp310-win_amd64.whl", hash = "sha256:d3c6b54e304c60c4181da1c9dadf83e4a54fd266a99c70ba646a9baa626819eb"},
    {file = "Pillow-9.5.0-cp311-cp311-macosx_10_10_x86_64.whl", hash = "sha256:7ec6f6ce99dab90b52da21cf0dc519e21095e332ff3b399a357c187b1a5eee32"},
    {file = "Pillow-9.5.0-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:560737e70cb9c6255d6dcba3de6578a9e2ec4b573659943a5e7e4af13f298f5c"},
    {file = "Pillow-9.5.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:96e88745a55b88a7c64fa49bceff363a1a27d9a64e04019c2281049444a571e3"},
    {file = "Pillow-9.5.0-cp311-cp311-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:d9c206c29b46cfd343ea7cdfe1232443072bbb270d6a46f59c259460db76779a"},
    {file = "Pillow-9.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:cfcc2c53c06f2ccb8976fb5c71d448bdd0a07d26d8e07e321c103416444c7ad1"},
    {file = "Pillow-9.5.0-cp311-cp311-manylinux_2_28_aarch64.whl", hash = "sha256:a0f9bb6c80e6efcde93ffc51256d5cfb2155ff8f78292f074f60f9e70b942d99"},
    {file = "Pillow-9.5.0-cp311-cp311-manylinux_2_28_x86_64.whl", hash = "sha256:8d935f924bbab8f0a9a28404422da8af4904e36d5c33fc6f677e4c4485515625"},
    {file = "Pillow-9.5.0-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:fed1e1cf6a42577953abbe8e6cf2fe2f566daebde7c34724ec8803c4c0cda579"},
    {file = "Pillow-9.5.0-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:c1170d6b195555644f0616fd6ed929dfcf6333b8675fcca044ae5ab110ded296"},
    {file = "Pillow-9.5.0-cp311-cp311-win32.whl", hash = "sha256:54f7102ad31a3de5666827526e248c3530b3a33539dbda27c6843d19d72644ec"},
    {file = "Pillow-9.5.0-cp311-cp311-win_amd64.whl", hash = "sha256:cfa4561277f677ecf651e2b22dc43e8f5368b74a25a8f7d1d4a3a243e573f2d4"},
    {file = "Pillow-9.5.0-cp311-cp311-win_arm64.whl", hash = "sha256:965e4a05ef364e7b973dd17fc765f42233415974d773e82144c9bbaaaea5d089"},
    {file = "Pillow-9.5.0-cp312-cp312-win32.whl", hash = "sha256:22baf0c3cf0c7f26e82d6e1adf118027afb325e703922c8dfc1d5d0156bb2eeb"},
    {file = "Pillow-9.5.0-cp312-cp312-win_amd64.whl", hash = "sha256:432b975c009cf649420615388561c0ce7cc31ce9b2e374db659ee4f7d57a1f8b"},
    {file = "Pillow-9.5.0-cp37-cp37m-macosx_10_10_x86_64.whl", hash = "sha256:5d4ebf8e1db4441a55c509c4baa7a0587a0210f7cd25fcfe74dbbce7a4bd1906"},
    {file = "Pillow-9.5.0-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:375f6e5ee9620a271acb6820b3d1e94ffa8e741c0601db4c0c4d3cb0a9c224bf"},
    {file = "Pillow-9.5.0-cp37-cp37m-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:99eb6cafb6ba90e436684e08dad8be1637efb71c4f2180ee6b8f940739406e78"},
    {file = "Pillow-9.5.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:2dfaaf10b6172697b9bceb9a3bd7b951819d1ca339a5ef294d1f1ac6d7f63270"},
    {file = "Pillow-9.5.0-cp37-cp37m-manylinux_2_28_aarch64.whl", hash = "sha256:763782b2e03e45e2c77d7779875f4432e25121ef002a41829d8868700d119392"},
    {file = "Pillow-9.5.0-cp37-cp37m-manylinux_2_28_x86_64.whl", hash = "sha256:35f6e77122a0c0762268216315bf239cf52b88865bba522999dc38f1c52b9b47"},
    {file = "Pillow-9.5.0-cp37-cp37m-win32.whl", hash = "sha256:aca1c196f407ec7cf04dcbb15d19a43c507a81f7ffc45b690899d6a76ac9fda7"},
    {file = "Pillow-9.5.0-cp37-cp37m-win_amd64.whl", hash = "sha256:322724c0032af6692456cd6ed554bb85f8149214d97398bb80613b04e33769f6"},
    {file = "Pillow-9.5.0-cp38-cp38-macosx_10_10_x86_64.whl", hash = "sha256:a0aa9417994d91301056f3d0038af1199eb7adc86e646a36b9e050b06f526597"},
    {file = "Pillow-9.5.0-cp38-cp38-macosx_11_0_arm64.whl", hash = "sha256:f8286396b351785801a976b1e85ea88e937712ee2c3ac653710a4a57a8da5d9c"},
    {file = "Pillow-9.5.0-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:c830a02caeb789633863b466b9de10c015bded434deb3ec87c768e53752ad22a"},
    {file = "Pillow-9.5.0-cp38-cp38-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:fbd359831c1657d69bb81f0db962905ee05e5e9451913b18b831febfe0519082"},
    {file = "Pillow-9.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:f8fc330c3370a81bbf3f88557097d1ea26cd8b019d6433aa59f71195f5ddebbf"},
    {file = "Pillow-9.5.0-cp38-cp38-manylinux_2_28_aarch64.whl", hash = "sha256:7002d0797a3e4193c7cdee3198d7c14f92c0836d6b4a3f3046a64bd1ce8df2bf"},
    {file = "Pillow-9.5.0-cp38-cp38-manylinux_2_28_x86_64.whl", hash = "sha256:229e2c79c00e85989a34b5981a2b67aa079fd08c903f0aaead522a1d68d79e51"},
    {file = "Pillow-9.5.0-cp38-cp38-musllinux_1_1_aarch64.whl", hash = "sha256:9adf58f5d64e474bed00d69bcd86ec4bcaa4123bfa70a65ce72e424bfb88ed96"},
    {file = "Pillow-9.5.0-cp38-cp38-musllinux_1_1_x86_64.whl", hash = "sha256:662da1f3f89a302cc22faa9f14a262c2e3951f9dbc9617609a47521c69dd9f8f"},
    {file = "Pillow-9.5.0-cp38-cp38-win32.whl", hash = "sha256:6608ff3bf781eee0cd14d0901a2b9cc3d3834516532e3bd673a0a204dc8615fc"},
    {file = "Pillow-9.5.0-cp38-cp38-win_amd64.whl", hash = "sha256:e49eb4e95ff6fd7c0c402508894b1ef0e01b99a44320ba7d8ecbabefddcc5569"},
    {file = "Pillow-9.5.0-cp39-cp39-macosx_10_10_x86_64.whl", hash = "sha256:482877592e927fd263028c105b36272398e3e1be3269efda09f6ba21fd83ec66"},
    {file = "Pillow-9.5.0-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:3ded42b9ad70e5f1754fb7c2e2d6465a9c842e41d178f262e08b8c85ed8a1d8e"},
    {file = "Pillow-9.5.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:c446d2245ba29820d405315083d55299a796695d747efceb5717a8b450324115"},
    {file = "Pillow-9.5.0-cp39-cp39-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:8aca1152d93dcc27dc55395604dcfc55bed5f25ef4c98716a928bacba90d33a3"},
    {file = "Pillow-9.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:608488bdcbdb4ba7837461442b90ea6f3079397ddc968c31265c1e056964f1ef"},
    {file = "Pillow-9.5.0-cp39-cp39-manylinux_2_28_aarch64.whl", hash = "sha256:60037a8db8750e474af7ffc9faa9b5859e6c6d0a50e55c45576bf28be7419705"},
    {file = "Pillow-9.5.0-cp39-cp39-manylinux_2_28_x86_64.whl", hash = "sha256:07999f5834bdc404c442146942a2ecadd1cb6292f5229f4ed3b31e0a108746b1"},
    {file = "Pillow-9.5.0-cp39-cp39-musllinux_1_1_aarch64.whl", hash = "sha256:a127ae76092974abfbfa38ca2d12cbeddcdeac0fb71f9627cc1135bedaf9d51a"},
    {file = "Pillow-9.5.0-cp39-cp39-musllinux_1_1_x86_64.whl", hash = "sha256:489f8389261e5ed43ac8ff7b453162af39c3e8abd730af8363587ba64bb2e865"},
    {file = "Pillow-9.5.0-cp39-cp39-win32.whl", hash = "sha256:9b1af95c3a967bf1da94f253e56b6286b50af23392a886720f563c547e48e964"},
    {file = "Pillow-9.5.0-cp39-cp39-win_amd64.whl", hash = "sha256:77165c4a5e7d5a284f10a6efaa39a0ae8ba839da344f20b111d62cc932fa4e5d"},
    {file = "Pillow-9.5.0-pp38-pypy38_pp73-macosx_10_10_x86_64.whl", hash = "sha256:833b86a98e0ede388fa29363159c9b1a294b0905b5128baf01db683672f230f5"},
    {file = "Pillow-9.5.0-pp38-pypy38_pp73-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:aaf305d6d40bd9632198c766fb64f0c1a83ca5b667f16c1e79e1661ab5060140"},
    {file = "Pillow-9.5.0-pp38-pypy38_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:0852ddb76d85f127c135b6dd1f0bb88dbb9ee990d2cd9aa9e28526c93e794fba"},
    {file = "Pillow-9.5.0-pp38-pypy38_pp73-manylinux_2_28_x86_64.whl", hash = "sha256:91ec6fe47b5eb5a9968c79ad9ed78c342b1f97a091677ba0e012701add857829"},
    {file = "Pillow-9.5.0-pp38-pypy38_pp73-win_amd64.whl", hash = "sha256:cb841572862f629b99725ebaec3287fc6d275be9b14443ea746c1dd325053cbd"},
    {file = "Pillow-9.5.0-pp39-pypy39_pp73-macosx_10_10_x86_64.whl", hash = "sha256:c380b27d041209b849ed246b111b7c166ba36d7933ec6e41175fd15ab9eb1572"},
    {file = "Pillow-9.5.0-pp39-pypy39_pp73-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:7c9af5a3b406a50e313467e3565fc99929717f780164fe6fbb7704edba0cebbe"},
    {file = "Pillow-9.5.0-pp39-pypy39_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:5671583eab84af046a397d6d0ba25343c00cd50bce03787948e0fff01d4fd9b1"},
    {file = "Pillow-9.5.0-pp39-pypy39_pp73-manylinux_2_28_x86_64.whl", hash = "sha256:84a6f19ce086c1bf894644b43cd129702f781ba5751ca8572f08aa40ef0ab7b7"},
    {file = "Pillow-9.5.0-pp39-pypy39_pp73-win_amd64.whl", hash = "sha256:1e7723bd90ef94eda669a3c2c19d549874dd5badaeefabefd26053304abe5799"},
    {file = "Pillow-9.5.0.tar.gz", hash = "sha256:bf548479d336726d7a0eceb6e767e179fbde37833ae42794602631a070d630f1"},
]

[package.extras]
docs = ["furo", "olefile", "sphinx (>=2.4)", "sphinx-copybutton", "sphinx-inline-tabs", "sphinx-removed-in", "sphinxext-opengraph"]
tests = ["check-manifest", "coverage", "defusedxml", "markdown2", "olefile", "packaging", "pyroma", "pytest", "pytest-cov", "pytest-timeout"]

[[package]]
name = "pinecone-client"
version = "2.2.1"
description = "Pinecone client and SDK"
category = "main"
optional = false
python-versions = ">=3.6"
files = [
    {file = "pinecone-client-2.2.1.tar.gz", hash = "sha256:0878dcaee447c46c8d1b3d71c854689daa7e548e5009a171780907c7d4e74789"},
    {file = "pinecone_client-2.2.1-py3-none-any.whl", hash = "sha256:6976a22aee57a9813378607506c8c36b0317dfa36a08a5397aaaeab2eef66c1b"},
]

[package.dependencies]
dnspython = ">=2.0.0"
loguru = ">=0.5.0"
numpy = "*"
python-dateutil = ">=2.5.3"
pyyaml = ">=5.4"
requests = ">=2.19.0"
tqdm = ">=4.64.1"
typing-extensions = ">=3.7.4"
urllib3 = ">=1.21.1"

[package.extras]
grpc = ["googleapis-common-protos (>=1.53.0)", "grpc-gateway-protoc-gen-openapiv2 (==0.1.0)", "grpcio (>=1.44.0)", "lz4 (>=3.1.3)", "protobuf (==3.19.3)"]

[[package]]
name = "pkginfo"
version = "1.9.6"
description = "Query metadata from sdists / bdists / installed packages."
category = "main"
optional = false
python-versions = ">=3.6"
files = [
    {file = "pkginfo-1.9.6-py3-none-any.whl", hash = "sha256:4b7a555a6d5a22169fcc9cf7bfd78d296b0361adad412a346c1226849af5e546"},
    {file = "pkginfo-1.9.6.tar.gz", hash = "sha256:8fd5896e8718a4372f0ea9cc9d96f6417c9b986e23a4d116dda26b62cc29d046"},
]

[package.extras]
testing = ["pytest", "pytest-cov"]

[[package]]
name = "pluggy"
version = "1.0.0"
description = "plugin and hook calling mechanisms for python"
category = "dev"
optional = false
python-versions = ">=3.6"
files = [
    {file = "pluggy-1.0.0-py2.py3-none-any.whl", hash = "sha256:74134bbf457f031a36d68416e1509f34bd5ccc019f0bcc952c7b909d06b37bd3"},
    {file = "pluggy-1.0.0.tar.gz", hash = "sha256:4224373bacce55f955a878bf9cfa763c1e360858e330072059e10bad68531159"},
]

[package.extras]
dev = ["pre-commit", "tox"]
testing = ["pytest", "pytest-benchmark"]

[[package]]
name = "portalocker"
version = "2.7.0"
description = "Wraps the portalocker recipe for easy usage"
category = "main"
optional = false
python-versions = ">=3.5"
files = [
    {file = "portalocker-2.7.0-py2.py3-none-any.whl", hash = "sha256:a07c5b4f3985c3cf4798369631fb7011adb498e2a46d8440efc75a8f29a0f983"},
    {file = "portalocker-2.7.0.tar.gz", hash = "sha256:032e81d534a88ec1736d03f780ba073f047a06c478b06e2937486f334e955c51"},
]

[package.dependencies]
pywin32 = {version = ">=226", markers = "platform_system == \"Windows\""}

[package.extras]
docs = ["sphinx (>=1.7.1)"]
redis = ["redis"]
tests = ["pytest (>=5.4.1)", "pytest-cov (>=2.8.1)", "pytest-mypy (>=0.8.0)", "pytest-timeout (>=2.1.0)", "redis", "sphinx (>=6.0.0)"]

[[package]]
name = "postgrest"
version = "0.10.6"
description = "PostgREST client for Python. This library provides an ORM interface to PostgREST."
category = "main"
optional = false
python-versions = ">=3.8,<4.0"
files = [
    {file = "postgrest-0.10.6-py3-none-any.whl", hash = "sha256:7302068ce3cd80e761e35d6d665d3e65632442488258e3299c008013119d7fe6"},
    {file = "postgrest-0.10.6.tar.gz", hash = "sha256:ee145d53ea8642a16fa7f42848443baa08ae1e6f41e071865f5f54bcb3b24aa3"},
]

[package.dependencies]
deprecation = ">=2.1.0,<3.0.0"
httpx = ">=0.23.0,<0.24.0"
pydantic = ">=1.9.0,<2.0.0"
strenum = ">=0.4.9,<0.5.0"

[[package]]
name = "posthog"
version = "3.0.1"
description = "Integrate PostHog into any python application."
category = "main"
optional = false
python-versions = "*"
files = [
    {file = "posthog-3.0.1-py2.py3-none-any.whl", hash = "sha256:9c7f92fecc713257d4b2710d05b456569c9156fbdd3e85655ba7ba5ba6c7b3ae"},
    {file = "posthog-3.0.1.tar.gz", hash = "sha256:57d2791ff5752ce56ba0f9bb8876faf3ca9208f1c2c6ceaeb5a2504c34493767"},
]

[package.dependencies]
backoff = ">=1.10.0"
monotonic = ">=1.5"
python-dateutil = ">2.1"
requests = ">=2.7,<3.0"
six = ">=1.5"

[package.extras]
dev = ["black", "flake8", "flake8-print", "isort", "pre-commit"]
sentry = ["django", "sentry-sdk"]
test = ["coverage", "flake8", "freezegun (==0.3.15)", "mock (>=2.0.0)", "pylint", "pytest"]

[[package]]
name = "protobuf"
version = "4.23.2"
description = ""
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "protobuf-4.23.2-cp310-abi3-win32.whl", hash = "sha256:384dd44cb4c43f2ccddd3645389a23ae61aeb8cfa15ca3a0f60e7c3ea09b28b3"},
    {file = "protobuf-4.23.2-cp310-abi3-win_amd64.whl", hash = "sha256:09310bce43353b46d73ba7e3bca78273b9bc50349509b9698e64d288c6372c2a"},
    {file = "protobuf-4.23.2-cp37-abi3-macosx_10_9_universal2.whl", hash = "sha256:b2cfab63a230b39ae603834718db74ac11e52bccaaf19bf20f5cce1a84cf76df"},
    {file = "protobuf-4.23.2-cp37-abi3-manylinux2014_aarch64.whl", hash = "sha256:c52cfcbfba8eb791255edd675c1fe6056f723bf832fa67f0442218f8817c076e"},
    {file = "protobuf-4.23.2-cp37-abi3-manylinux2014_x86_64.whl", hash = "sha256:86df87016d290143c7ce3be3ad52d055714ebaebb57cc659c387e76cfacd81aa"},
    {file = "protobuf-4.23.2-cp37-cp37m-win32.whl", hash = "sha256:281342ea5eb631c86697e1e048cb7e73b8a4e85f3299a128c116f05f5c668f8f"},
    {file = "protobuf-4.23.2-cp37-cp37m-win_amd64.whl", hash = "sha256:ce744938406de1e64b91410f473736e815f28c3b71201302612a68bf01517fea"},
    {file = "protobuf-4.23.2-cp38-cp38-win32.whl", hash = "sha256:6c081863c379bb1741be8f8193e893511312b1d7329b4a75445d1ea9955be69e"},
    {file = "protobuf-4.23.2-cp38-cp38-win_amd64.whl", hash = "sha256:25e3370eda26469b58b602e29dff069cfaae8eaa0ef4550039cc5ef8dc004511"},
    {file = "protobuf-4.23.2-cp39-cp39-win32.whl", hash = "sha256:efabbbbac1ab519a514579ba9ec52f006c28ae19d97915951f69fa70da2c9e91"},
    {file = "protobuf-4.23.2-cp39-cp39-win_amd64.whl", hash = "sha256:54a533b971288af3b9926e53850c7eb186886c0c84e61daa8444385a4720297f"},
    {file = "protobuf-4.23.2-py3-none-any.whl", hash = "sha256:8da6070310d634c99c0db7df48f10da495cc283fd9e9234877f0cd182d43ab7f"},
    {file = "protobuf-4.23.2.tar.gz", hash = "sha256:20874e7ca4436f683b64ebdbee2129a5a2c301579a67d1a7dda2cdf62fb7f5f7"},
]

[[package]]
name = "psycopg2"
version = "2.9.6"
description = "psycopg2 - Python-PostgreSQL Database Adapter"
category = "main"
optional = false
python-versions = ">=3.6"
files = [
    {file = "psycopg2-2.9.6-cp310-cp310-win32.whl", hash = "sha256:f7a7a5ee78ba7dc74265ba69e010ae89dae635eea0e97b055fb641a01a31d2b1"},
    {file = "psycopg2-2.9.6-cp310-cp310-win_amd64.whl", hash = "sha256:f75001a1cbbe523e00b0ef896a5a1ada2da93ccd752b7636db5a99bc57c44494"},
    {file = "psycopg2-2.9.6-cp311-cp311-win32.whl", hash = "sha256:53f4ad0a3988f983e9b49a5d9765d663bbe84f508ed655affdb810af9d0972ad"},
    {file = "psycopg2-2.9.6-cp311-cp311-win_amd64.whl", hash = "sha256:b81fcb9ecfc584f661b71c889edeae70bae30d3ef74fa0ca388ecda50b1222b7"},
    {file = "psycopg2-2.9.6-cp36-cp36m-win32.whl", hash = "sha256:11aca705ec888e4f4cea97289a0bf0f22a067a32614f6ef64fcf7b8bfbc53744"},
    {file = "psycopg2-2.9.6-cp36-cp36m-win_amd64.whl", hash = "sha256:36c941a767341d11549c0fbdbb2bf5be2eda4caf87f65dfcd7d146828bd27f39"},
    {file = "psycopg2-2.9.6-cp37-cp37m-win32.whl", hash = "sha256:869776630c04f335d4124f120b7fb377fe44b0a7645ab3c34b4ba42516951889"},
    {file = "psycopg2-2.9.6-cp37-cp37m-win_amd64.whl", hash = "sha256:a8ad4a47f42aa6aec8d061fdae21eaed8d864d4bb0f0cade5ad32ca16fcd6258"},
    {file = "psycopg2-2.9.6-cp38-cp38-win32.whl", hash = "sha256:2362ee4d07ac85ff0ad93e22c693d0f37ff63e28f0615a16b6635a645f4b9214"},
    {file = "psycopg2-2.9.6-cp38-cp38-win_amd64.whl", hash = "sha256:d24ead3716a7d093b90b27b3d73459fe8cd90fd7065cf43b3c40966221d8c394"},
    {file = "psycopg2-2.9.6-cp39-cp39-win32.whl", hash = "sha256:1861a53a6a0fd248e42ea37c957d36950da00266378746588eab4f4b5649e95f"},
    {file = "psycopg2-2.9.6-cp39-cp39-win_amd64.whl", hash = "sha256:ded2faa2e6dfb430af7713d87ab4abbfc764d8d7fb73eafe96a24155f906ebf5"},
    {file = "psycopg2-2.9.6.tar.gz", hash = "sha256:f15158418fd826831b28585e2ab48ed8df2d0d98f502a2b4fe619e7d5ca29011"},
]

[[package]]
name = "psycopg2cffi"
version = "2.9.0"
description = ".. image:: https://travis-ci.org/chtd/psycopg2cffi.svg?branch=master"
category = "main"
optional = true
python-versions = "*"
files = [
    {file = "psycopg2cffi-2.9.0.tar.gz", hash = "sha256:7e272edcd837de3a1d12b62185eb85c45a19feda9e62fa1b120c54f9e8d35c52"},
]

[package.dependencies]
cffi = ">=1.0"
six = "*"

[[package]]
name = "pycparser"
version = "2.21"
description = "C parser in Python"
category = "main"
optional = false
python-versions = ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*"
files = [
    {file = "pycparser-2.21-py2.py3-none-any.whl", hash = "sha256:8ee45429555515e1f6b185e78100aea234072576aa43ab53aefcae078162fca9"},
    {file = "pycparser-2.21.tar.gz", hash = "sha256:e644fdec12f7872f86c58ff790da456218b10f863970249516d60a5eaca77206"},
]

[[package]]
name = "pycryptodomex"
version = "3.18.0"
description = "Cryptographic library for Python"
category = "main"
optional = false
python-versions = ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*"
files = [
    {file = "pycryptodomex-3.18.0-cp27-cp27m-macosx_10_9_x86_64.whl", hash = "sha256:160a39a708c36fa0b168ab79386dede588e62aec06eb505add870739329aecc6"},
    {file = "pycryptodomex-3.18.0-cp27-cp27m-manylinux2010_i686.whl", hash = "sha256:c2953afebf282a444c51bf4effe751706b4d0d63d7ca2cc51db21f902aa5b84e"},
    {file = "pycryptodomex-3.18.0-cp27-cp27m-manylinux2010_x86_64.whl", hash = "sha256:ba95abd563b0d1b88401658665a260852a8e6c647026ee6a0a65589287681df8"},
    {file = "pycryptodomex-3.18.0-cp27-cp27m-manylinux2014_aarch64.whl", hash = "sha256:192306cf881fe3467dda0e174a4f47bb3a8bb24b90c9cdfbdc248eec5fc0578c"},
    {file = "pycryptodomex-3.18.0-cp27-cp27m-musllinux_1_1_aarch64.whl", hash = "sha256:f9ab5ef0718f6a8716695dea16d83b671b22c45e9c0c78fd807c32c0192e54b5"},
    {file = "pycryptodomex-3.18.0-cp27-cp27m-win32.whl", hash = "sha256:50308fcdbf8345e5ec224a5502b4215178bdb5e95456ead8ab1a69ffd94779cb"},
    {file = "pycryptodomex-3.18.0-cp27-cp27m-win_amd64.whl", hash = "sha256:4d9379c684efea80fdab02a3eb0169372bca7db13f9332cb67483b8dc8b67c37"},
    {file = "pycryptodomex-3.18.0-cp27-cp27mu-manylinux2010_i686.whl", hash = "sha256:5594a125dae30d60e94f37797fc67ce3c744522de7992c7c360d02fdb34918f8"},
    {file = "pycryptodomex-3.18.0-cp27-cp27mu-manylinux2010_x86_64.whl", hash = "sha256:8ff129a5a0eb5ff16e45ca4fa70a6051da7f3de303c33b259063c19be0c43d35"},
    {file = "pycryptodomex-3.18.0-cp27-cp27mu-manylinux2014_aarch64.whl", hash = "sha256:3d9314ac785a5b75d5aaf924c5f21d6ca7e8df442e5cf4f0fefad4f6e284d422"},
    {file = "pycryptodomex-3.18.0-cp27-cp27mu-musllinux_1_1_aarch64.whl", hash = "sha256:f237278836dda412a325e9340ba2e6a84cb0f56b9244781e5b61f10b3905de88"},
    {file = "pycryptodomex-3.18.0-cp35-abi3-macosx_10_9_universal2.whl", hash = "sha256:ac614363a86cc53d8ba44b6c469831d1555947e69ab3276ae8d6edc219f570f7"},
    {file = "pycryptodomex-3.18.0-cp35-abi3-macosx_10_9_x86_64.whl", hash = "sha256:302a8f37c224e7b5d72017d462a2be058e28f7be627bdd854066e16722d0fc0c"},
    {file = "pycryptodomex-3.18.0-cp35-abi3-manylinux2014_aarch64.whl", hash = "sha256:6421d23d6a648e83ba2670a352bcd978542dad86829209f59d17a3f087f4afef"},
    {file = "pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:d84e105787f5e5d36ec6a581ff37a1048d12e638688074b2a00bcf402f9aa1c2"},
    {file = "pycryptodomex-3.18.0-cp35-abi3-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:6875eb8666f68ddbd39097867325bd22771f595b4e2b0149739b5623c8bf899b"},
    {file = "pycryptodomex-3.18.0-cp35-abi3-musllinux_1_1_aarch64.whl", hash = "sha256:27072a494ce621cc7a9096bbf60ed66826bb94db24b49b7359509e7951033e74"},
    {file = "pycryptodomex-3.18.0-cp35-abi3-musllinux_1_1_i686.whl", hash = "sha256:1949e09ea49b09c36d11a951b16ff2a05a0ffe969dda1846e4686ee342fe8646"},
    {file = "pycryptodomex-3.18.0-cp35-abi3-musllinux_1_1_x86_64.whl", hash = "sha256:6ed3606832987018615f68e8ed716a7065c09a0fe94afd7c9ca1b6777f0ac6eb"},
    {file = "pycryptodomex-3.18.0-cp35-abi3-win32.whl", hash = "sha256:d56c9ec41258fd3734db9f5e4d2faeabe48644ba9ca23b18e1839b3bdf093222"},
    {file = "pycryptodomex-3.18.0-cp35-abi3-win_amd64.whl", hash = "sha256:e00a4bacb83a2627e8210cb353a2e31f04befc1155db2976e5e239dd66482278"},
    {file = "pycryptodomex-3.18.0-pp27-pypy_73-manylinux2010_x86_64.whl", hash = "sha256:2dc4eab20f4f04a2d00220fdc9258717b82d31913552e766d5f00282c031b70a"},
    {file = "pycryptodomex-3.18.0-pp27-pypy_73-win32.whl", hash = "sha256:75672205148bdea34669173366df005dbd52be05115e919551ee97171083423d"},
    {file = "pycryptodomex-3.18.0-pp38-pypy38_pp73-macosx_10_9_x86_64.whl", hash = "sha256:bec6c80994d4e7a38312072f89458903b65ec99bed2d65aa4de96d997a53ea7a"},
    {file = "pycryptodomex-3.18.0-pp38-pypy38_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:d35a8ffdc8b05e4b353ba281217c8437f02c57d7233363824e9d794cf753c419"},
    {file = "pycryptodomex-3.18.0-pp38-pypy38_pp73-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:76f0a46bee539dae4b3dfe37216f678769349576b0080fdbe431d19a02da42ff"},
    {file = "pycryptodomex-3.18.0-pp38-pypy38_pp73-win_amd64.whl", hash = "sha256:71687eed47df7e965f6e0bf3cadef98f368d5221f0fb89d2132effe1a3e6a194"},
    {file = "pycryptodomex-3.18.0-pp39-pypy39_pp73-macosx_10_9_x86_64.whl", hash = "sha256:73d64b32d84cf48d9ec62106aa277dbe99ab5fbfd38c5100bc7bddd3beb569f7"},
    {file = "pycryptodomex-3.18.0-pp39-pypy39_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:bbdcce0a226d9205560a5936b05208c709b01d493ed8307792075dedfaaffa5f"},
    {file = "pycryptodomex-3.18.0-pp39-pypy39_pp73-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:58fc0aceb9c961b9897facec9da24c6a94c5db04597ec832060f53d4d6a07196"},
    {file = "pycryptodomex-3.18.0-pp39-pypy39_pp73-win_amd64.whl", hash = "sha256:215be2980a6b70704c10796dd7003eb4390e7be138ac6fb8344bf47e71a8d470"},
    {file = "pycryptodomex-3.18.0.tar.gz", hash = "sha256:3e3ecb5fe979e7c1bb0027e518340acf7ee60415d79295e5251d13c68dde576e"},
]

[[package]]
name = "pydantic"
version = "1.10.8"
description = "Data validation and settings management using python type hints"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "pydantic-1.10.8-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:1243d28e9b05003a89d72e7915fdb26ffd1d39bdd39b00b7dbe4afae4b557f9d"},
    {file = "pydantic-1.10.8-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:c0ab53b609c11dfc0c060d94335993cc2b95b2150e25583bec37a49b2d6c6c3f"},
    {file = "pydantic-1.10.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:f9613fadad06b4f3bc5db2653ce2f22e0de84a7c6c293909b48f6ed37b83c61f"},
    {file = "pydantic-1.10.8-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:df7800cb1984d8f6e249351139667a8c50a379009271ee6236138a22a0c0f319"},
    {file = "pydantic-1.10.8-cp310-cp310-musllinux_1_1_i686.whl", hash = "sha256:0c6fafa0965b539d7aab0a673a046466d23b86e4b0e8019d25fd53f4df62c277"},
    {file = "pydantic-1.10.8-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:e82d4566fcd527eae8b244fa952d99f2ca3172b7e97add0b43e2d97ee77f81ab"},
    {file = "pydantic-1.10.8-cp310-cp310-win_amd64.whl", hash = "sha256:ab523c31e22943713d80d8d342d23b6f6ac4b792a1e54064a8d0cf78fd64e800"},
    {file = "pydantic-1.10.8-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:666bdf6066bf6dbc107b30d034615d2627e2121506c555f73f90b54a463d1f33"},
    {file = "pydantic-1.10.8-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:35db5301b82e8661fa9c505c800d0990bc14e9f36f98932bb1d248c0ac5cada5"},
    {file = "pydantic-1.10.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:f90c1e29f447557e9e26afb1c4dbf8768a10cc676e3781b6a577841ade126b85"},
    {file = "pydantic-1.10.8-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:93e766b4a8226e0708ef243e843105bf124e21331694367f95f4e3b4a92bbb3f"},
    {file = "pydantic-1.10.8-cp311-cp311-musllinux_1_1_i686.whl", hash = "sha256:88f195f582851e8db960b4a94c3e3ad25692c1c1539e2552f3df7a9e972ef60e"},
    {file = "pydantic-1.10.8-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:34d327c81e68a1ecb52fe9c8d50c8a9b3e90d3c8ad991bfc8f953fb477d42fb4"},
    {file = "pydantic-1.10.8-cp311-cp311-win_amd64.whl", hash = "sha256:d532bf00f381bd6bc62cabc7d1372096b75a33bc197a312b03f5838b4fb84edd"},
    {file = "pydantic-1.10.8-cp37-cp37m-macosx_10_9_x86_64.whl", hash = "sha256:7d5b8641c24886d764a74ec541d2fc2c7fb19f6da2a4001e6d580ba4a38f7878"},
    {file = "pydantic-1.10.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:7b1f6cb446470b7ddf86c2e57cd119a24959af2b01e552f60705910663af09a4"},
    {file = "pydantic-1.10.8-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:c33b60054b2136aef8cf190cd4c52a3daa20b2263917c49adad20eaf381e823b"},
    {file = "pydantic-1.10.8-cp37-cp37m-musllinux_1_1_i686.whl", hash = "sha256:1952526ba40b220b912cdc43c1c32bcf4a58e3f192fa313ee665916b26befb68"},
    {file = "pydantic-1.10.8-cp37-cp37m-musllinux_1_1_x86_64.whl", hash = "sha256:bb14388ec45a7a0dc429e87def6396f9e73c8c77818c927b6a60706603d5f2ea"},
    {file = "pydantic-1.10.8-cp37-cp37m-win_amd64.whl", hash = "sha256:16f8c3e33af1e9bb16c7a91fc7d5fa9fe27298e9f299cff6cb744d89d573d62c"},
    {file = "pydantic-1.10.8-cp38-cp38-macosx_10_9_x86_64.whl", hash = "sha256:1ced8375969673929809d7f36ad322934c35de4af3b5e5b09ec967c21f9f7887"},
    {file = "pydantic-1.10.8-cp38-cp38-macosx_11_0_arm64.whl", hash = "sha256:93e6bcfccbd831894a6a434b0aeb1947f9e70b7468f274154d03d71fabb1d7c6"},
    {file = "pydantic-1.10.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:191ba419b605f897ede9892f6c56fb182f40a15d309ef0142212200a10af4c18"},
    {file = "pydantic-1.10.8-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:052d8654cb65174d6f9490cc9b9a200083a82cf5c3c5d3985db765757eb3b375"},
    {file = "pydantic-1.10.8-cp38-cp38-musllinux_1_1_i686.whl", hash = "sha256:ceb6a23bf1ba4b837d0cfe378329ad3f351b5897c8d4914ce95b85fba96da5a1"},
    {file = "pydantic-1.10.8-cp38-cp38-musllinux_1_1_x86_64.whl", hash = "sha256:6f2e754d5566f050954727c77f094e01793bcb5725b663bf628fa6743a5a9108"},
    {file = "pydantic-1.10.8-cp38-cp38-win_amd64.whl", hash = "sha256:6a82d6cda82258efca32b40040228ecf43a548671cb174a1e81477195ed3ed56"},
    {file = "pydantic-1.10.8-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:3e59417ba8a17265e632af99cc5f35ec309de5980c440c255ab1ca3ae96a3e0e"},
    {file = "pydantic-1.10.8-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:84d80219c3f8d4cad44575e18404099c76851bc924ce5ab1c4c8bb5e2a2227d0"},
    {file = "pydantic-1.10.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:2e4148e635994d57d834be1182a44bdb07dd867fa3c2d1b37002000646cc5459"},
    {file = "pydantic-1.10.8-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:12f7b0bf8553e310e530e9f3a2f5734c68699f42218bf3568ef49cd9b0e44df4"},
    {file = "pydantic-1.10.8-cp39-cp39-musllinux_1_1_i686.whl", hash = "sha256:42aa0c4b5c3025483240a25b09f3c09a189481ddda2ea3a831a9d25f444e03c1"},
    {file = "pydantic-1.10.8-cp39-cp39-musllinux_1_1_x86_64.whl", hash = "sha256:17aef11cc1b997f9d574b91909fed40761e13fac438d72b81f902226a69dac01"},
    {file = "pydantic-1.10.8-cp39-cp39-win_amd64.whl", hash = "sha256:66a703d1983c675a6e0fed8953b0971c44dba48a929a2000a493c3772eb61a5a"},
    {file = "pydantic-1.10.8-py3-none-any.whl", hash = "sha256:7456eb22ed9aaa24ff3e7b4757da20d9e5ce2a81018c1b3ebd81a0b88a18f3b2"},
    {file = "pydantic-1.10.8.tar.gz", hash = "sha256:1410275520dfa70effadf4c21811d755e7ef9bb1f1d077a21958153a92c8d9ca"},
]

[package.dependencies]
typing-extensions = ">=4.2.0"

[package.extras]
dotenv = ["python-dotenv (>=0.10.4)"]
email = ["email-validator (>=1.0.3)"]

[[package]]
name = "pygments"
version = "2.15.1"
description = "Pygments is a syntax highlighting package written in Python."
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "Pygments-2.15.1-py3-none-any.whl", hash = "sha256:db2db3deb4b4179f399a09054b023b6a586b76499d36965813c71aa8ed7b5fd1"},
    {file = "Pygments-2.15.1.tar.gz", hash = "sha256:8ace4d3c1dd481894b2005f560ead0f9f19ee64fe983366be1a21e171d12775c"},
]

[package.extras]
plugins = ["importlib-metadata"]

[[package]]
name = "pyjwt"
version = "2.7.0"
description = "JSON Web Token implementation in Python"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "PyJWT-2.7.0-py3-none-any.whl", hash = "sha256:ba2b425b15ad5ef12f200dc67dd56af4e26de2331f965c5439994dad075876e1"},
    {file = "PyJWT-2.7.0.tar.gz", hash = "sha256:bd6ca4a3c4285c1a2d4349e5a035fdf8fb94e04ccd0fcbe6ba289dae9cc3e074"},
]

[package.dependencies]
cryptography = {version = ">=3.4.0", optional = true, markers = "extra == \"crypto\""}

[package.extras]
crypto = ["cryptography (>=3.4.0)"]
dev = ["coverage[toml] (==5.0.4)", "cryptography (>=3.4.0)", "pre-commit", "pytest (>=6.0.0,<7.0.0)", "sphinx (>=4.5.0,<5.0.0)", "sphinx-rtd-theme", "zope.interface"]
docs = ["sphinx (>=4.5.0,<5.0.0)", "sphinx-rtd-theme", "zope.interface"]
tests = ["coverage[toml] (==5.0.4)", "pytest (>=6.0.0,<7.0.0)"]

[[package]]
name = "pymilvus"
version = "2.2.8"
description = "Python Sdk for Milvus"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "pymilvus-2.2.8-py3-none-any.whl", hash = "sha256:ccf427700c886b3cd73455b40fdac0bf2087243ba1eac5cb6d5745571d1ae467"},
    {file = "pymilvus-2.2.8.tar.gz", hash = "sha256:f866c44a17403a9b7ba681f0c0a28aa8cdecba0f18b8c43f31879d627bd7d1e8"},
]

[package.dependencies]
environs = "<=9.5.0"
grpcio = ">=1.49.1,<=1.53.0"
pandas = ">=1.2.4"
protobuf = ">=3.20.0"
ujson = ">=2.0.0"

[[package]]
name = "pypdf2"
version = "3.0.1"
description = "A pure-python PDF library capable of splitting, merging, cropping, and transforming PDF files"
category = "main"
optional = false
python-versions = ">=3.6"
files = [
    {file = "PyPDF2-3.0.1.tar.gz", hash = "sha256:a74408f69ba6271f71b9352ef4ed03dc53a31aa404d29b5d31f53bfecfee1440"},
    {file = "pypdf2-3.0.1-py3-none-any.whl", hash = "sha256:d16e4205cfee272fbdc0568b68d82be796540b1537508cef59388f839c191928"},
]

[package.extras]
crypto = ["PyCryptodome"]
dev = ["black", "flit", "pip-tools", "pre-commit (<2.18.0)", "pytest-cov", "wheel"]
docs = ["myst_parser", "sphinx", "sphinx_rtd_theme"]
full = ["Pillow", "PyCryptodome"]
image = ["Pillow"]

[[package]]
name = "pyreadline3"
version = "3.4.1"
description = "A python implementation of GNU readline."
category = "main"
optional = false
python-versions = "*"
files = [
    {file = "pyreadline3-3.4.1-py3-none-any.whl", hash = "sha256:b0efb6516fd4fb07b45949053826a62fa4cb353db5be2bbb4a7aa1fdd1e345fb"},
    {file = "pyreadline3-3.4.1.tar.gz", hash = "sha256:6f3d1f7b8a31ba32b73917cefc1f28cc660562f39aea8646d30bd6eff21f7bae"},
]

[[package]]
name = "pytest"
version = "7.3.1"
description = "pytest: simple powerful testing with Python"
category = "dev"
optional = false
python-versions = ">=3.7"
files = [
    {file = "pytest-7.3.1-py3-none-any.whl", hash = "sha256:3799fa815351fea3a5e96ac7e503a96fa51cc9942c3753cda7651b93c1cfa362"},
    {file = "pytest-7.3.1.tar.gz", hash = "sha256:434afafd78b1d78ed0addf160ad2b77a30d35d4bdf8af234fe621919d9ed15e3"},
]

[package.dependencies]
colorama = {version = "*", markers = "sys_platform == \"win32\""}
exceptiongroup = {version = ">=1.0.0rc8", markers = "python_version < \"3.11\""}
iniconfig = "*"
packaging = "*"
pluggy = ">=0.12,<2.0"
tomli = {version = ">=1.0.0", markers = "python_version < \"3.11\""}

[package.extras]
testing = ["argcomplete", "attrs (>=19.2.0)", "hypothesis (>=3.56)", "mock", "nose", "pygments (>=2.7.2)", "requests", "xmlschema"]

[[package]]
name = "pytest-asyncio"
version = "0.20.3"
description = "Pytest support for asyncio"
category = "dev"
optional = false
python-versions = ">=3.7"
files = [
    {file = "pytest-asyncio-0.20.3.tar.gz", hash = "sha256:83cbf01169ce3e8eb71c6c278ccb0574d1a7a3bb8eaaf5e50e0ad342afb33b36"},
    {file = "pytest_asyncio-0.20.3-py3-none-any.whl", hash = "sha256:f129998b209d04fcc65c96fc85c11e5316738358909a8399e93be553d7656442"},
]

[package.dependencies]
pytest = ">=6.1.0"

[package.extras]
docs = ["sphinx (>=5.3)", "sphinx-rtd-theme (>=1.0)"]
testing = ["coverage (>=6.2)", "flaky (>=3.5.0)", "hypothesis (>=5.7.1)", "mypy (>=0.931)", "pytest-trio (>=0.7.0)"]

[[package]]
name = "pytest-cov"
version = "4.1.0"
description = "Pytest plugin for measuring coverage."
category = "dev"
optional = false
python-versions = ">=3.7"
files = [
    {file = "pytest-cov-4.1.0.tar.gz", hash = "sha256:3904b13dfbfec47f003b8e77fd5b589cd11904a21ddf1ab38a64f204d6a10ef6"},
    {file = "pytest_cov-4.1.0-py3-none-any.whl", hash = "sha256:6ba70b9e97e69fcc3fb45bfeab2d0a138fb65c4d0d6a41ef33983ad114be8c3a"},
]

[package.dependencies]
coverage = {version = ">=5.2.1", extras = ["toml"]}
pytest = ">=4.6"

[package.extras]
testing = ["fields", "hunter", "process-tests", "pytest-xdist", "six", "virtualenv"]

[[package]]
name = "python-dateutil"
version = "2.8.2"
description = "Extensions to the standard Python datetime module"
category = "main"
optional = false
python-versions = "!=3.0.*,!=3.1.*,!=3.2.*,>=2.7"
files = [
    {file = "python-dateutil-2.8.2.tar.gz", hash = "sha256:0123cacc1627ae19ddf3c27a5de5bd67ee4586fbdd6440d9748f8abb483d3e86"},
    {file = "python_dateutil-2.8.2-py2.py3-none-any.whl", hash = "sha256:961d03dc3453ebbc59dbdea9e4e11c5651520a876d0f4db161e8674aae935da9"},
]

[package.dependencies]
six = ">=1.5"

[[package]]
name = "python-dotenv"
version = "0.21.1"
description = "Read key-value pairs from a .env file and set them as environment variables"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "python-dotenv-0.21.1.tar.gz", hash = "sha256:1c93de8f636cde3ce377292818d0e440b6e45a82f215c3744979151fa8151c49"},
    {file = "python_dotenv-0.21.1-py3-none-any.whl", hash = "sha256:41e12e0318bebc859fcc4d97d4db8d20ad21721a6aa5047dd59f090391cb549a"},
]

[package.extras]
cli = ["click (>=5.0)"]

[[package]]
name = "python-gitlab"
version = "3.14.0"
description = "Interact with GitLab API"
category = "main"
optional = false
python-versions = ">=3.7.0"
files = [
    {file = "python-gitlab-3.14.0.tar.gz", hash = "sha256:ef3b8960faeee9880f82b0872d807e3fab94ace12b0d2a8418a97875c8812d3c"},
    {file = "python_gitlab-3.14.0-py3-none-any.whl", hash = "sha256:da614c014c6860147783dde8c216218d8fc6bd83a8bd2e3929dcdf11b211aa58"},
]

[package.dependencies]
requests = ">=2.25.0"
requests-toolbelt = ">=0.10.1"

[package.extras]
autocompletion = ["argcomplete (>=1.10.0,<3)"]
yaml = ["PyYaml (>=5.2)"]

[[package]]
name = "python-multipart"
version = "0.0.6"
description = "A streaming multipart parser for Python"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "python_multipart-0.0.6-py3-none-any.whl", hash = "sha256:ee698bab5ef148b0a760751c261902cd096e57e10558e11aca17646b74ee1c18"},
    {file = "python_multipart-0.0.6.tar.gz", hash = "sha256:e9925a80bb668529f1b67c7fdb0a5dacdd7cbfc6fb0bff3ea443fe22bdd62132"},
]

[package.extras]
dev = ["atomicwrites (==1.2.1)", "attrs (==19.2.0)", "coverage (==6.5.0)", "hatch", "invoke (==1.7.3)", "more-itertools (==4.3.0)", "pbr (==4.3.0)", "pluggy (==1.0.0)", "py (==1.11.0)", "pytest (==7.2.0)", "pytest-cov (==4.0.0)", "pytest-timeout (==2.1.0)", "pyyaml (==5.1)"]

[[package]]
name = "python-pptx"
version = "0.6.21"
description = "Generate and manipulate Open XML PowerPoint (.pptx) files"
category = "main"
optional = false
python-versions = "*"
files = [
    {file = "python-pptx-0.6.21.tar.gz", hash = "sha256:7798a2aaf89563565b3c7120c0acfe9aff775db0db3580544e3bf4840c2e378f"},
]

[package.dependencies]
lxml = ">=3.1.0"
Pillow = ">=3.3.2"
XlsxWriter = ">=0.5.7"

[[package]]
name = "python-semantic-release"
version = "7.33.2"
description = "Automatic Semantic Versioning for Python projects"
category = "main"
optional = false
python-versions = "*"
files = [
    {file = "python-semantic-release-7.33.2.tar.gz", hash = "sha256:c23b4bb746e9ddbe1ba7497c48f7d81403e67a14ceb37928ef667c1fbee5e324"},
    {file = "python_semantic_release-7.33.2-py3-none-any.whl", hash = "sha256:9e4990cc0a4dc37482ac5ec7fe6f70f71681228f68f0fa39370415701fdcf632"},
]

[package.dependencies]
click = ">=7,<9"
click-log = ">=0.3,<1"
dotty-dict = ">=1.3.0,<2"
gitpython = ">=3.0.8,<4"
invoke = ">=1.4.1,<2"
packaging = "*"
python-gitlab = ">=2,<4"
requests = ">=2.25,<3"
semver = ">=2.10,<3"
tomlkit = ">=0.10,<1.0"
twine = ">=3,<4"
wheel = "*"

[package.extras]
dev = ["black", "isort", "tox"]
docs = ["Jinja2 (==3.0.3)", "Sphinx (==1.3.6)"]
mypy = ["mypy", "types-requests"]
test = ["coverage (>=5,<6)", "mock (==1.3.0)", "pytest (>=7,<8)", "pytest-mock (>=2,<3)", "pytest-xdist (>=1,<2)", "responses (==0.13.3)"]

[[package]]
name = "pytz"
version = "2023.3"
description = "World timezone definitions, modern and historical"
category = "main"
optional = false
python-versions = "*"
files = [
    {file = "pytz-2023.3-py2.py3-none-any.whl", hash = "sha256:a151b3abb88eda1d4e34a9814df37de2a80e301e68ba0fd856fb9b46bfbbbffb"},
    {file = "pytz-2023.3.tar.gz", hash = "sha256:1d8ce29db189191fb55338ee6d0387d82ab59f3d00eac103412d64e0ebd0c588"},
]

[[package]]
name = "pywin32"
version = "306"
description = "Python for Window Extensions"
category = "main"
optional = false
python-versions = "*"
files = [
    {file = "pywin32-306-cp310-cp310-win32.whl", hash = "sha256:06d3420a5155ba65f0b72f2699b5bacf3109f36acbe8923765c22938a69dfc8d"},
    {file = "pywin32-306-cp310-cp310-win_amd64.whl", hash = "sha256:84f4471dbca1887ea3803d8848a1616429ac94a4a8d05f4bc9c5dcfd42ca99c8"},
    {file = "pywin32-306-cp311-cp311-win32.whl", hash = "sha256:e65028133d15b64d2ed8f06dd9fbc268352478d4f9289e69c190ecd6818b6407"},
    {file = "pywin32-306-cp311-cp311-win_amd64.whl", hash = "sha256:a7639f51c184c0272e93f244eb24dafca9b1855707d94c192d4a0b4c01e1100e"},
    {file = "pywin32-306-cp311-cp311-win_arm64.whl", hash = "sha256:70dba0c913d19f942a2db25217d9a1b726c278f483a919f1abfed79c9cf64d3a"},
    {file = "pywin32-306-cp312-cp312-win32.whl", hash = "sha256:383229d515657f4e3ed1343da8be101000562bf514591ff383ae940cad65458b"},
    {file = "pywin32-306-cp312-cp312-win_amd64.whl", hash = "sha256:37257794c1ad39ee9be652da0462dc2e394c8159dfd913a8a4e8eb6fd346da0e"},
    {file = "pywin32-306-cp312-cp312-win_arm64.whl", hash = "sha256:5821ec52f6d321aa59e2db7e0a35b997de60c201943557d108af9d4ae1ec7040"},
    {file = "pywin32-306-cp37-cp37m-win32.whl", hash = "sha256:1c73ea9a0d2283d889001998059f5eaaba3b6238f767c9cf2833b13e6a685f65"},
    {file = "pywin32-306-cp37-cp37m-win_amd64.whl", hash = "sha256:72c5f621542d7bdd4fdb716227be0dd3f8565c11b280be6315b06ace35487d36"},
    {file = "pywin32-306-cp38-cp38-win32.whl", hash = "sha256:e4c092e2589b5cf0d365849e73e02c391c1349958c5ac3e9d5ccb9a28e017b3a"},
    {file = "pywin32-306-cp38-cp38-win_amd64.whl", hash = "sha256:e8ac1ae3601bee6ca9f7cb4b5363bf1c0badb935ef243c4733ff9a393b1690c0"},
    {file = "pywin32-306-cp39-cp39-win32.whl", hash = "sha256:e25fd5b485b55ac9c057f67d94bc203f3f6595078d1fb3b458c9c28b7153a802"},
    {file = "pywin32-306-cp39-cp39-win_amd64.whl", hash = "sha256:39b61c15272833b5c329a2989999dcae836b1eed650252ab1b7bfbe1d59f30f4"},
]

[[package]]
name = "pywin32-ctypes"
version = "0.2.0"
description = ""
category = "main"
optional = false
python-versions = "*"
files = [
    {file = "pywin32-ctypes-0.2.0.tar.gz", hash = "sha256:24ffc3b341d457d48e8922352130cf2644024a4ff09762a2261fd34c36ee5942"},
    {file = "pywin32_ctypes-0.2.0-py2.py3-none-any.whl", hash = "sha256:9dc2d991b3479cc2df15930958b674a48a227d5361d413827a4cfd0b5876fc98"},
]

[[package]]
name = "pyyaml"
version = "6.0"
description = "YAML parser and emitter for Python"
category = "main"
optional = false
python-versions = ">=3.6"
files = [
    {file = "PyYAML-6.0-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:d4db7c7aef085872ef65a8fd7d6d09a14ae91f691dec3e87ee5ee0539d516f53"},
    {file = "PyYAML-6.0-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:9df7ed3b3d2e0ecfe09e14741b857df43adb5a3ddadc919a2d94fbdf78fea53c"},
    {file = "PyYAML-6.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:77f396e6ef4c73fdc33a9157446466f1cff553d979bd00ecb64385760c6babdc"},
    {file = "PyYAML-6.0-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:a80a78046a72361de73f8f395f1f1e49f956c6be882eed58505a15f3e430962b"},
    {file = "PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl", hash = "sha256:f84fbc98b019fef2ee9a1cb3ce93e3187a6df0b2538a651bfb890254ba9f90b5"},
    {file = "PyYAML-6.0-cp310-cp310-win32.whl", hash = "sha256:2cd5df3de48857ed0544b34e2d40e9fac445930039f3cfe4bcc592a1f836d513"},
    {file = "PyYAML-6.0-cp310-cp310-win_amd64.whl", hash = "sha256:daf496c58a8c52083df09b80c860005194014c3698698d1a57cbcfa182142a3a"},
    {file = "PyYAML-6.0-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:d4b0ba9512519522b118090257be113b9468d804b19d63c71dbcf4a48fa32358"},
    {file = "PyYAML-6.0-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:81957921f441d50af23654aa6c5e5eaf9b06aba7f0a19c18a538dc7ef291c5a1"},
    {file = "PyYAML-6.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:afa17f5bc4d1b10afd4466fd3a44dc0e245382deca5b3c353d8b757f9e3ecb8d"},
    {file = "PyYAML-6.0-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:dbad0e9d368bb989f4515da330b88a057617d16b6a8245084f1b05400f24609f"},
    {file = "PyYAML-6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:432557aa2c09802be39460360ddffd48156e30721f5e8d917f01d31694216782"},
    {file = "PyYAML-6.0-cp311-cp311-win32.whl", hash = "sha256:bfaef573a63ba8923503d27530362590ff4f576c626d86a9fed95822a8255fd7"},
    {file = "PyYAML-6.0-cp311-cp311-win_amd64.whl", hash = "sha256:01b45c0191e6d66c470b6cf1b9531a771a83c1c4208272ead47a3ae4f2f603bf"},
    {file = "PyYAML-6.0-cp36-cp36m-macosx_10_9_x86_64.whl", hash = "sha256:897b80890765f037df3403d22bab41627ca8811ae55e9a722fd0392850ec4d86"},
    {file = "PyYAML-6.0-cp36-cp36m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:50602afada6d6cbfad699b0c7bb50d5ccffa7e46a3d738092afddc1f9758427f"},
    {file = "PyYAML-6.0-cp36-cp36m-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:48c346915c114f5fdb3ead70312bd042a953a8ce5c7106d5bfb1a5254e47da92"},
    {file = "PyYAML-6.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl", hash = "sha256:98c4d36e99714e55cfbaaee6dd5badbc9a1ec339ebfc3b1f52e293aee6bb71a4"},
    {file = "PyYAML-6.0-cp36-cp36m-win32.whl", hash = "sha256:0283c35a6a9fbf047493e3a0ce8d79ef5030852c51e9d911a27badfde0605293"},
    {file = "PyYAML-6.0-cp36-cp36m-win_amd64.whl", hash = "sha256:07751360502caac1c067a8132d150cf3d61339af5691fe9e87803040dbc5db57"},
    {file = "PyYAML-6.0-cp37-cp37m-macosx_10_9_x86_64.whl", hash = "sha256:819b3830a1543db06c4d4b865e70ded25be52a2e0631ccd2f6a47a2822f2fd7c"},
    {file = "PyYAML-6.0-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:473f9edb243cb1935ab5a084eb238d842fb8f404ed2193a915d1784b5a6b5fc0"},
    {file = "PyYAML-6.0-cp37-cp37m-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:0ce82d761c532fe4ec3f87fc45688bdd3a4c1dc5e0b4a19814b9009a29baefd4"},
    {file = "PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl", hash = "sha256:231710d57adfd809ef5d34183b8ed1eeae3f76459c18fb4a0b373ad56bedcdd9"},
    {file = "PyYAML-6.0-cp37-cp37m-win32.whl", hash = "sha256:c5687b8d43cf58545ade1fe3e055f70eac7a5a1a0bf42824308d868289a95737"},
    {file = "PyYAML-6.0-cp37-cp37m-win_amd64.whl", hash = "sha256:d15a181d1ecd0d4270dc32edb46f7cb7733c7c508857278d3d378d14d606db2d"},
    {file = "PyYAML-6.0-cp38-cp38-macosx_10_9_x86_64.whl", hash = "sha256:0b4624f379dab24d3725ffde76559cff63d9ec94e1736b556dacdfebe5ab6d4b"},
    {file = "PyYAML-6.0-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:213c60cd50106436cc818accf5baa1aba61c0189ff610f64f4a3e8c6726218ba"},
    {file = "PyYAML-6.0-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:9fa600030013c4de8165339db93d182b9431076eb98eb40ee068700c9c813e34"},
    {file = "PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl", hash = "sha256:277a0ef2981ca40581a47093e9e2d13b3f1fbbeffae064c1d21bfceba2030287"},
    {file = "PyYAML-6.0-cp38-cp38-win32.whl", hash = "sha256:d4eccecf9adf6fbcc6861a38015c2a64f38b9d94838ac1810a9023a0609e1b78"},
    {file = "PyYAML-6.0-cp38-cp38-win_amd64.whl", hash = "sha256:1e4747bc279b4f613a09eb64bba2ba602d8a6664c6ce6396a4d0cd413a50ce07"},
    {file = "PyYAML-6.0-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:055d937d65826939cb044fc8c9b08889e8c743fdc6a32b33e2390f66013e449b"},
    {file = "PyYAML-6.0-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:e61ceaab6f49fb8bdfaa0f92c4b57bcfbea54c09277b1b4f7ac376bfb7a7c174"},
    {file = "PyYAML-6.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d67d839ede4ed1b28a4e8909735fc992a923cdb84e618544973d7dfc71540803"},
    {file = "PyYAML-6.0-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:cba8c411ef271aa037d7357a2bc8f9ee8b58b9965831d9e51baf703280dc73d3"},
    {file = "PyYAML-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl", hash = "sha256:40527857252b61eacd1d9af500c3337ba8deb8fc298940291486c465c8b46ec0"},
    {file = "PyYAML-6.0-cp39-cp39-win32.whl", hash = "sha256:b5b9eccad747aabaaffbc6064800670f0c297e52c12754eb1d976c57e4f74dcb"},
    {file = "PyYAML-6.0-cp39-cp39-win_amd64.whl", hash = "sha256:b3d267842bf12586ba6c734f89d1f5b871df0273157918b0ccefa29deb05c21c"},
    {file = "PyYAML-6.0.tar.gz", hash = "sha256:68fb519c14306fec9720a2a5b45bc9f0c8d1b9c72adf45c37baedfcd949c35a2"},
]

[[package]]
name = "qdrant-client"
version = "1.1.7"
description = "Client library for the Qdrant vector search engine"
category = "main"
optional = false
python-versions = ">=3.7,<3.12"
files = [
    {file = "qdrant_client-1.1.7-py3-none-any.whl", hash = "sha256:4f5d883660b8193840d8982919ab813a0470ace9a7ff46ee730f909841be5319"},
    {file = "qdrant_client-1.1.7.tar.gz", hash = "sha256:686d86934bec2ebb70676fc0650c9a44a9e552e0149124ca5a22ee8533879deb"},
]

[package.dependencies]
grpcio = ">=1.41.0"
grpcio-tools = ">=1.41.0"
httpx = {version = ">=0.14.0", extras = ["http2"]}
numpy = {version = ">=1.21", markers = "python_version >= \"3.8\""}
portalocker = ">=2.7.0,<3.0.0"
pydantic = ">=1.8,<2.0"
typing-extensions = ">=4.0.0,<5.0.0"
urllib3 = ">=1.26.14,<2.0.0"

[[package]]
name = "readme-renderer"
version = "37.3"
description = "readme_renderer is a library for rendering \"readme\" descriptions for Warehouse"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "readme_renderer-37.3-py3-none-any.whl", hash = "sha256:f67a16caedfa71eef48a31b39708637a6f4664c4394801a7b0d6432d13907343"},
    {file = "readme_renderer-37.3.tar.gz", hash = "sha256:cd653186dfc73055656f090f227f5cb22a046d7f71a841dfa305f55c9a513273"},
]

[package.dependencies]
bleach = ">=2.1.0"
docutils = ">=0.13.1"
Pygments = ">=2.5.1"

[package.extras]
md = ["cmarkgfm (>=0.8.0)"]

[[package]]
name = "realtime"
version = "1.0.0"
description = ""
category = "main"
optional = false
python-versions = ">=3.8,<4.0"
files = [
    {file = "realtime-1.0.0-py3-none-any.whl", hash = "sha256:ceab9e292211ab08b5792ac52b3fa25398440031d5b369bd5799b8125056e2d8"},
    {file = "realtime-1.0.0.tar.gz", hash = "sha256:14e540c4a0cc2736ae83e0cbd7efbbfb8b736df1681df2b9141556cb4848502d"},
]

[package.dependencies]
python-dateutil = ">=2.8.1,<3.0.0"
typing-extensions = ">=4.2.0,<5.0.0"
websockets = ">=10.3,<11.0"

[[package]]
name = "redis"
version = "4.5.4"
description = "Python client for Redis database and key-value store"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "redis-4.5.4-py3-none-any.whl", hash = "sha256:2c19e6767c474f2e85167909061d525ed65bea9301c0770bb151e041b7ac89a2"},
    {file = "redis-4.5.4.tar.gz", hash = "sha256:73ec35da4da267d6847e47f68730fdd5f62e2ca69e3ef5885c6a78a9374c3893"},
]

[package.dependencies]
async-timeout = {version = ">=4.0.2", markers = "python_version <= \"3.11.2\""}

[package.extras]
hiredis = ["hiredis (>=1.0.0)"]
ocsp = ["cryptography (>=36.0.1)", "pyopenssl (==20.0.1)", "requests (>=2.26.0)"]

[[package]]
name = "regex"
version = "2023.5.5"
description = "Alternative regular expression module, to replace re."
category = "main"
optional = false
python-versions = ">=3.6"
files = [
    {file = "regex-2023.5.5-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:48c9ec56579d4ba1c88f42302194b8ae2350265cb60c64b7b9a88dcb7fbde309"},
    {file = "regex-2023.5.5-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:02f4541550459c08fdd6f97aa4e24c6f1932eec780d58a2faa2068253df7d6ff"},
    {file = "regex-2023.5.5-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:53e22e4460f0245b468ee645156a4f84d0fc35a12d9ba79bd7d79bdcd2f9629d"},
    {file = "regex-2023.5.5-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:4b870b6f632fc74941cadc2a0f3064ed8409e6f8ee226cdfd2a85ae50473aa94"},
    {file = "regex-2023.5.5-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:171c52e320fe29260da550d81c6b99f6f8402450dc7777ef5ced2e848f3b6f8f"},
    {file = "regex-2023.5.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:aad5524c2aedaf9aa14ef1bc9327f8abd915699dea457d339bebbe2f0d218f86"},
    {file = "regex-2023.5.5-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:5a0f874ee8c0bc820e649c900243c6d1e6dc435b81da1492046716f14f1a2a96"},
    {file = "regex-2023.5.5-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl", hash = "sha256:e645c757183ee0e13f0bbe56508598e2d9cd42b8abc6c0599d53b0d0b8dd1479"},
    {file = "regex-2023.5.5-cp310-cp310-musllinux_1_1_aarch64.whl", hash = "sha256:a4c5da39bca4f7979eefcbb36efea04471cd68db2d38fcbb4ee2c6d440699833"},
    {file = "regex-2023.5.5-cp310-cp310-musllinux_1_1_i686.whl", hash = "sha256:5e3f4468b8c6fd2fd33c218bbd0a1559e6a6fcf185af8bb0cc43f3b5bfb7d636"},
    {file = "regex-2023.5.5-cp310-cp310-musllinux_1_1_ppc64le.whl", hash = "sha256:59e4b729eae1a0919f9e4c0fc635fbcc9db59c74ad98d684f4877be3d2607dd6"},
    {file = "regex-2023.5.5-cp310-cp310-musllinux_1_1_s390x.whl", hash = "sha256:ba73a14e9c8f9ac409863543cde3290dba39098fc261f717dc337ea72d3ebad2"},
    {file = "regex-2023.5.5-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:0bbd5dcb19603ab8d2781fac60114fb89aee8494f4505ae7ad141a3314abb1f9"},
    {file = "regex-2023.5.5-cp310-cp310-win32.whl", hash = "sha256:40005cbd383438aecf715a7b47fe1e3dcbc889a36461ed416bdec07e0ef1db66"},
    {file = "regex-2023.5.5-cp310-cp310-win_amd64.whl", hash = "sha256:59597cd6315d3439ed4b074febe84a439c33928dd34396941b4d377692eca810"},
    {file = "regex-2023.5.5-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:8f08276466fedb9e36e5193a96cb944928301152879ec20c2d723d1031cd4ddd"},
    {file = "regex-2023.5.5-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:cd46f30e758629c3ee91713529cfbe107ac50d27110fdcc326a42ce2acf4dafc"},
    {file = "regex-2023.5.5-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:f2910502f718828cecc8beff004917dcf577fc5f8f5dd40ffb1ea7612124547b"},
    {file = "regex-2023.5.5-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:445d6f4fc3bd9fc2bf0416164454f90acab8858cd5a041403d7a11e3356980e8"},
    {file = "regex-2023.5.5-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:18196c16a584619c7c1d843497c069955d7629ad4a3fdee240eb347f4a2c9dbe"},
    {file = "regex-2023.5.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:33d430a23b661629661f1fe8395be2004006bc792bb9fc7c53911d661b69dd7e"},
    {file = "regex-2023.5.5-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:72a28979cc667e5f82ef433db009184e7ac277844eea0f7f4d254b789517941d"},
    {file = "regex-2023.5.5-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:f764e4dfafa288e2eba21231f455d209f4709436baeebb05bdecfb5d8ddc3d35"},
    {file = "regex-2023.5.5-cp311-cp311-musllinux_1_1_i686.whl", hash = "sha256:23d86ad2121b3c4fc78c58f95e19173790e22ac05996df69b84e12da5816cb17"},
    {file = "regex-2023.5.5-cp311-cp311-musllinux_1_1_ppc64le.whl", hash = "sha256:690a17db524ee6ac4a27efc5406530dd90e7a7a69d8360235323d0e5dafb8f5b"},
    {file = "regex-2023.5.5-cp311-cp311-musllinux_1_1_s390x.whl", hash = "sha256:1ecf3dcff71f0c0fe3e555201cbe749fa66aae8d18f80d2cc4de8e66df37390a"},
    {file = "regex-2023.5.5-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:811040d7f3dd9c55eb0d8b00b5dcb7fd9ae1761c454f444fd9f37fe5ec57143a"},
    {file = "regex-2023.5.5-cp311-cp311-win32.whl", hash = "sha256:c8c143a65ce3ca42e54d8e6fcaf465b6b672ed1c6c90022794a802fb93105d22"},
    {file = "regex-2023.5.5-cp311-cp311-win_amd64.whl", hash = "sha256:586a011f77f8a2da4b888774174cd266e69e917a67ba072c7fc0e91878178a80"},
    {file = "regex-2023.5.5-cp36-cp36m-macosx_10_9_x86_64.whl", hash = "sha256:b6365703e8cf1644b82104cdd05270d1a9f043119a168d66c55684b1b557d008"},
    {file = "regex-2023.5.5-cp36-cp36m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:a56c18f21ac98209da9c54ae3ebb3b6f6e772038681d6cb43b8d53da3b09ee81"},
    {file = "regex-2023.5.5-cp36-cp36m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:b8b942d8b3ce765dbc3b1dad0a944712a89b5de290ce8f72681e22b3c55f3cc8"},
    {file = "regex-2023.5.5-cp36-cp36m-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:844671c9c1150fcdac46d43198364034b961bd520f2c4fdaabfc7c7d7138a2dd"},
    {file = "regex-2023.5.5-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:c2ce65bdeaf0a386bb3b533a28de3994e8e13b464ac15e1e67e4603dd88787fa"},
    {file = "regex-2023.5.5-cp36-cp36m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:fee0016cc35a8a91e8cc9312ab26a6fe638d484131a7afa79e1ce6165328a135"},
    {file = "regex-2023.5.5-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl", hash = "sha256:18f05d14f14a812fe9723f13afafefe6b74ca042d99f8884e62dbd34dcccf3e2"},
    {file = "regex-2023.5.5-cp36-cp36m-musllinux_1_1_aarch64.whl", hash = "sha256:941b3f1b2392f0bcd6abf1bc7a322787d6db4e7457be6d1ffd3a693426a755f2"},
    {file = "regex-2023.5.5-cp36-cp36m-musllinux_1_1_i686.whl", hash = "sha256:921473a93bcea4d00295799ab929522fc650e85c6b9f27ae1e6bb32a790ea7d3"},
    {file = "regex-2023.5.5-cp36-cp36m-musllinux_1_1_ppc64le.whl", hash = "sha256:e2205a81f815b5bb17e46e74cc946c575b484e5f0acfcb805fb252d67e22938d"},
    {file = "regex-2023.5.5-cp36-cp36m-musllinux_1_1_s390x.whl", hash = "sha256:385992d5ecf1a93cb85adff2f73e0402dd9ac29b71b7006d342cc920816e6f32"},
    {file = "regex-2023.5.5-cp36-cp36m-musllinux_1_1_x86_64.whl", hash = "sha256:890a09cb0a62198bff92eda98b2b507305dd3abf974778bae3287f98b48907d3"},
    {file = "regex-2023.5.5-cp36-cp36m-win32.whl", hash = "sha256:821a88b878b6589c5068f4cc2cfeb2c64e343a196bc9d7ac68ea8c2a776acd46"},
    {file = "regex-2023.5.5-cp36-cp36m-win_amd64.whl", hash = "sha256:7918a1b83dd70dc04ab5ed24c78ae833ae8ea228cef84e08597c408286edc926"},
    {file = "regex-2023.5.5-cp37-cp37m-macosx_10_9_x86_64.whl", hash = "sha256:338994d3d4ca4cf12f09822e025731a5bdd3a37aaa571fa52659e85ca793fb67"},
    {file = "regex-2023.5.5-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0a69cf0c00c4d4a929c6c7717fd918414cab0d6132a49a6d8fc3ded1988ed2ea"},
    {file = "regex-2023.5.5-cp37-cp37m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:8f5e06df94fff8c4c85f98c6487f6636848e1dc85ce17ab7d1931df4a081f657"},
    {file = "regex-2023.5.5-cp37-cp37m-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:a8906669b03c63266b6a7693d1f487b02647beb12adea20f8840c1a087e2dfb5"},
    {file = "regex-2023.5.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9fda3e50abad8d0f48df621cf75adc73c63f7243cbe0e3b2171392b445401550"},
    {file = "regex-2023.5.5-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:5ac2b7d341dc1bd102be849d6dd33b09701223a851105b2754339e390be0627a"},
    {file = "regex-2023.5.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl", hash = "sha256:fb2b495dd94b02de8215625948132cc2ea360ae84fe6634cd19b6567709c8ae2"},
    {file = "regex-2023.5.5-cp37-cp37m-musllinux_1_1_aarch64.whl", hash = "sha256:aa7d032c1d84726aa9edeb6accf079b4caa87151ca9fabacef31fa028186c66d"},
    {file = "regex-2023.5.5-cp37-cp37m-musllinux_1_1_i686.whl", hash = "sha256:3d45864693351c15531f7e76f545ec35000d50848daa833cead96edae1665559"},
    {file = "regex-2023.5.5-cp37-cp37m-musllinux_1_1_ppc64le.whl", hash = "sha256:21e90a288e6ba4bf44c25c6a946cb9b0f00b73044d74308b5e0afd190338297c"},
    {file = "regex-2023.5.5-cp37-cp37m-musllinux_1_1_s390x.whl", hash = "sha256:10250a093741ec7bf74bcd2039e697f519b028518f605ff2aa7ac1e9c9f97423"},
    {file = "regex-2023.5.5-cp37-cp37m-musllinux_1_1_x86_64.whl", hash = "sha256:6b8d0c153f07a953636b9cdb3011b733cadd4178123ef728ccc4d5969e67f3c2"},
    {file = "regex-2023.5.5-cp37-cp37m-win32.whl", hash = "sha256:10374c84ee58c44575b667310d5bbfa89fb2e64e52349720a0182c0017512f6c"},
    {file = "regex-2023.5.5-cp37-cp37m-win_amd64.whl", hash = "sha256:9b320677521aabf666cdd6e99baee4fb5ac3996349c3b7f8e7c4eee1c00dfe3a"},
    {file = "regex-2023.5.5-cp38-cp38-macosx_10_9_x86_64.whl", hash = "sha256:afb1c70ec1e594a547f38ad6bf5e3d60304ce7539e677c1429eebab115bce56e"},
    {file = "regex-2023.5.5-cp38-cp38-macosx_11_0_arm64.whl", hash = "sha256:cf123225945aa58b3057d0fba67e8061c62d14cc8a4202630f8057df70189051"},
    {file = "regex-2023.5.5-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:a99757ad7fe5c8a2bb44829fc57ced11253e10f462233c1255fe03888e06bc19"},
    {file = "regex-2023.5.5-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:a623564d810e7a953ff1357f7799c14bc9beeab699aacc8b7ab7822da1e952b8"},
    {file = "regex-2023.5.5-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:ced02e3bd55e16e89c08bbc8128cff0884d96e7f7a5633d3dc366b6d95fcd1d6"},
    {file = "regex-2023.5.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:d1cbe6b5be3b9b698d8cc4ee4dee7e017ad655e83361cd0ea8e653d65e469468"},
    {file = "regex-2023.5.5-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:4a6e4b0e0531223f53bad07ddf733af490ba2b8367f62342b92b39b29f72735a"},
    {file = "regex-2023.5.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl", hash = "sha256:2e9c4f778514a560a9c9aa8e5538bee759b55f6c1dcd35613ad72523fd9175b8"},
    {file = "regex-2023.5.5-cp38-cp38-musllinux_1_1_aarch64.whl", hash = "sha256:256f7f4c6ba145f62f7a441a003c94b8b1af78cee2cccacfc1e835f93bc09426"},
    {file = "regex-2023.5.5-cp38-cp38-musllinux_1_1_i686.whl", hash = "sha256:bd7b68fd2e79d59d86dcbc1ccd6e2ca09c505343445daaa4e07f43c8a9cc34da"},
    {file = "regex-2023.5.5-cp38-cp38-musllinux_1_1_ppc64le.whl", hash = "sha256:4a5059bd585e9e9504ef9c07e4bc15b0a621ba20504388875d66b8b30a5c4d18"},
    {file = "regex-2023.5.5-cp38-cp38-musllinux_1_1_s390x.whl", hash = "sha256:6893544e06bae009916a5658ce7207e26ed17385149f35a3125f5259951f1bbe"},
    {file = "regex-2023.5.5-cp38-cp38-musllinux_1_1_x86_64.whl", hash = "sha256:c64d5abe91a3dfe5ff250c6bb267ef00dbc01501518225b45a5f9def458f31fb"},
    {file = "regex-2023.5.5-cp38-cp38-win32.whl", hash = "sha256:7923470d6056a9590247ff729c05e8e0f06bbd4efa6569c916943cb2d9b68b91"},
    {file = "regex-2023.5.5-cp38-cp38-win_amd64.whl", hash = "sha256:4035d6945cb961c90c3e1c1ca2feb526175bcfed44dfb1cc77db4fdced060d3e"},
    {file = "regex-2023.5.5-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:50fd2d9b36938d4dcecbd684777dd12a407add4f9f934f235c66372e630772b0"},
    {file = "regex-2023.5.5-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:d19e57f888b00cd04fc38f5e18d0efbd91ccba2d45039453ab2236e6eec48d4d"},
    {file = "regex-2023.5.5-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:bd966475e963122ee0a7118ec9024388c602d12ac72860f6eea119a3928be053"},
    {file = "regex-2023.5.5-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:db09e6c18977a33fea26fe67b7a842f706c67cf8bda1450974d0ae0dd63570df"},
    {file = "regex-2023.5.5-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:6164d4e2a82f9ebd7752a06bd6c504791bedc6418c0196cd0a23afb7f3e12b2d"},
    {file = "regex-2023.5.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:84397d3f750d153ebd7f958efaa92b45fea170200e2df5e0e1fd4d85b7e3f58a"},
    {file = "regex-2023.5.5-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:9c3efee9bb53cbe7b285760c81f28ac80dc15fa48b5fe7e58b52752e642553f1"},
    {file = "regex-2023.5.5-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl", hash = "sha256:144b5b017646b5a9392a5554a1e5db0000ae637be4971c9747566775fc96e1b2"},
    {file = "regex-2023.5.5-cp39-cp39-musllinux_1_1_aarch64.whl", hash = "sha256:1189fbbb21e2c117fda5303653b61905aeeeea23de4a94d400b0487eb16d2d60"},
    {file = "regex-2023.5.5-cp39-cp39-musllinux_1_1_i686.whl", hash = "sha256:f83fe9e10f9d0b6cf580564d4d23845b9d692e4c91bd8be57733958e4c602956"},
    {file = "regex-2023.5.5-cp39-cp39-musllinux_1_1_ppc64le.whl", hash = "sha256:72aa4746993a28c841e05889f3f1b1e5d14df8d3daa157d6001a34c98102b393"},
    {file = "regex-2023.5.5-cp39-cp39-musllinux_1_1_s390x.whl", hash = "sha256:de2f780c3242ea114dd01f84848655356af4dd561501896c751d7b885ea6d3a1"},
    {file = "regex-2023.5.5-cp39-cp39-musllinux_1_1_x86_64.whl", hash = "sha256:290fd35219486dfbc00b0de72f455ecdd63e59b528991a6aec9fdfc0ce85672e"},
    {file = "regex-2023.5.5-cp39-cp39-win32.whl", hash = "sha256:732176f5427e72fa2325b05c58ad0b45af341c459910d766f814b0584ac1f9ac"},
    {file = "regex-2023.5.5-cp39-cp39-win_amd64.whl", hash = "sha256:1307aa4daa1cbb23823d8238e1f61292fd07e4e5d8d38a6efff00b67a7cdb764"},
    {file = "regex-2023.5.5.tar.gz", hash = "sha256:7d76a8a1fc9da08296462a18f16620ba73bcbf5909e42383b253ef34d9d5141e"},
]

[[package]]
name = "requests"
version = "2.28.2"
description = "Python HTTP for Humans."
category = "main"
optional = false
python-versions = ">=3.7, <4"
files = [
    {file = "requests-2.28.2-py3-none-any.whl", hash = "sha256:64299f4909223da747622c030b781c0d7811e359c37124b4bd368fb8c6518baa"},
    {file = "requests-2.28.2.tar.gz", hash = "sha256:98b1b2782e3c6c4904938b84c0eb932721069dfdb9134313beff7c83c2df24bf"},
]

[package.dependencies]
certifi = ">=2017.4.17"
charset-normalizer = ">=2,<4"
idna = ">=2.5,<4"
urllib3 = ">=1.21.1,<1.27"

[package.extras]
socks = ["PySocks (>=1.5.6,!=1.5.7)"]
use-chardet-on-py3 = ["chardet (>=3.0.2,<6)"]

[[package]]
name = "requests-toolbelt"
version = "1.0.0"
description = "A utility belt for advanced users of python-requests"
category = "main"
optional = false
python-versions = ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*"
files = [
    {file = "requests-toolbelt-1.0.0.tar.gz", hash = "sha256:7681a0a3d047012b5bdc0ee37d7f8f07ebe76ab08caeccfc3921ce23c88d5bc6"},
    {file = "requests_toolbelt-1.0.0-py2.py3-none-any.whl", hash = "sha256:cccfdd665f0a24fcf4726e690f65639d272bb0637b9b92dfd91a5568ccf6bd06"},
]

[package.dependencies]
requests = ">=2.0.1,<3.0.0"

[[package]]
name = "rfc3986"
version = "1.5.0"
description = "Validating URI References per RFC 3986"
category = "main"
optional = false
python-versions = "*"
files = [
    {file = "rfc3986-1.5.0-py2.py3-none-any.whl", hash = "sha256:a86d6e1f5b1dc238b218b012df0aa79409667bb209e58da56d0b94704e712a97"},
    {file = "rfc3986-1.5.0.tar.gz", hash = "sha256:270aaf10d87d0d4e095063c65bf3ddbc6ee3d0b226328ce21e036f946e421835"},
]

[package.dependencies]
idna = {version = "*", optional = true, markers = "extra == \"idna2008\""}

[package.extras]
idna2008 = ["idna"]

[[package]]
name = "secretstorage"
version = "3.3.3"
description = "Python bindings to FreeDesktop.org Secret Service API"
category = "main"
optional = false
python-versions = ">=3.6"
files = [
    {file = "SecretStorage-3.3.3-py3-none-any.whl", hash = "sha256:f356e6628222568e3af06f2eba8df495efa13b3b63081dafd4f7d9a7b7bc9f99"},
    {file = "SecretStorage-3.3.3.tar.gz", hash = "sha256:2403533ef369eca6d2ba81718576c5e0f564d5cca1b58f73a8b23e7d4eeebd77"},
]

[package.dependencies]
cryptography = ">=2.0"
jeepney = ">=0.6"

[[package]]
name = "semver"
version = "2.13.0"
description = "Python helper for Semantic Versioning (http://semver.org/)"
category = "main"
optional = false
python-versions = ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*"
files = [
    {file = "semver-2.13.0-py2.py3-none-any.whl", hash = "sha256:ced8b23dceb22134307c1b8abfa523da14198793d9787ac838e70e29e77458d4"},
    {file = "semver-2.13.0.tar.gz", hash = "sha256:fa0fe2722ee1c3f57eac478820c3a5ae2f624af8264cbdf9000c980ff7f75e3f"},
]

[[package]]
name = "setuptools"
version = "67.8.0"
description = "Easily download, build, install, upgrade, and uninstall Python packages"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "setuptools-67.8.0-py3-none-any.whl", hash = "sha256:5df61bf30bb10c6f756eb19e7c9f3b473051f48db77fddbe06ff2ca307df9a6f"},
    {file = "setuptools-67.8.0.tar.gz", hash = "sha256:62642358adc77ffa87233bc4d2354c4b2682d214048f500964dbe760ccedf102"},
]

[package.extras]
docs = ["furo", "jaraco.packaging (>=9)", "jaraco.tidelift (>=1.4)", "pygments-github-lexers (==0.0.5)", "rst.linker (>=1.9)", "sphinx (>=3.5)", "sphinx-favicon", "sphinx-hoverxref (<2)", "sphinx-inline-tabs", "sphinx-lint", "sphinx-notfound-page (==0.8.3)", "sphinx-reredirects", "sphinxcontrib-towncrier"]
testing = ["build[virtualenv]", "filelock (>=3.4.0)", "flake8-2020", "ini2toml[lite] (>=0.9)", "jaraco.envs (>=2.2)", "jaraco.path (>=3.2.0)", "pip (>=19.1)", "pip-run (>=8.8)", "pytest (>=6)", "pytest-black (>=0.3.7)", "pytest-checkdocs (>=2.4)", "pytest-cov", "pytest-enabler (>=1.3)", "pytest-mypy (>=0.9.1)", "pytest-perf", "pytest-ruff", "pytest-timeout", "pytest-xdist", "tomli-w (>=1.0.0)", "virtualenv (>=13.0.0)", "wheel"]
testing-integration = ["build[virtualenv]", "filelock (>=3.4.0)", "jaraco.envs (>=2.2)", "jaraco.path (>=3.2.0)", "pytest", "pytest-enabler", "pytest-xdist", "tomli", "virtualenv (>=13.0.0)", "wheel"]

[[package]]
name = "six"
version = "1.16.0"
description = "Python 2 and 3 compatibility utilities"
category = "main"
optional = false
python-versions = ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*"
files = [
    {file = "six-1.16.0-py2.py3-none-any.whl", hash = "sha256:8abb2f1d86890a2dfb989f9a77cfcfd3e47c2a354b01111771326f8aa26e0254"},
    {file = "six-1.16.0.tar.gz", hash = "sha256:1e61c37477a1626458e36f7b1d82aa5c9b094fa4802892072e49de9c60c4c926"},
]

[[package]]
name = "smmap"
version = "5.0.0"
description = "A pure Python implementation of a sliding window memory map manager"
category = "main"
optional = false
python-versions = ">=3.6"
files = [
    {file = "smmap-5.0.0-py3-none-any.whl", hash = "sha256:2aba19d6a040e78d8b09de5c57e96207b09ed71d8e55ce0959eeee6c8e190d94"},
    {file = "smmap-5.0.0.tar.gz", hash = "sha256:c840e62059cd3be204b0c9c9f74be2c09d5648eddd4580d9314c3ecde0b30936"},
]

[[package]]
name = "sniffio"
version = "1.3.0"
description = "Sniff out which async library your code is running under"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "sniffio-1.3.0-py3-none-any.whl", hash = "sha256:eecefdce1e5bbfb7ad2eeaabf7c1eeb404d7757c379bd1f7e5cce9d8bf425384"},
    {file = "sniffio-1.3.0.tar.gz", hash = "sha256:e60305c5e5d314f5389259b7f22aaa33d8f7dee49763119234af3755c55b9101"},
]

[[package]]
name = "sqlalchemy"
version = "2.0.15"
description = "Database Abstraction Library"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "SQLAlchemy-2.0.15-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:78303719c6f72af97814b0072ad18bee72e70adca8d95cf8fecd59c5e1ddb040"},
    {file = "SQLAlchemy-2.0.15-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:9d810b4aacd5ef4e293aa4ea01f19fca53999e9edcfc4a8ef1146238b30bdc28"},
    {file = "SQLAlchemy-2.0.15-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:3fb5d09f1d51480f711b69fe28ad42e4f8b08600a85ab2473baee669e1257800"},
    {file = "SQLAlchemy-2.0.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:51b19887c96d405599880da6a7cbdf8545a7e78ec5683e46a43bac8885e32d0f"},
    {file = "SQLAlchemy-2.0.15-cp310-cp310-musllinux_1_1_aarch64.whl", hash = "sha256:d6b17cb86908e7f88be14007d6afe7d2ab11966e373044137f96a6a4d83eb21c"},
    {file = "SQLAlchemy-2.0.15-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:df25052b92bd514357a9b370d74f240db890ea79aaa428fb893520e10ee5bc18"},
    {file = "SQLAlchemy-2.0.15-cp310-cp310-win32.whl", hash = "sha256:55ec62ddc0200b4fee94d11abbec7aa25948d5d21cb8df8807f4bdd3c51ba44b"},
    {file = "SQLAlchemy-2.0.15-cp310-cp310-win_amd64.whl", hash = "sha256:ae1d8deb391ab39cc8f0d5844e588a115ae3717e607d91482023917f920f777f"},
    {file = "SQLAlchemy-2.0.15-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:4670ce853cb25f72115a1bbe366ae13cf3f28fc5c87222df14f8d3d55d51816e"},
    {file = "SQLAlchemy-2.0.15-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:cea7c4a3dfc2ca61f88a2b1ddd6b0bfbd116c9b1a361b3b66fd826034b833142"},
    {file = "SQLAlchemy-2.0.15-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:6f5784dfb2d45c19cde03c45c04a54bf47428610106197ed6e6fa79f33bc63d3"},
    {file = "SQLAlchemy-2.0.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9b31ebde27575b3b0708673ec14f0c305c4564d995b545148ab7ac0f4d9b847a"},
    {file = "SQLAlchemy-2.0.15-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:6b42913a0259267e9ee335da0c36498077799e59c5e332d506e72b4f32de781d"},
    {file = "SQLAlchemy-2.0.15-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:6a3f8020e013e9b3b7941dcf20b0fc8f7429daaf7158760846731cbd8caa5e45"},
    {file = "SQLAlchemy-2.0.15-cp311-cp311-win32.whl", hash = "sha256:88ab245ed2c96265441ed2818977be28c840cfa5204ba167425d6c26eb67b7e7"},
    {file = "SQLAlchemy-2.0.15-cp311-cp311-win_amd64.whl", hash = "sha256:5cc48a7fda2b5c5b8860494d6c575db3a101a68416492105fed6591dc8a2728a"},
    {file = "SQLAlchemy-2.0.15-cp37-cp37m-macosx_10_9_x86_64.whl", hash = "sha256:f6fd3c88ea4b170d13527e93be1945e69facd917661d3725a63470eb683fbffe"},
    {file = "SQLAlchemy-2.0.15-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:1e885dacb167077df15af2f9ccdacbd7f5dd0d538a6d74b94074f2cefc7bb589"},
    {file = "SQLAlchemy-2.0.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:201a99f922ac8c780b3929128fbd9df901418877c70e160e19adb05665e51c31"},
    {file = "SQLAlchemy-2.0.15-cp37-cp37m-musllinux_1_1_aarch64.whl", hash = "sha256:e17fdcb8971e77c439113642ca8861f9465e21fc693bd3916654ceef3ac26883"},
    {file = "SQLAlchemy-2.0.15-cp37-cp37m-musllinux_1_1_x86_64.whl", hash = "sha256:db269f67ed17b07e80aaa8fba1f650c0d84aa0bdd9d5352e4ac38d5bf47ac568"},
    {file = "SQLAlchemy-2.0.15-cp37-cp37m-win32.whl", hash = "sha256:994a75b197662e0608b6a76935d7c345f7fd874eac0b7093d561033db61b0e8c"},
    {file = "SQLAlchemy-2.0.15-cp37-cp37m-win_amd64.whl", hash = "sha256:4d61731a35eddb0f667774fe15e5a4831e444d066081d1e809e1b8a0e3f97cae"},
    {file = "SQLAlchemy-2.0.15-cp38-cp38-macosx_10_9_x86_64.whl", hash = "sha256:f7f994a53c0e6b44a2966fd6bfc53e37d34b7dca34e75b6be295de6db598255e"},
    {file = "SQLAlchemy-2.0.15-cp38-cp38-macosx_11_0_arm64.whl", hash = "sha256:79bfe728219239bdc493950ea4a4d15b02138ecb304771f9024d0d6f5f4e3706"},
    {file = "SQLAlchemy-2.0.15-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:1d6320a1d175447dce63618ec997a53836de48ed3b44bbe952f0b4b399b19941"},
    {file = "SQLAlchemy-2.0.15-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6f80a9c9a9af0e4bd5080cc0955ce70274c28e9b931ad7e0fb07021afcd32af6"},
    {file = "SQLAlchemy-2.0.15-cp38-cp38-musllinux_1_1_aarch64.whl", hash = "sha256:4a75fdb9a84072521bb2ebd31eefe1165d4dccea3039dda701a864f4b5daa17f"},
    {file = "SQLAlchemy-2.0.15-cp38-cp38-musllinux_1_1_x86_64.whl", hash = "sha256:21c89044fc48a25c2184eba332edeffbbf9367913bb065cd31538235d828f06f"},
    {file = "SQLAlchemy-2.0.15-cp38-cp38-win32.whl", hash = "sha256:1a0754c2d9f0c7982bec0a31138e495ed1f6b8435d7e677c45be60ec18370acf"},
    {file = "SQLAlchemy-2.0.15-cp38-cp38-win_amd64.whl", hash = "sha256:bc5c2b0da46c26c5f73f700834f871d0723e1e882641932468d56833bab09775"},
    {file = "SQLAlchemy-2.0.15-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:670ecf74ee2e70b917028a06446ad26ff9b1195e84b09c3139c215123d57dc30"},
    {file = "SQLAlchemy-2.0.15-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:d14282bf5b4de87f922db3c70858953fd081ef4f05dba6cca3dd705daffe1cc9"},
    {file = "SQLAlchemy-2.0.15-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:256b2b9660e51ad7055a9835b12717416cf7288afcf465107413917b6bb2316f"},
    {file = "SQLAlchemy-2.0.15-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:810199d1c5b43603a9e815ae9487aef3ab1ade7ed9c0c485e12519358929fbfe"},
    {file = "SQLAlchemy-2.0.15-cp39-cp39-musllinux_1_1_aarch64.whl", hash = "sha256:536c86ec81ca89291d533ff41a3a05f9e4e88e01906dcee0751fc7082f3e8d6c"},
    {file = "SQLAlchemy-2.0.15-cp39-cp39-musllinux_1_1_x86_64.whl", hash = "sha256:435f6807fa6a0597d84741470f19db204a7d34625ea121abd63e8d95f673f0c4"},
    {file = "SQLAlchemy-2.0.15-cp39-cp39-win32.whl", hash = "sha256:da7381a883aee20b7d2ffda17d909b38134b6a625920e65239a1c681881df800"},
    {file = "SQLAlchemy-2.0.15-cp39-cp39-win_amd64.whl", hash = "sha256:788d1772fb8dcd12091ca82809eef504ce0f2c423e45284bc351b872966ff554"},
    {file = "SQLAlchemy-2.0.15-py3-none-any.whl", hash = "sha256:933d30273861fe61f014ce2a7e3c364915f5efe9ed250ec1066ca6ea5942c0bd"},
    {file = "SQLAlchemy-2.0.15.tar.gz", hash = "sha256:2e940a8659ef870ae10e0d9e2a6d5aaddf0ff6e91f7d0d7732afc9e8c4be9bbc"},
]

[package.dependencies]
greenlet = {version = "!=0.4.17", markers = "platform_machine == \"aarch64\" or platform_machine == \"ppc64le\" or platform_machine == \"x86_64\" or platform_machine == \"amd64\" or platform_machine == \"AMD64\" or platform_machine == \"win32\" or platform_machine == \"WIN32\""}
typing-extensions = ">=4.2.0"

[package.extras]
aiomysql = ["aiomysql", "greenlet (!=0.4.17)"]
aiosqlite = ["aiosqlite", "greenlet (!=0.4.17)", "typing-extensions (!=3.10.0.1)"]
asyncio = ["greenlet (!=0.4.17)"]
asyncmy = ["asyncmy (>=0.2.3,!=0.2.4,!=0.2.6)", "greenlet (!=0.4.17)"]
mariadb-connector = ["mariadb (>=1.0.1,!=1.1.2,!=1.1.5)"]
mssql = ["pyodbc"]
mssql-pymssql = ["pymssql"]
mssql-pyodbc = ["pyodbc"]
mypy = ["mypy (>=0.910)"]
mysql = ["mysqlclient (>=1.4.0)"]
mysql-connector = ["mysql-connector-python"]
oracle = ["cx-oracle (>=7)"]
oracle-oracledb = ["oracledb (>=1.0.1)"]
postgresql = ["psycopg2 (>=2.7)"]
postgresql-asyncpg = ["asyncpg", "greenlet (!=0.4.17)"]
postgresql-pg8000 = ["pg8000 (>=1.29.1)"]
postgresql-psycopg = ["psycopg (>=3.0.7)"]
postgresql-psycopg2binary = ["psycopg2-binary"]
postgresql-psycopg2cffi = ["psycopg2cffi"]
pymysql = ["pymysql"]
sqlcipher = ["sqlcipher3-binary"]

[[package]]
name = "starlette"
version = "0.25.0"
description = "The little ASGI library that shines."
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "starlette-0.25.0-py3-none-any.whl", hash = "sha256:774f1df1983fd594b9b6fb3ded39c2aa1979d10ac45caac0f4255cbe2acb8628"},
    {file = "starlette-0.25.0.tar.gz", hash = "sha256:854c71e73736c429c2bdb07801f2c76c9cba497e7c3cf4988fde5e95fe4cdb3c"},
]

[package.dependencies]
anyio = ">=3.4.0,<5"

[package.extras]
full = ["httpx (>=0.22.0)", "itsdangerous", "jinja2", "python-multipart", "pyyaml"]

[[package]]
name = "storage3"
version = "0.5.2"
description = "Supabase Storage client for Python."
category = "main"
optional = false
python-versions = ">=3.8,<4.0"
files = [
    {file = "storage3-0.5.2-py3-none-any.whl", hash = "sha256:3aaba8cebf89eef6b5fc48739b8c8c8539461f2eed9ea1dc4c763dea10c6d009"},
    {file = "storage3-0.5.2.tar.gz", hash = "sha256:e9932fca869a8f9cdab9a20e5249439928cfe2d07c4524141b15fef1882a7f61"},
]

[package.dependencies]
httpx = ">=0.23,<0.24"
python-dateutil = ">=2.8.2,<3.0.0"
typing-extensions = ">=4.2.0,<5.0.0"

[[package]]
name = "strenum"
version = "0.4.10"
description = "An Enum that inherits from str."
category = "main"
optional = false
python-versions = "*"
files = [
    {file = "StrEnum-0.4.10-py3-none-any.whl", hash = "sha256:aebf04bba8e5af435937c452d69a86798b6f8d5ca5f20ba18561dbfad571ccdd"},
    {file = "StrEnum-0.4.10.tar.gz", hash = "sha256:898cc0ebb5054ee07400341ac1d75fdfee489d76d6df3fbc1c2eaf95971e3916"},
]

[package.extras]
docs = ["myst-parser[linkify]", "sphinx", "sphinx-rtd-theme"]
release = ["twine"]
test = ["pylint", "pytest", "pytest-black", "pytest-cov", "pytest-pylint"]

[[package]]
name = "supabase"
version = "1.0.3"
description = "Supabase client for Python."
category = "main"
optional = false
python-versions = ">=3.8,<4.0"
files = [
    {file = "supabase-1.0.3-py3-none-any.whl", hash = "sha256:2418113b7f503522d33fafd442e587356636bad6cb803f7e406e614acf2611d7"},
    {file = "supabase-1.0.3.tar.gz", hash = "sha256:c6eac0144b4236a61ccc72024a8e88d8f08979e47ea635307afae7fb4fc24bc6"},
]

[package.dependencies]
gotrue = ">=1.0.1,<2.0.0"
httpx = ">=0.23.0,<0.24.0"
postgrest = ">=0.10.6,<0.11.0"
python-semantic-release = "7.33.2"
realtime = ">=1.0.0,<2.0.0"
storage3 = ">=0.5.2,<0.6.0"
supafunc = ">=0.2.2,<0.3.0"

[[package]]
name = "supafunc"
version = "0.2.2"
description = "Library for Supabase Functions"
category = "main"
optional = false
python-versions = ">=3.7,<4.0"
files = [
    {file = "supafunc-0.2.2-py3-none-any.whl", hash = "sha256:a292812532cca05afc08d2cc040eea5bd79a8909e46051630620b67508070795"},
    {file = "supafunc-0.2.2.tar.gz", hash = "sha256:84f1f8d47297b0c8b712f1d8e20843406c025a203bba00cb7216e2163f295c24"},
]

[package.dependencies]
httpx = ">=0.23.0,<0.24.0"

[[package]]
name = "sympy"
version = "1.12"
description = "Computer algebra system (CAS) in Python"
category = "main"
optional = false
python-versions = ">=3.8"
files = [
    {file = "sympy-1.12-py3-none-any.whl", hash = "sha256:c3588cd4295d0c0f603d0f2ae780587e64e2efeedb3521e46b9bb1d08d184fa5"},
    {file = "sympy-1.12.tar.gz", hash = "sha256:ebf595c8dac3e0fdc4152c51878b498396ec7f30e7a914d6071e674d49420fb8"},
]

[package.dependencies]
mpmath = ">=0.19"

[[package]]
name = "tenacity"
version = "8.2.2"
description = "Retry code until it succeeds"
category = "main"
optional = false
python-versions = ">=3.6"
files = [
    {file = "tenacity-8.2.2-py3-none-any.whl", hash = "sha256:2f277afb21b851637e8f52e6a613ff08734c347dc19ade928e519d7d2d8569b0"},
    {file = "tenacity-8.2.2.tar.gz", hash = "sha256:43af037822bd0029025877f3b2d97cc4d7bb0c2991000a3d59d71517c5c969e0"},
]

[package.extras]
doc = ["reno", "sphinx", "tornado (>=4.5)"]

[[package]]
name = "tiktoken"
version = "0.2.0"
description = ""
category = "main"
optional = false
python-versions = ">=3.8"
files = [
    {file = "tiktoken-0.2.0-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:d06705b55bb5f6c194285b6d15ad31bd7586d44fe433be31bc3694cf8c70169c"},
    {file = "tiktoken-0.2.0-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:29f2969945fc430f817c907f59a2da9e7b797fe65527ba5b9442618643a0dc86"},
    {file = "tiktoken-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:546455f27b6f7981d17de265b8b99e2fef980fbc3fde1d94b551f8354902000e"},
    {file = "tiktoken-0.2.0-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:54b5dc05f934ac68e8da4d2cc3acd77bc6968114b09669056f1bff12acc57049"},
    {file = "tiktoken-0.2.0-cp310-cp310-win_amd64.whl", hash = "sha256:5d3c48cb5649ce6bb2b207377dfdaa855e1e771b2e7f59fb251182c227573619"},
    {file = "tiktoken-0.2.0-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:a55f983735745df9a87161d9e0ce9ef7d216039d389246be98c6d416bbb2452f"},
    {file = "tiktoken-0.2.0-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:175de868393039a85fdf4c7cfb9b8883d1b248b9a3d9d0129d30414f5a59c333"},
    {file = "tiktoken-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:d6cd97b8cd14e3fe6647baa71c67f7f6b21a401fa996ccc3d93bf0ae02162af2"},
    {file = "tiktoken-0.2.0-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:806e2b8c0b9786c0e3212e8b3a6ac8f5840066c00a31b89e6c8d9ba0421e77d7"},
    {file = "tiktoken-0.2.0-cp311-cp311-win_amd64.whl", hash = "sha256:57b753aa9813f06fa5a26da2622114bf9769a8d1dca1b276d3613ee15da5b09d"},
    {file = "tiktoken-0.2.0-cp38-cp38-macosx_10_9_x86_64.whl", hash = "sha256:aa3c15b87bb2cea56ecc8fe4c7bf105c5c2dc4090c2df97c141100488297173a"},
    {file = "tiktoken-0.2.0-cp38-cp38-macosx_11_0_arm64.whl", hash = "sha256:bd98fc4a9ec967a089c62497f21277b53aa3e15a6fec731ac707eea4d5527938"},
    {file = "tiktoken-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:ab10ad3280f348a0d3bfea6d503c6aa84676b159692701bc7604e67129bd2135"},
    {file = "tiktoken-0.2.0-cp38-cp38-musllinux_1_1_x86_64.whl", hash = "sha256:59296d495aa6aec375a75f07da44fabb9720632c9404b41b9cbfe95e17966345"},
    {file = "tiktoken-0.2.0-cp38-cp38-win_amd64.whl", hash = "sha256:3b078e6109d522c5ffc52859520eef6c17a3b120ed52b79f48cae0badff08fe0"},
    {file = "tiktoken-0.2.0-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:aef47e8037652b18d2665b77e1f9416d3a86ccd383b039d0dfcb7d92085cef6d"},
    {file = "tiktoken-0.2.0-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:d0f62f8349a5412962326dbc41c3823a1f381d8ab62afbee94480d8296499d8e"},
    {file = "tiktoken-0.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9d0dbf7e1940427c11f0c8ab9046ad98d774850b21559b37ca60ff30d3a14620"},
    {file = "tiktoken-0.2.0-cp39-cp39-musllinux_1_1_x86_64.whl", hash = "sha256:8f1a7c6bec42a2fb5309a161d1b891fe5e181d4b620a962923a925f45fe25697"},
    {file = "tiktoken-0.2.0-cp39-cp39-win_amd64.whl", hash = "sha256:3349fd809d17b722814a6a700e4bc0125527f39057b57a02ed42f53bb4e6e2f5"},
    {file = "tiktoken-0.2.0.tar.gz", hash = "sha256:df41a3d478499757b5b32eae5e97657cf159d8d9e6764049dd7c3abb49e1b40f"},
]

[package.dependencies]
blobfile = ">=2"
regex = ">=2022.1.18"
requests = ">=2.26.0"

[[package]]
name = "tokenizers"
version = "0.13.3"
description = "Fast and Customizable Tokenizers"
category = "main"
optional = false
python-versions = "*"
files = [
    {file = "tokenizers-0.13.3-cp310-cp310-macosx_10_11_x86_64.whl", hash = "sha256:f3835c5be51de8c0a092058a4d4380cb9244fb34681fd0a295fbf0a52a5fdf33"},
    {file = "tokenizers-0.13.3-cp310-cp310-macosx_12_0_arm64.whl", hash = "sha256:4ef4c3e821730f2692489e926b184321e887f34fb8a6b80b8096b966ba663d07"},
    {file = "tokenizers-0.13.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:c5fd1a6a25353e9aa762e2aae5a1e63883cad9f4e997c447ec39d071020459bc"},
    {file = "tokenizers-0.13.3-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:ee0b1b311d65beab83d7a41c56a1e46ab732a9eed4460648e8eb0bd69fc2d059"},
    {file = "tokenizers-0.13.3-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:5ef4215284df1277dadbcc5e17d4882bda19f770d02348e73523f7e7d8b8d396"},
    {file = "tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:a4d53976079cff8a033f778fb9adca2d9d69d009c02fa2d71a878b5f3963ed30"},
    {file = "tokenizers-0.13.3-cp310-cp310-win32.whl", hash = "sha256:1f0e3b4c2ea2cd13238ce43548959c118069db7579e5d40ec270ad77da5833ce"},
    {file = "tokenizers-0.13.3-cp310-cp310-win_amd64.whl", hash = "sha256:89649c00d0d7211e8186f7a75dfa1db6996f65edce4b84821817eadcc2d3c79e"},
    {file = "tokenizers-0.13.3-cp311-cp311-macosx_10_11_universal2.whl", hash = "sha256:56b726e0d2bbc9243872b0144515ba684af5b8d8cd112fb83ee1365e26ec74c8"},
    {file = "tokenizers-0.13.3-cp311-cp311-macosx_12_0_arm64.whl", hash = "sha256:cc5c022ce692e1f499d745af293ab9ee6f5d92538ed2faf73f9708c89ee59ce6"},
    {file = "tokenizers-0.13.3-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:f55c981ac44ba87c93e847c333e58c12abcbb377a0c2f2ef96e1a266e4184ff2"},
    {file = "tokenizers-0.13.3-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:f247eae99800ef821a91f47c5280e9e9afaeed9980fc444208d5aa6ba69ff148"},
    {file = "tokenizers-0.13.3-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:4b3e3215d048e94f40f1c95802e45dcc37c5b05eb46280fc2ccc8cd351bff839"},
    {file = "tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9ba2b0bf01777c9b9bc94b53764d6684554ce98551fec496f71bc5be3a03e98b"},
    {file = "tokenizers-0.13.3-cp311-cp311-win32.whl", hash = "sha256:cc78d77f597d1c458bf0ea7c2a64b6aa06941c7a99cb135b5969b0278824d808"},
    {file = "tokenizers-0.13.3-cp311-cp311-win_amd64.whl", hash = "sha256:ecf182bf59bd541a8876deccf0360f5ae60496fd50b58510048020751cf1724c"},
    {file = "tokenizers-0.13.3-cp37-cp37m-macosx_10_11_x86_64.whl", hash = "sha256:0527dc5436a1f6bf2c0327da3145687d3bcfbeab91fed8458920093de3901b44"},
    {file = "tokenizers-0.13.3-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:07cbb2c307627dc99b44b22ef05ff4473aa7c7cc1fec8f0a8b37d8a64b1a16d2"},
    {file = "tokenizers-0.13.3-cp37-cp37m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:4560dbdeaae5b7ee0d4e493027e3de6d53c991b5002d7ff95083c99e11dd5ac0"},
    {file = "tokenizers-0.13.3-cp37-cp37m-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:64064bd0322405c9374305ab9b4c07152a1474370327499911937fd4a76d004b"},
    {file = "tokenizers-0.13.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:b8c6e2ab0f2e3d939ca66aa1d596602105fe33b505cd2854a4c1717f704c51de"},
    {file = "tokenizers-0.13.3-cp37-cp37m-win32.whl", hash = "sha256:6cc29d410768f960db8677221e497226e545eaaea01aa3613fa0fdf2cc96cff4"},
    {file = "tokenizers-0.13.3-cp37-cp37m-win_amd64.whl", hash = "sha256:fc2a7fdf864554a0dacf09d32e17c0caa9afe72baf9dd7ddedc61973bae352d8"},
    {file = "tokenizers-0.13.3-cp38-cp38-macosx_10_11_x86_64.whl", hash = "sha256:8791dedba834c1fc55e5f1521be325ea3dafb381964be20684b92fdac95d79b7"},
    {file = "tokenizers-0.13.3-cp38-cp38-macosx_12_0_arm64.whl", hash = "sha256:d607a6a13718aeb20507bdf2b96162ead5145bbbfa26788d6b833f98b31b26e1"},
    {file = "tokenizers-0.13.3-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:3791338f809cd1bf8e4fee6b540b36822434d0c6c6bc47162448deee3f77d425"},
    {file = "tokenizers-0.13.3-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:c2f35f30e39e6aab8716f07790f646bdc6e4a853816cc49a95ef2a9016bf9ce6"},
    {file = "tokenizers-0.13.3-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:310204dfed5aa797128b65d63538a9837cbdd15da2a29a77d67eefa489edda26"},
    {file = "tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:a0f9b92ea052305166559f38498b3b0cae159caea712646648aaa272f7160963"},
    {file = "tokenizers-0.13.3-cp38-cp38-win32.whl", hash = "sha256:9a3fa134896c3c1f0da6e762d15141fbff30d094067c8f1157b9fdca593b5806"},
    {file = "tokenizers-0.13.3-cp38-cp38-win_amd64.whl", hash = "sha256:8e7b0cdeace87fa9e760e6a605e0ae8fc14b7d72e9fc19c578116f7287bb873d"},
    {file = "tokenizers-0.13.3-cp39-cp39-macosx_10_11_x86_64.whl", hash = "sha256:00cee1e0859d55507e693a48fa4aef07060c4bb6bd93d80120e18fea9371c66d"},
    {file = "tokenizers-0.13.3-cp39-cp39-macosx_12_0_arm64.whl", hash = "sha256:a23ff602d0797cea1d0506ce69b27523b07e70f6dda982ab8cf82402de839088"},
    {file = "tokenizers-0.13.3-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:70ce07445050b537d2696022dafb115307abdffd2a5c106f029490f84501ef97"},
    {file = "tokenizers-0.13.3-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:280ffe95f50eaaf655b3a1dc7ff1d9cf4777029dbbc3e63a74e65a056594abc3"},
    {file = "tokenizers-0.13.3-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:97acfcec592f7e9de8cadcdcda50a7134423ac8455c0166b28c9ff04d227b371"},
    {file = "tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:dd7730c98a3010cd4f523465867ff95cd9d6430db46676ce79358f65ae39797b"},
    {file = "tokenizers-0.13.3-cp39-cp39-win32.whl", hash = "sha256:48625a108029cb1ddf42e17a81b5a3230ba6888a70c9dc14e81bc319e812652d"},
    {file = "tokenizers-0.13.3-cp39-cp39-win_amd64.whl", hash = "sha256:bc0a6f1ba036e482db6453571c9e3e60ecd5489980ffd95d11dc9f960483d783"},
    {file = "tokenizers-0.13.3.tar.gz", hash = "sha256:2e546dbb68b623008a5442353137fbb0123d311a6d7ba52f2667c8862a75af2e"},
]

[package.extras]
dev = ["black (==22.3)", "datasets", "numpy", "pytest", "requests"]
docs = ["setuptools-rust", "sphinx", "sphinx-rtd-theme"]
testing = ["black (==22.3)", "datasets", "numpy", "pytest", "requests"]

[[package]]
name = "tomli"
version = "2.0.1"
description = "A lil' TOML parser"
category = "dev"
optional = false
python-versions = ">=3.7"
files = [
    {file = "tomli-2.0.1-py3-none-any.whl", hash = "sha256:939de3e7a6161af0c887ef91b7d41a53e7c5a1ca976325f429cb46ea9bc30ecc"},
    {file = "tomli-2.0.1.tar.gz", hash = "sha256:de526c12914f0c550d15924c62d72abc48d6fe7364aa87328337a31007fe8a4f"},
]

[[package]]
name = "tomlkit"
version = "0.11.8"
description = "Style preserving TOML library"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "tomlkit-0.11.8-py3-none-any.whl", hash = "sha256:8c726c4c202bdb148667835f68d68780b9a003a9ec34167b6c673b38eff2a171"},
    {file = "tomlkit-0.11.8.tar.gz", hash = "sha256:9330fc7faa1db67b541b28e62018c17d20be733177d290a13b24c62d1614e0c3"},
]

[[package]]
name = "tqdm"
version = "4.65.0"
description = "Fast, Extensible Progress Meter"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "tqdm-4.65.0-py3-none-any.whl", hash = "sha256:c4f53a17fe37e132815abceec022631be8ffe1b9381c2e6e30aa70edc99e9671"},
    {file = "tqdm-4.65.0.tar.gz", hash = "sha256:1871fb68a86b8fb3b59ca4cdd3dcccbc7e6d613eeed31f4c332531977b89beb5"},
]

[package.dependencies]
colorama = {version = "*", markers = "platform_system == \"Windows\""}

[package.extras]
dev = ["py-make (>=0.1.0)", "twine", "wheel"]
notebook = ["ipywidgets (>=6)"]
slack = ["slack-sdk"]
telegram = ["requests"]

[[package]]
name = "twine"
version = "3.8.0"
description = "Collection of utilities for publishing packages on PyPI"
category = "main"
optional = false
python-versions = ">=3.6"
files = [
    {file = "twine-3.8.0-py3-none-any.whl", hash = "sha256:d0550fca9dc19f3d5e8eadfce0c227294df0a2a951251a4385797c8a6198b7c8"},
    {file = "twine-3.8.0.tar.gz", hash = "sha256:8efa52658e0ae770686a13b675569328f1fba9837e5de1867bfe5f46a9aefe19"},
]

[package.dependencies]
colorama = ">=0.4.3"
importlib-metadata = ">=3.6"
keyring = ">=15.1"
pkginfo = ">=1.8.1"
readme-renderer = ">=21.0"
requests = ">=2.20"
requests-toolbelt = ">=0.8.0,<0.9.0 || >0.9.0"
rfc3986 = ">=1.4.0"
tqdm = ">=4.14"
urllib3 = ">=1.26.0"

[[package]]
name = "typing-extensions"
version = "4.6.2"
description = "Backported and Experimental Type Hints for Python 3.7+"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "typing_extensions-4.6.2-py3-none-any.whl", hash = "sha256:3a8b36f13dd5fdc5d1b16fe317f5668545de77fa0b8e02006381fd49d731ab98"},
    {file = "typing_extensions-4.6.2.tar.gz", hash = "sha256:06006244c70ac8ee83fa8282cb188f697b8db25bc8b4df07be1873c43897060c"},
]

[[package]]
name = "typing-inspect"
version = "0.9.0"
description = "Runtime inspection utilities for typing module."
category = "main"
optional = false
python-versions = "*"
files = [
    {file = "typing_inspect-0.9.0-py3-none-any.whl", hash = "sha256:9ee6fc59062311ef8547596ab6b955e1b8aa46242d854bfc78f4f6b0eff35f9f"},
    {file = "typing_inspect-0.9.0.tar.gz", hash = "sha256:b23fc42ff6f6ef6954e4852c1fb512cdd18dbea03134f91f856a95ccc9461f78"},
]

[package.dependencies]
mypy-extensions = ">=0.3.0"
typing-extensions = ">=3.7.4"

[[package]]
name = "tzdata"
version = "2023.3"
description = "Provider of IANA time zone data"
category = "main"
optional = false
python-versions = ">=2"
files = [
    {file = "tzdata-2023.3-py2.py3-none-any.whl", hash = "sha256:7e65763eef3120314099b6939b5546db7adce1e7d6f2e179e3df563c70511eda"},
    {file = "tzdata-2023.3.tar.gz", hash = "sha256:11ef1e08e54acb0d4f95bdb1be05da659673de4acbd21bf9c69e94cc5e907a3a"},
]

[[package]]
name = "ujson"
version = "5.7.0"
description = "Ultra fast JSON encoder and decoder for Python"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "ujson-5.7.0-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:5eba5e69e4361ac3a311cf44fa71bc619361b6e0626768a494771aacd1c2f09b"},
    {file = "ujson-5.7.0-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:aae4d9e1b4c7b61780f0a006c897a4a1904f862fdab1abb3ea8f45bd11aa58f3"},
    {file = "ujson-5.7.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d2e43ccdba1cb5c6d3448eadf6fc0dae7be6c77e357a3abc968d1b44e265866d"},
    {file = "ujson-5.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:54384ce4920a6d35fa9ea8e580bc6d359e3eb961fa7e43f46c78e3ed162d56ff"},
    {file = "ujson-5.7.0-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:24ad1aa7fc4e4caa41d3d343512ce68e41411fb92adf7f434a4d4b3749dc8f58"},
    {file = "ujson-5.7.0-cp310-cp310-musllinux_1_1_aarch64.whl", hash = "sha256:afff311e9f065a8f03c3753db7011bae7beb73a66189c7ea5fcb0456b7041ea4"},
    {file = "ujson-5.7.0-cp310-cp310-musllinux_1_1_i686.whl", hash = "sha256:6e80f0d03e7e8646fc3d79ed2d875cebd4c83846e129737fdc4c2532dbd43d9e"},
    {file = "ujson-5.7.0-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:137831d8a0db302fb6828ee21c67ad63ac537bddc4376e1aab1c8573756ee21c"},
    {file = "ujson-5.7.0-cp310-cp310-win32.whl", hash = "sha256:7df3fd35ebc14dafeea031038a99232b32f53fa4c3ecddb8bed132a43eefb8ad"},
    {file = "ujson-5.7.0-cp310-cp310-win_amd64.whl", hash = "sha256:af4639f684f425177d09ae409c07602c4096a6287027469157bfb6f83e01448b"},
    {file = "ujson-5.7.0-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:9b0f2680ce8a70f77f5d70aaf3f013d53e6af6d7058727a35d8ceb4a71cdd4e9"},
    {file = "ujson-5.7.0-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:67a19fd8e7d8cc58a169bea99fed5666023adf707a536d8f7b0a3c51dd498abf"},
    {file = "ujson-5.7.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:6abb8e6d8f1ae72f0ed18287245f5b6d40094e2656d1eab6d99d666361514074"},
    {file = "ujson-5.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:d8cd622c069368d5074bd93817b31bdb02f8d818e57c29e206f10a1f9c6337dd"},
    {file = "ujson-5.7.0-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:14f9082669f90e18e64792b3fd0bf19f2b15e7fe467534a35ea4b53f3bf4b755"},
    {file = "ujson-5.7.0-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:d7ff6ebb43bc81b057724e89550b13c9a30eda0f29c2f506f8b009895438f5a6"},
    {file = "ujson-5.7.0-cp311-cp311-musllinux_1_1_i686.whl", hash = "sha256:f7f241488879d91a136b299e0c4ce091996c684a53775e63bb442d1a8e9ae22a"},
    {file = "ujson-5.7.0-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:5593263a7fcfb934107444bcfba9dde8145b282de0ee9f61e285e59a916dda0f"},
    {file = "ujson-5.7.0-cp311-cp311-win32.whl", hash = "sha256:26c2b32b489c393106e9cb68d0a02e1a7b9d05a07429d875c46b94ee8405bdb7"},
    {file = "ujson-5.7.0-cp311-cp311-win_amd64.whl", hash = "sha256:ed24406454bb5a31df18f0a423ae14beb27b28cdfa34f6268e7ebddf23da807e"},
    {file = "ujson-5.7.0-cp37-cp37m-macosx_10_9_x86_64.whl", hash = "sha256:18679484e3bf9926342b1c43a3bd640f93a9eeeba19ef3d21993af7b0c44785d"},
    {file = "ujson-5.7.0-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0ee295761e1c6c30400641f0a20d381633d7622633cdf83a194f3c876a0e4b7e"},
    {file = "ujson-5.7.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:b738282e12a05f400b291966630a98d622da0938caa4bc93cf65adb5f4281c60"},
    {file = "ujson-5.7.0-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:00343501dbaa5172e78ef0e37f9ebd08040110e11c12420ff7c1f9f0332d939e"},
    {file = "ujson-5.7.0-cp37-cp37m-musllinux_1_1_aarch64.whl", hash = "sha256:c0d1f7c3908357ee100aa64c4d1cf91edf99c40ac0069422a4fd5fd23b263263"},
    {file = "ujson-5.7.0-cp37-cp37m-musllinux_1_1_i686.whl", hash = "sha256:a5d2f44331cf04689eafac7a6596c71d6657967c07ac700b0ae1c921178645da"},
    {file = "ujson-5.7.0-cp37-cp37m-musllinux_1_1_x86_64.whl", hash = "sha256:16b2254a77b310f118717715259a196662baa6b1f63b1a642d12ab1ff998c3d7"},
    {file = "ujson-5.7.0-cp37-cp37m-win32.whl", hash = "sha256:6faf46fa100b2b89e4db47206cf8a1ffb41542cdd34dde615b2fc2288954f194"},
    {file = "ujson-5.7.0-cp37-cp37m-win_amd64.whl", hash = "sha256:ff0004c3f5a9a6574689a553d1b7819d1a496b4f005a7451f339dc2d9f4cf98c"},
    {file = "ujson-5.7.0-cp38-cp38-macosx_10_9_x86_64.whl", hash = "sha256:75204a1dd7ec6158c8db85a2f14a68d2143503f4bafb9a00b63fe09d35762a5e"},
    {file = "ujson-5.7.0-cp38-cp38-macosx_11_0_arm64.whl", hash = "sha256:7312731c7826e6c99cdd3ac503cd9acd300598e7a80bcf41f604fee5f49f566c"},
    {file = "ujson-5.7.0-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:7b9dc5a90e2149643df7f23634fe202fed5ebc787a2a1be95cf23632b4d90651"},
    {file = "ujson-5.7.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:b6a6961fc48821d84b1198a09516e396d56551e910d489692126e90bf4887d29"},
    {file = "ujson-5.7.0-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:b01a9af52a0d5c46b2c68e3f258fdef2eacaa0ce6ae3e9eb97983f5b1166edb6"},
    {file = "ujson-5.7.0-cp38-cp38-musllinux_1_1_aarch64.whl", hash = "sha256:b7316d3edeba8a403686cdcad4af737b8415493101e7462a70ff73dd0609eafc"},
    {file = "ujson-5.7.0-cp38-cp38-musllinux_1_1_i686.whl", hash = "sha256:4ee997799a23227e2319a3f8817ce0b058923dbd31904761b788dc8f53bd3e30"},
    {file = "ujson-5.7.0-cp38-cp38-musllinux_1_1_x86_64.whl", hash = "sha256:dda9aa4c33435147262cd2ea87c6b7a1ca83ba9b3933ff7df34e69fee9fced0c"},
    {file = "ujson-5.7.0-cp38-cp38-win32.whl", hash = "sha256:bea8d30e362180aafecabbdcbe0e1f0b32c9fa9e39c38e4af037b9d3ca36f50c"},
    {file = "ujson-5.7.0-cp38-cp38-win_amd64.whl", hash = "sha256:c96e3b872bf883090ddf32cc41957edf819c5336ab0007d0cf3854e61841726d"},
    {file = "ujson-5.7.0-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:6411aea4c94a8e93c2baac096fbf697af35ba2b2ed410b8b360b3c0957a952d3"},
    {file = "ujson-5.7.0-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:3d3b3499c55911f70d4e074c626acdb79a56f54262c3c83325ffb210fb03e44d"},
    {file = "ujson-5.7.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:341f891d45dd3814d31764626c55d7ab3fd21af61fbc99d070e9c10c1190680b"},
    {file = "ujson-5.7.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:2f242eec917bafdc3f73a1021617db85f9958df80f267db69c76d766058f7b19"},
    {file = "ujson-5.7.0-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:c3af9f9f22a67a8c9466a32115d9073c72a33ae627b11de6f592df0ee09b98b6"},
    {file = "ujson-5.7.0-cp39-cp39-musllinux_1_1_aarch64.whl", hash = "sha256:4a3d794afbf134df3056a813e5c8a935208cddeae975bd4bc0ef7e89c52f0ce0"},
    {file = "ujson-5.7.0-cp39-cp39-musllinux_1_1_i686.whl", hash = "sha256:800bf998e78dae655008dd10b22ca8dc93bdcfcc82f620d754a411592da4bbf2"},
    {file = "ujson-5.7.0-cp39-cp39-musllinux_1_1_x86_64.whl", hash = "sha256:b5ac3d5c5825e30b438ea92845380e812a476d6c2a1872b76026f2e9d8060fc2"},
    {file = "ujson-5.7.0-cp39-cp39-win32.whl", hash = "sha256:cd90027e6d93e8982f7d0d23acf88c896d18deff1903dd96140613389b25c0dd"},
    {file = "ujson-5.7.0-cp39-cp39-win_amd64.whl", hash = "sha256:523ee146cdb2122bbd827f4dcc2a8e66607b3f665186bce9e4f78c9710b6d8ab"},
    {file = "ujson-5.7.0-pp37-pypy37_pp73-macosx_10_9_x86_64.whl", hash = "sha256:e87cec407ec004cf1b04c0ed7219a68c12860123dfb8902ef880d3d87a71c172"},
    {file = "ujson-5.7.0-pp37-pypy37_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:bab10165db6a7994e67001733f7f2caf3400b3e11538409d8756bc9b1c64f7e8"},
    {file = "ujson-5.7.0-pp37-pypy37_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:b522be14a28e6ac1cf818599aeff1004a28b42df4ed4d7bc819887b9dac915fc"},
    {file = "ujson-5.7.0-pp37-pypy37_pp73-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:7592f40175c723c032cdbe9fe5165b3b5903604f774ab0849363386e99e1f253"},
    {file = "ujson-5.7.0-pp37-pypy37_pp73-win_amd64.whl", hash = "sha256:ed22f9665327a981f288a4f758a432824dc0314e4195a0eaeb0da56a477da94d"},
    {file = "ujson-5.7.0-pp38-pypy38_pp73-macosx_10_9_x86_64.whl", hash = "sha256:adf445a49d9a97a5a4c9bb1d652a1528de09dd1c48b29f79f3d66cea9f826bf6"},
    {file = "ujson-5.7.0-pp38-pypy38_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:64772a53f3c4b6122ed930ae145184ebaed38534c60f3d859d8c3f00911eb122"},
    {file = "ujson-5.7.0-pp38-pypy38_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:35209cb2c13fcb9d76d249286105b4897b75a5e7f0efb0c0f4b90f222ce48910"},
    {file = "ujson-5.7.0-pp38-pypy38_pp73-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:90712dfc775b2c7a07d4d8e059dd58636bd6ff1776d79857776152e693bddea6"},
    {file = "ujson-5.7.0-pp38-pypy38_pp73-win_amd64.whl", hash = "sha256:0e4e8981c6e7e9e637e637ad8ffe948a09e5434bc5f52ecbb82b4b4cfc092bfb"},
    {file = "ujson-5.7.0-pp39-pypy39_pp73-macosx_10_9_x86_64.whl", hash = "sha256:581c945b811a3d67c27566539bfcb9705ea09cb27c4be0002f7a553c8886b817"},
    {file = "ujson-5.7.0-pp39-pypy39_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d36a807a24c7d44f71686685ae6fbc8793d784bca1adf4c89f5f780b835b6243"},
    {file = "ujson-5.7.0-pp39-pypy39_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8b4257307e3662aa65e2644a277ca68783c5d51190ed9c49efebdd3cbfd5fa44"},
    {file = "ujson-5.7.0-pp39-pypy39_pp73-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:ea7423d8a2f9e160c5e011119741682414c5b8dce4ae56590a966316a07a4618"},
    {file = "ujson-5.7.0-pp39-pypy39_pp73-win_amd64.whl", hash = "sha256:4c592eb91a5968058a561d358d0fef59099ed152cfb3e1cd14eee51a7a93879e"},
    {file = "ujson-5.7.0.tar.gz", hash = "sha256:e788e5d5dcae8f6118ac9b45d0b891a0d55f7ac480eddcb7f07263f2bcf37b23"},
]

[[package]]
name = "urllib3"
version = "1.26.16"
description = "HTTP library with thread-safe connection pooling, file post, and more."
category = "main"
optional = false
python-versions = ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*, !=3.5.*"
files = [
    {file = "urllib3-1.26.16-py2.py3-none-any.whl", hash = "sha256:8d36afa7616d8ab714608411b4a3b13e58f463aee519024578e062e141dce20f"},
    {file = "urllib3-1.26.16.tar.gz", hash = "sha256:8f135f6502756bde6b2a9b28989df5fbe87c9970cecaa69041edcce7f0589b14"},
]

[package.extras]
brotli = ["brotli (>=1.0.9)", "brotlicffi (>=0.8.0)", "brotlipy (>=0.6.0)"]
secure = ["certifi", "cryptography (>=1.3.4)", "idna (>=2.0.0)", "ipaddress", "pyOpenSSL (>=0.14)", "urllib3-secure-extra"]
socks = ["PySocks (>=1.5.6,!=1.5.7,<2.0)"]

[[package]]
name = "uvicorn"
version = "0.20.0"
description = "The lightning-fast ASGI server."
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "uvicorn-0.20.0-py3-none-any.whl", hash = "sha256:c3ed1598a5668208723f2bb49336f4509424ad198d6ab2615b7783db58d919fd"},
    {file = "uvicorn-0.20.0.tar.gz", hash = "sha256:a4e12017b940247f836bc90b72e725d7dfd0c8ed1c51eb365f5ba30d9f5127d8"},
]

[package.dependencies]
click = ">=7.0"
colorama = {version = ">=0.4", optional = true, markers = "sys_platform == \"win32\" and extra == \"standard\""}
h11 = ">=0.8"
httptools = {version = ">=0.5.0", optional = true, markers = "extra == \"standard\""}
python-dotenv = {version = ">=0.13", optional = true, markers = "extra == \"standard\""}
pyyaml = {version = ">=5.1", optional = true, markers = "extra == \"standard\""}
uvloop = {version = ">=0.14.0,<0.15.0 || >0.15.0,<0.15.1 || >0.15.1", optional = true, markers = "sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"PyPy\" and extra == \"standard\""}
watchfiles = {version = ">=0.13", optional = true, markers = "extra == \"standard\""}
websockets = {version = ">=10.4", optional = true, markers = "extra == \"standard\""}

[package.extras]
standard = ["colorama (>=0.4)", "httptools (>=0.5.0)", "python-dotenv (>=0.13)", "pyyaml (>=5.1)", "uvloop (>=0.14.0,!=0.15.0,!=0.15.1)", "watchfiles (>=0.13)", "websockets (>=10.4)"]

[[package]]
name = "uvloop"
version = "0.17.0"
description = "Fast implementation of asyncio event loop on top of libuv"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "uvloop-0.17.0-cp310-cp310-macosx_10_9_universal2.whl", hash = "sha256:ce9f61938d7155f79d3cb2ffa663147d4a76d16e08f65e2c66b77bd41b356718"},
    {file = "uvloop-0.17.0-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:68532f4349fd3900b839f588972b3392ee56042e440dd5873dfbbcd2cc67617c"},
    {file = "uvloop-0.17.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0949caf774b9fcefc7c5756bacbbbd3fc4c05a6b7eebc7c7ad6f825b23998d6d"},
    {file = "uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:ff3d00b70ce95adce264462c930fbaecb29718ba6563db354608f37e49e09024"},
    {file = "uvloop-0.17.0-cp310-cp310-musllinux_1_1_aarch64.whl", hash = "sha256:a5abddb3558d3f0a78949c750644a67be31e47936042d4f6c888dd6f3c95f4aa"},
    {file = "uvloop-0.17.0-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:8efcadc5a0003d3a6e887ccc1fb44dec25594f117a94e3127954c05cf144d811"},
    {file = "uvloop-0.17.0-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:3378eb62c63bf336ae2070599e49089005771cc651c8769aaad72d1bd9385a7c"},
    {file = "uvloop-0.17.0-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:6aafa5a78b9e62493539456f8b646f85abc7093dd997f4976bb105537cf2635e"},
    {file = "uvloop-0.17.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:c686a47d57ca910a2572fddfe9912819880b8765e2f01dc0dd12a9bf8573e539"},
    {file = "uvloop-0.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:864e1197139d651a76c81757db5eb199db8866e13acb0dfe96e6fc5d1cf45fc4"},
    {file = "uvloop-0.17.0-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:2a6149e1defac0faf505406259561bc14b034cdf1d4711a3ddcdfbaa8d825a05"},
    {file = "uvloop-0.17.0-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:6708f30db9117f115eadc4f125c2a10c1a50d711461699a0cbfaa45b9a78e376"},
    {file = "uvloop-0.17.0-cp37-cp37m-macosx_10_9_x86_64.whl", hash = "sha256:23609ca361a7fc587031429fa25ad2ed7242941adec948f9d10c045bfecab06b"},
    {file = "uvloop-0.17.0-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:2deae0b0fb00a6af41fe60a675cec079615b01d68beb4cc7b722424406b126a8"},
    {file = "uvloop-0.17.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:45cea33b208971e87a31c17622e4b440cac231766ec11e5d22c76fab3bf9df62"},
    {file = "uvloop-0.17.0-cp37-cp37m-musllinux_1_1_aarch64.whl", hash = "sha256:9b09e0f0ac29eee0451d71798878eae5a4e6a91aa275e114037b27f7db72702d"},
    {file = "uvloop-0.17.0-cp37-cp37m-musllinux_1_1_x86_64.whl", hash = "sha256:dbbaf9da2ee98ee2531e0c780455f2841e4675ff580ecf93fe5c48fe733b5667"},
    {file = "uvloop-0.17.0-cp38-cp38-macosx_10_9_universal2.whl", hash = "sha256:a4aee22ece20958888eedbad20e4dbb03c37533e010fb824161b4f05e641f738"},
    {file = "uvloop-0.17.0-cp38-cp38-macosx_10_9_x86_64.whl", hash = "sha256:307958f9fc5c8bb01fad752d1345168c0abc5d62c1b72a4a8c6c06f042b45b20"},
    {file = "uvloop-0.17.0-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:3ebeeec6a6641d0adb2ea71dcfb76017602ee2bfd8213e3fcc18d8f699c5104f"},
    {file = "uvloop-0.17.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:1436c8673c1563422213ac6907789ecb2b070f5939b9cbff9ef7113f2b531595"},
    {file = "uvloop-0.17.0-cp38-cp38-musllinux_1_1_aarch64.whl", hash = "sha256:8887d675a64cfc59f4ecd34382e5b4f0ef4ae1da37ed665adba0c2badf0d6578"},
    {file = "uvloop-0.17.0-cp38-cp38-musllinux_1_1_x86_64.whl", hash = "sha256:3db8de10ed684995a7f34a001f15b374c230f7655ae840964d51496e2f8a8474"},
    {file = "uvloop-0.17.0-cp39-cp39-macosx_10_9_universal2.whl", hash = "sha256:7d37dccc7ae63e61f7b96ee2e19c40f153ba6ce730d8ba4d3b4e9738c1dccc1b"},
    {file = "uvloop-0.17.0-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:cbbe908fda687e39afd6ea2a2f14c2c3e43f2ca88e3a11964b297822358d0e6c"},
    {file = "uvloop-0.17.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:3d97672dc709fa4447ab83276f344a165075fd9f366a97b712bdd3fee05efae8"},
    {file = "uvloop-0.17.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:f1e507c9ee39c61bfddd79714e4f85900656db1aec4d40c6de55648e85c2799c"},
    {file = "uvloop-0.17.0-cp39-cp39-musllinux_1_1_aarch64.whl", hash = "sha256:c092a2c1e736086d59ac8e41f9c98f26bbf9b9222a76f21af9dfe949b99b2eb9"},
    {file = "uvloop-0.17.0-cp39-cp39-musllinux_1_1_x86_64.whl", hash = "sha256:30babd84706115626ea78ea5dbc7dd8d0d01a2e9f9b306d24ca4ed5796c66ded"},
    {file = "uvloop-0.17.0.tar.gz", hash = "sha256:0ddf6baf9cf11a1a22c71487f39f15b2cf78eb5bde7e5b45fbb99e8a9d91b9e1"},
]

[package.extras]
dev = ["Cython (>=0.29.32,<0.30.0)", "Sphinx (>=4.1.2,<4.2.0)", "aiohttp", "flake8 (>=3.9.2,<3.10.0)", "mypy (>=0.800)", "psutil", "pyOpenSSL (>=22.0.0,<22.1.0)", "pycodestyle (>=2.7.0,<2.8.0)", "pytest (>=3.6.0)", "sphinx-rtd-theme (>=0.5.2,<0.6.0)", "sphinxcontrib-asyncio (>=0.3.0,<0.4.0)"]
docs = ["Sphinx (>=4.1.2,<4.2.0)", "sphinx-rtd-theme (>=0.5.2,<0.6.0)", "sphinxcontrib-asyncio (>=0.3.0,<0.4.0)"]
test = ["Cython (>=0.29.32,<0.30.0)", "aiohttp", "flake8 (>=3.9.2,<3.10.0)", "mypy (>=0.800)", "psutil", "pyOpenSSL (>=22.0.0,<22.1.0)", "pycodestyle (>=2.7.0,<2.8.0)"]

[[package]]
name = "validators"
version = "0.20.0"
description = "Python Data Validation for Humansâ„¢."
category = "main"
optional = false
python-versions = ">=3.4"
files = [
    {file = "validators-0.20.0.tar.gz", hash = "sha256:24148ce4e64100a2d5e267233e23e7afeb55316b47d30faae7eb6e7292bc226a"},
]

[package.dependencies]
decorator = ">=3.4.0"

[package.extras]
test = ["flake8 (>=2.4.0)", "isort (>=4.2.2)", "pytest (>=2.2.3)"]

[[package]]
name = "watchfiles"
version = "0.19.0"
description = "Simple, modern and high performance file watching and code reload in python."
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "watchfiles-0.19.0-cp37-abi3-macosx_10_7_x86_64.whl", hash = "sha256:91633e64712df3051ca454ca7d1b976baf842d7a3640b87622b323c55f3345e7"},
    {file = "watchfiles-0.19.0-cp37-abi3-macosx_11_0_arm64.whl", hash = "sha256:b6577b8c6c8701ba8642ea9335a129836347894b666dd1ec2226830e263909d3"},
    {file = "watchfiles-0.19.0-cp37-abi3-manylinux_2_12_i686.manylinux2010_i686.whl", hash = "sha256:18b28f6ad871b82df9542ff958d0c86bb0d8310bb09eb8e87d97318a3b5273af"},
    {file = "watchfiles-0.19.0-cp37-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:fac19dc9cbc34052394dbe81e149411a62e71999c0a19e1e09ce537867f95ae0"},
    {file = "watchfiles-0.19.0-cp37-abi3-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:09ea3397aecbc81c19ed7f025e051a7387feefdb789cf768ff994c1228182fda"},
    {file = "watchfiles-0.19.0-cp37-abi3-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:c0376deac92377817e4fb8f347bf559b7d44ff556d9bc6f6208dd3f79f104aaf"},
    {file = "watchfiles-0.19.0-cp37-abi3-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:9c75eff897786ee262c9f17a48886f4e98e6cfd335e011c591c305e5d083c056"},
    {file = "watchfiles-0.19.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:cb5d45c4143c1dd60f98a16187fd123eda7248f84ef22244818c18d531a249d1"},
    {file = "watchfiles-0.19.0-cp37-abi3-musllinux_1_1_aarch64.whl", hash = "sha256:79c533ff593db861ae23436541f481ec896ee3da4e5db8962429b441bbaae16e"},
    {file = "watchfiles-0.19.0-cp37-abi3-musllinux_1_1_x86_64.whl", hash = "sha256:3d7d267d27aceeeaa3de0dd161a0d64f0a282264d592e335fff7958cc0cbae7c"},
    {file = "watchfiles-0.19.0-cp37-abi3-win32.whl", hash = "sha256:176a9a7641ec2c97b24455135d58012a5be5c6217fc4d5fef0b2b9f75dbf5154"},
    {file = "watchfiles-0.19.0-cp37-abi3-win_amd64.whl", hash = "sha256:945be0baa3e2440151eb3718fd8846751e8b51d8de7b884c90b17d271d34cae8"},
    {file = "watchfiles-0.19.0-cp37-abi3-win_arm64.whl", hash = "sha256:0089c6dc24d436b373c3c57657bf4f9a453b13767150d17284fc6162b2791911"},
    {file = "watchfiles-0.19.0-pp38-pypy38_pp73-macosx_10_7_x86_64.whl", hash = "sha256:cae3dde0b4b2078f31527acff6f486e23abed307ba4d3932466ba7cdd5ecec79"},
    {file = "watchfiles-0.19.0-pp38-pypy38_pp73-macosx_11_0_arm64.whl", hash = "sha256:7f3920b1285a7d3ce898e303d84791b7bf40d57b7695ad549dc04e6a44c9f120"},
    {file = "watchfiles-0.19.0-pp38-pypy38_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:9afd0d69429172c796164fd7fe8e821ade9be983f51c659a38da3faaaaac44dc"},
    {file = "watchfiles-0.19.0-pp38-pypy38_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:68dce92b29575dda0f8d30c11742a8e2b9b8ec768ae414b54f7453f27bdf9545"},
    {file = "watchfiles-0.19.0-pp39-pypy39_pp73-macosx_10_7_x86_64.whl", hash = "sha256:5569fc7f967429d4bc87e355cdfdcee6aabe4b620801e2cf5805ea245c06097c"},
    {file = "watchfiles-0.19.0-pp39-pypy39_pp73-macosx_11_0_arm64.whl", hash = "sha256:5471582658ea56fca122c0f0d0116a36807c63fefd6fdc92c71ca9a4491b6b48"},
    {file = "watchfiles-0.19.0-pp39-pypy39_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:b538014a87f94d92f98f34d3e6d2635478e6be6423a9ea53e4dd96210065e193"},
    {file = "watchfiles-0.19.0-pp39-pypy39_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:20b44221764955b1e703f012c74015306fb7e79a00c15370785f309b1ed9aa8d"},
    {file = "watchfiles-0.19.0.tar.gz", hash = "sha256:d9b073073e048081e502b6c6b0b88714c026a1a4c890569238d04aca5f9ca74b"},
]

[package.dependencies]
anyio = ">=3.0.0"

[[package]]
name = "weaviate-client"
version = "3.19.2"
description = "A python native weaviate client"
category = "main"
optional = false
python-versions = ">=3.8"
files = [
    {file = "weaviate-client-3.19.2.tar.gz", hash = "sha256:662cb2a5f6dacc2c9cdf6db2df70e9a3ac9d18b404d0c2ff971d9cb85d84ebed"},
    {file = "weaviate_client-3.19.2-py3-none-any.whl", hash = "sha256:f4bbfb868907089f57fdfb836c4d00cf8a6fc5e296fa08879681ba1d2273cd40"},
]

[package.dependencies]
authlib = ">=1.1.0"
requests = ">=2.28.0,<2.29.0"
tqdm = ">=4.59.0,<5.0.0"
validators = ">=0.18.2,<=0.21.0"

[package.extras]
grpc = ["grpcio", "grpcio-tools"]

[[package]]
name = "webencodings"
version = "0.5.1"
description = "Character encoding aliases for legacy web content"
category = "main"
optional = false
python-versions = "*"
files = [
    {file = "webencodings-0.5.1-py2.py3-none-any.whl", hash = "sha256:a0af1213f3c2226497a97e2b3aa01a7e4bee4f403f95be16fc9acd2947514a78"},
    {file = "webencodings-0.5.1.tar.gz", hash = "sha256:b36a1c245f2d304965eb4e0a82848379241dc04b865afcc4aab16748587e1923"},
]

[[package]]
name = "websockets"
version = "10.4"
description = "An implementation of the WebSocket Protocol (RFC 6455 & 7692)"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "websockets-10.4-cp310-cp310-macosx_10_9_universal2.whl", hash = "sha256:d58804e996d7d2307173d56c297cf7bc132c52df27a3efaac5e8d43e36c21c48"},
    {file = "websockets-10.4-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:bc0b82d728fe21a0d03e65f81980abbbcb13b5387f733a1a870672c5be26edab"},
    {file = "websockets-10.4-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:ba089c499e1f4155d2a3c2a05d2878a3428cf321c848f2b5a45ce55f0d7d310c"},
    {file = "websockets-10.4-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:33d69ca7612f0ddff3316b0c7b33ca180d464ecac2d115805c044bf0a3b0d032"},
    {file = "websockets-10.4-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:62e627f6b6d4aed919a2052efc408da7a545c606268d5ab5bfab4432734b82b4"},
    {file = "websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:38ea7b82bfcae927eeffc55d2ffa31665dc7fec7b8dc654506b8e5a518eb4d50"},
    {file = "websockets-10.4-cp310-cp310-musllinux_1_1_aarch64.whl", hash = "sha256:e0cb5cc6ece6ffa75baccfd5c02cffe776f3f5c8bf486811f9d3ea3453676ce8"},
    {file = "websockets-10.4-cp310-cp310-musllinux_1_1_i686.whl", hash = "sha256:ae5e95cfb53ab1da62185e23b3130e11d64431179debac6dc3c6acf08760e9b1"},
    {file = "websockets-10.4-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:7c584f366f46ba667cfa66020344886cf47088e79c9b9d39c84ce9ea98aaa331"},
    {file = "websockets-10.4-cp310-cp310-win32.whl", hash = "sha256:b029fb2032ae4724d8ae8d4f6b363f2cc39e4c7b12454df8df7f0f563ed3e61a"},
    {file = "websockets-10.4-cp310-cp310-win_amd64.whl", hash = "sha256:8dc96f64ae43dde92530775e9cb169979f414dcf5cff670455d81a6823b42089"},
    {file = "websockets-10.4-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:47a2964021f2110116cc1125b3e6d87ab5ad16dea161949e7244ec583b905bb4"},
    {file = "websockets-10.4-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:e789376b52c295c4946403bd0efecf27ab98f05319df4583d3c48e43c7342c2f"},
    {file = "websockets-10.4-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:7d3f0b61c45c3fa9a349cf484962c559a8a1d80dae6977276df8fd1fa5e3cb8c"},
    {file = "websockets-10.4-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:f55b5905705725af31ccef50e55391621532cd64fbf0bc6f4bac935f0fccec46"},
    {file = "websockets-10.4-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:00c870522cdb69cd625b93f002961ffb0c095394f06ba8c48f17eef7c1541f96"},
    {file = "websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8f38706e0b15d3c20ef6259fd4bc1700cd133b06c3c1bb108ffe3f8947be15fa"},
    {file = "websockets-10.4-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:f2c38d588887a609191d30e902df2a32711f708abfd85d318ca9b367258cfd0c"},
    {file = "websockets-10.4-cp311-cp311-musllinux_1_1_i686.whl", hash = "sha256:fe10ddc59b304cb19a1bdf5bd0a7719cbbc9fbdd57ac80ed436b709fcf889106"},
    {file = "websockets-10.4-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:90fcf8929836d4a0e964d799a58823547df5a5e9afa83081761630553be731f9"},
    {file = "websockets-10.4-cp311-cp311-win32.whl", hash = "sha256:b9968694c5f467bf67ef97ae7ad4d56d14be2751000c1207d31bf3bb8860bae8"},
    {file = "websockets-10.4-cp311-cp311-win_amd64.whl", hash = "sha256:a7a240d7a74bf8d5cb3bfe6be7f21697a28ec4b1a437607bae08ac7acf5b4882"},
    {file = "websockets-10.4-cp37-cp37m-macosx_10_9_x86_64.whl", hash = "sha256:74de2b894b47f1d21cbd0b37a5e2b2392ad95d17ae983e64727e18eb281fe7cb"},
    {file = "websockets-10.4-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:e3a686ecb4aa0d64ae60c9c9f1a7d5d46cab9bfb5d91a2d303d00e2cd4c4c5cc"},
    {file = "websockets-10.4-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:b0d15c968ea7a65211e084f523151dbf8ae44634de03c801b8bd070b74e85033"},
    {file = "websockets-10.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:00213676a2e46b6ebf6045bc11d0f529d9120baa6f58d122b4021ad92adabd41"},
    {file = "websockets-10.4-cp37-cp37m-musllinux_1_1_aarch64.whl", hash = "sha256:e23173580d740bf8822fd0379e4bf30aa1d5a92a4f252d34e893070c081050df"},
    {file = "websockets-10.4-cp37-cp37m-musllinux_1_1_i686.whl", hash = "sha256:dd500e0a5e11969cdd3320935ca2ff1e936f2358f9c2e61f100a1660933320ea"},
    {file = "websockets-10.4-cp37-cp37m-musllinux_1_1_x86_64.whl", hash = "sha256:4239b6027e3d66a89446908ff3027d2737afc1a375f8fd3eea630a4842ec9a0c"},
    {file = "websockets-10.4-cp37-cp37m-win32.whl", hash = "sha256:8a5cc00546e0a701da4639aa0bbcb0ae2bb678c87f46da01ac2d789e1f2d2038"},
    {file = "websockets-10.4-cp37-cp37m-win_amd64.whl", hash = "sha256:a9f9a735deaf9a0cadc2d8c50d1a5bcdbae8b6e539c6e08237bc4082d7c13f28"},
    {file = "websockets-10.4-cp38-cp38-macosx_10_9_universal2.whl", hash = "sha256:5c1289596042fad2cdceb05e1ebf7aadf9995c928e0da2b7a4e99494953b1b94"},
    {file = "websockets-10.4-cp38-cp38-macosx_10_9_x86_64.whl", hash = "sha256:0cff816f51fb33c26d6e2b16b5c7d48eaa31dae5488ace6aae468b361f422b63"},
    {file = "websockets-10.4-cp38-cp38-macosx_11_0_arm64.whl", hash = "sha256:dd9becd5fe29773d140d68d607d66a38f60e31b86df75332703757ee645b6faf"},
    {file = "websockets-10.4-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:45ec8e75b7dbc9539cbfafa570742fe4f676eb8b0d3694b67dabe2f2ceed8aa6"},
    {file = "websockets-10.4-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:4f72e5cd0f18f262f5da20efa9e241699e0cf3a766317a17392550c9ad7b37d8"},
    {file = "websockets-10.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:185929b4808b36a79c65b7865783b87b6841e852ef5407a2fb0c03381092fa3b"},
    {file = "websockets-10.4-cp38-cp38-musllinux_1_1_aarch64.whl", hash = "sha256:7d27a7e34c313b3a7f91adcd05134315002aaf8540d7b4f90336beafaea6217c"},
    {file = "websockets-10.4-cp38-cp38-musllinux_1_1_i686.whl", hash = "sha256:884be66c76a444c59f801ac13f40c76f176f1bfa815ef5b8ed44321e74f1600b"},
    {file = "websockets-10.4-cp38-cp38-musllinux_1_1_x86_64.whl", hash = "sha256:931c039af54fc195fe6ad536fde4b0de04da9d5916e78e55405436348cfb0e56"},
    {file = "websockets-10.4-cp38-cp38-win32.whl", hash = "sha256:db3c336f9eda2532ec0fd8ea49fef7a8df8f6c804cdf4f39e5c5c0d4a4ad9a7a"},
    {file = "websockets-10.4-cp38-cp38-win_amd64.whl", hash = "sha256:48c08473563323f9c9debac781ecf66f94ad5a3680a38fe84dee5388cf5acaf6"},
    {file = "websockets-10.4-cp39-cp39-macosx_10_9_universal2.whl", hash = "sha256:40e826de3085721dabc7cf9bfd41682dadc02286d8cf149b3ad05bff89311e4f"},
    {file = "websockets-10.4-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:56029457f219ade1f2fc12a6504ea61e14ee227a815531f9738e41203a429112"},
    {file = "websockets-10.4-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:f5fc088b7a32f244c519a048c170f14cf2251b849ef0e20cbbb0fdf0fdaf556f"},
    {file = "websockets-10.4-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:2fc8709c00704194213d45e455adc106ff9e87658297f72d544220e32029cd3d"},
    {file = "websockets-10.4-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:0154f7691e4fe6c2b2bc275b5701e8b158dae92a1ab229e2b940efe11905dff4"},
    {file = "websockets-10.4-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:4c6d2264f485f0b53adf22697ac11e261ce84805c232ed5dbe6b1bcb84b00ff0"},
    {file = "websockets-10.4-cp39-cp39-musllinux_1_1_aarch64.whl", hash = "sha256:9bc42e8402dc5e9905fb8b9649f57efcb2056693b7e88faa8fb029256ba9c68c"},
    {file = "websockets-10.4-cp39-cp39-musllinux_1_1_i686.whl", hash = "sha256:edc344de4dac1d89300a053ac973299e82d3db56330f3494905643bb68801269"},
    {file = "websockets-10.4-cp39-cp39-musllinux_1_1_x86_64.whl", hash = "sha256:84bc2a7d075f32f6ed98652db3a680a17a4edb21ca7f80fe42e38753a58ee02b"},
    {file = "websockets-10.4-cp39-cp39-win32.whl", hash = "sha256:c94ae4faf2d09f7c81847c63843f84fe47bf6253c9d60b20f25edfd30fb12588"},
    {file = "websockets-10.4-cp39-cp39-win_amd64.whl", hash = "sha256:bbccd847aa0c3a69b5f691a84d2341a4f8a629c6922558f2a70611305f902d74"},
    {file = "websockets-10.4-pp37-pypy37_pp73-macosx_10_9_x86_64.whl", hash = "sha256:82ff5e1cae4e855147fd57a2863376ed7454134c2bf49ec604dfe71e446e2193"},
    {file = "websockets-10.4-pp37-pypy37_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d210abe51b5da0ffdbf7b43eed0cfdff8a55a1ab17abbec4301c9ff077dd0342"},
    {file = "websockets-10.4-pp37-pypy37_pp73-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:942de28af58f352a6f588bc72490ae0f4ccd6dfc2bd3de5945b882a078e4e179"},
    {file = "websockets-10.4-pp37-pypy37_pp73-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:c9b27d6c1c6cd53dc93614967e9ce00ae7f864a2d9f99fe5ed86706e1ecbf485"},
    {file = "websockets-10.4-pp37-pypy37_pp73-win_amd64.whl", hash = "sha256:3d3cac3e32b2c8414f4f87c1b2ab686fa6284a980ba283617404377cd448f631"},
    {file = "websockets-10.4-pp38-pypy38_pp73-macosx_10_9_x86_64.whl", hash = "sha256:da39dd03d130162deb63da51f6e66ed73032ae62e74aaccc4236e30edccddbb0"},
    {file = "websockets-10.4-pp38-pypy38_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:389f8dbb5c489e305fb113ca1b6bdcdaa130923f77485db5b189de343a179393"},
    {file = "websockets-10.4-pp38-pypy38_pp73-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:09a1814bb15eff7069e51fed0826df0bc0702652b5cb8f87697d469d79c23576"},
    {file = "websockets-10.4-pp38-pypy38_pp73-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:ff64a1d38d156d429404aaa84b27305e957fd10c30e5880d1765c9480bea490f"},
    {file = "websockets-10.4-pp38-pypy38_pp73-win_amd64.whl", hash = "sha256:b343f521b047493dc4022dd338fc6db9d9282658862756b4f6fd0e996c1380e1"},
    {file = "websockets-10.4-pp39-pypy39_pp73-macosx_10_9_x86_64.whl", hash = "sha256:932af322458da7e4e35df32f050389e13d3d96b09d274b22a7aa1808f292fee4"},
    {file = "websockets-10.4-pp39-pypy39_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d6a4162139374a49eb18ef5b2f4da1dd95c994588f5033d64e0bbfda4b6b6fcf"},
    {file = "websockets-10.4-pp39-pypy39_pp73-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:c57e4c1349fbe0e446c9fa7b19ed2f8a4417233b6984277cce392819123142d3"},
    {file = "websockets-10.4-pp39-pypy39_pp73-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:b627c266f295de9dea86bd1112ed3d5fafb69a348af30a2422e16590a8ecba13"},
    {file = "websockets-10.4-pp39-pypy39_pp73-win_amd64.whl", hash = "sha256:05a7233089f8bd355e8cbe127c2e8ca0b4ea55467861906b80d2ebc7db4d6b72"},
    {file = "websockets-10.4.tar.gz", hash = "sha256:eef610b23933c54d5d921c92578ae5f89813438fded840c2e9809d378dc765d3"},
]

[[package]]
name = "wheel"
version = "0.40.0"
description = "A built-package format for Python"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "wheel-0.40.0-py3-none-any.whl", hash = "sha256:d236b20e7cb522daf2390fa84c55eea81c5c30190f90f29ae2ca1ad8355bf247"},
    {file = "wheel-0.40.0.tar.gz", hash = "sha256:cd1196f3faee2b31968d626e1731c94f99cbdb67cf5a46e4f5656cbee7738873"},
]

[package.extras]
test = ["pytest (>=6.0.0)"]

[[package]]
name = "win32-setctime"
version = "1.1.0"
description = "A small Python utility to set file creation time on Windows"
category = "main"
optional = false
python-versions = ">=3.5"
files = [
    {file = "win32_setctime-1.1.0-py3-none-any.whl", hash = "sha256:231db239e959c2fe7eb1d7dc129f11172354f98361c4fa2d6d2d7e278baa8aad"},
    {file = "win32_setctime-1.1.0.tar.gz", hash = "sha256:15cf5750465118d6929ae4de4eb46e8edae9a5634350c01ba582df868e932cb2"},
]

[package.extras]
dev = ["black (>=19.3b0)", "pytest (>=4.6.2)"]

[[package]]
name = "xlsxwriter"
version = "3.1.2"
description = "A Python module for creating Excel XLSX files."
category = "main"
optional = false
python-versions = ">=3.6"
files = [
    {file = "XlsxWriter-3.1.2-py3-none-any.whl", hash = "sha256:331508ff39d610ecdaf979e458840bc1eab6e6a02cfd5d08f044f0f73636236f"},
    {file = "XlsxWriter-3.1.2.tar.gz", hash = "sha256:78751099a770273f1c98b8d6643351f68f98ae8e6acf9d09d37dc6798f8cd3de"},
]

[[package]]
name = "yarl"
version = "1.9.2"
description = "Yet another URL library"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "yarl-1.9.2-cp310-cp310-macosx_10_9_universal2.whl", hash = "sha256:8c2ad583743d16ddbdf6bb14b5cd76bf43b0d0006e918809d5d4ddf7bde8dd82"},
    {file = "yarl-1.9.2-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:82aa6264b36c50acfb2424ad5ca537a2060ab6de158a5bd2a72a032cc75b9eb8"},
    {file = "yarl-1.9.2-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:c0c77533b5ed4bcc38e943178ccae29b9bcf48ffd1063f5821192f23a1bd27b9"},
    {file = "yarl-1.9.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:ee4afac41415d52d53a9833ebae7e32b344be72835bbb589018c9e938045a560"},
    {file = "yarl-1.9.2-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:9bf345c3a4f5ba7f766430f97f9cc1320786f19584acc7086491f45524a551ac"},
    {file = "yarl-1.9.2-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:2a96c19c52ff442a808c105901d0bdfd2e28575b3d5f82e2f5fd67e20dc5f4ea"},
    {file = "yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:891c0e3ec5ec881541f6c5113d8df0315ce5440e244a716b95f2525b7b9f3608"},
    {file = "yarl-1.9.2-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:c3a53ba34a636a256d767c086ceb111358876e1fb6b50dfc4d3f4951d40133d5"},
    {file = "yarl-1.9.2-cp310-cp310-musllinux_1_1_aarch64.whl", hash = "sha256:566185e8ebc0898b11f8026447eacd02e46226716229cea8db37496c8cdd26e0"},
    {file = "yarl-1.9.2-cp310-cp310-musllinux_1_1_i686.whl", hash = "sha256:2b0738fb871812722a0ac2154be1f049c6223b9f6f22eec352996b69775b36d4"},
    {file = "yarl-1.9.2-cp310-cp310-musllinux_1_1_ppc64le.whl", hash = "sha256:32f1d071b3f362c80f1a7d322bfd7b2d11e33d2adf395cc1dd4df36c9c243095"},
    {file = "yarl-1.9.2-cp310-cp310-musllinux_1_1_s390x.whl", hash = "sha256:e9fdc7ac0d42bc3ea78818557fab03af6181e076a2944f43c38684b4b6bed8e3"},
    {file = "yarl-1.9.2-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:56ff08ab5df8429901ebdc5d15941b59f6253393cb5da07b4170beefcf1b2528"},
    {file = "yarl-1.9.2-cp310-cp310-win32.whl", hash = "sha256:8ea48e0a2f931064469bdabca50c2f578b565fc446f302a79ba6cc0ee7f384d3"},
    {file = "yarl-1.9.2-cp310-cp310-win_amd64.whl", hash = "sha256:50f33040f3836e912ed16d212f6cc1efb3231a8a60526a407aeb66c1c1956dde"},
    {file = "yarl-1.9.2-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:646d663eb2232d7909e6601f1a9107e66f9791f290a1b3dc7057818fe44fc2b6"},
    {file = "yarl-1.9.2-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:aff634b15beff8902d1f918012fc2a42e0dbae6f469fce134c8a0dc51ca423bb"},
    {file = "yarl-1.9.2-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:a83503934c6273806aed765035716216cc9ab4e0364f7f066227e1aaea90b8d0"},
    {file = "yarl-1.9.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:b25322201585c69abc7b0e89e72790469f7dad90d26754717f3310bfe30331c2"},
    {file = "yarl-1.9.2-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:22a94666751778629f1ec4280b08eb11815783c63f52092a5953faf73be24191"},
    {file = "yarl-1.9.2-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:8ec53a0ea2a80c5cd1ab397925f94bff59222aa3cf9c6da938ce05c9ec20428d"},
    {file = "yarl-1.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:159d81f22d7a43e6eabc36d7194cb53f2f15f498dbbfa8edc8a3239350f59fe7"},
    {file = "yarl-1.9.2-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:832b7e711027c114d79dffb92576acd1bd2decc467dec60e1cac96912602d0e6"},
    {file = "yarl-1.9.2-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:95d2ecefbcf4e744ea952d073c6922e72ee650ffc79028eb1e320e732898d7e8"},
    {file = "yarl-1.9.2-cp311-cp311-musllinux_1_1_i686.whl", hash = "sha256:d4e2c6d555e77b37288eaf45b8f60f0737c9efa3452c6c44626a5455aeb250b9"},
    {file = "yarl-1.9.2-cp311-cp311-musllinux_1_1_ppc64le.whl", hash = "sha256:783185c75c12a017cc345015ea359cc801c3b29a2966c2655cd12b233bf5a2be"},
    {file = "yarl-1.9.2-cp311-cp311-musllinux_1_1_s390x.whl", hash = "sha256:b8cc1863402472f16c600e3e93d542b7e7542a540f95c30afd472e8e549fc3f7"},
    {file = "yarl-1.9.2-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:822b30a0f22e588b32d3120f6d41e4ed021806418b4c9f0bc3048b8c8cb3f92a"},
    {file = "yarl-1.9.2-cp311-cp311-win32.whl", hash = "sha256:a60347f234c2212a9f0361955007fcf4033a75bf600a33c88a0a8e91af77c0e8"},
    {file = "yarl-1.9.2-cp311-cp311-win_amd64.whl", hash = "sha256:be6b3fdec5c62f2a67cb3f8c6dbf56bbf3f61c0f046f84645cd1ca73532ea051"},
    {file = "yarl-1.9.2-cp37-cp37m-macosx_10_9_x86_64.whl", hash = "sha256:38a3928ae37558bc1b559f67410df446d1fbfa87318b124bf5032c31e3447b74"},
    {file = "yarl-1.9.2-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:ac9bb4c5ce3975aeac288cfcb5061ce60e0d14d92209e780c93954076c7c4367"},
    {file = "yarl-1.9.2-cp37-cp37m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:3da8a678ca8b96c8606bbb8bfacd99a12ad5dd288bc6f7979baddd62f71c63ef"},
    {file = "yarl-1.9.2-cp37-cp37m-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:13414591ff516e04fcdee8dc051c13fd3db13b673c7a4cb1350e6b2ad9639ad3"},
    {file = "yarl-1.9.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:bf74d08542c3a9ea97bb8f343d4fcbd4d8f91bba5ec9d5d7f792dbe727f88938"},
    {file = "yarl-1.9.2-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:6e7221580dc1db478464cfeef9b03b95c5852cc22894e418562997df0d074ccc"},
    {file = "yarl-1.9.2-cp37-cp37m-musllinux_1_1_aarch64.whl", hash = "sha256:494053246b119b041960ddcd20fd76224149cfea8ed8777b687358727911dd33"},
    {file = "yarl-1.9.2-cp37-cp37m-musllinux_1_1_i686.whl", hash = "sha256:52a25809fcbecfc63ac9ba0c0fb586f90837f5425edfd1ec9f3372b119585e45"},
    {file = "yarl-1.9.2-cp37-cp37m-musllinux_1_1_ppc64le.whl", hash = "sha256:e65610c5792870d45d7b68c677681376fcf9cc1c289f23e8e8b39c1485384185"},
    {file = "yarl-1.9.2-cp37-cp37m-musllinux_1_1_s390x.whl", hash = "sha256:1b1bba902cba32cdec51fca038fd53f8beee88b77efc373968d1ed021024cc04"},
    {file = "yarl-1.9.2-cp37-cp37m-musllinux_1_1_x86_64.whl", hash = "sha256:662e6016409828ee910f5d9602a2729a8a57d74b163c89a837de3fea050c7582"},
    {file = "yarl-1.9.2-cp37-cp37m-win32.whl", hash = "sha256:f364d3480bffd3aa566e886587eaca7c8c04d74f6e8933f3f2c996b7f09bee1b"},
    {file = "yarl-1.9.2-cp37-cp37m-win_amd64.whl", hash = "sha256:6a5883464143ab3ae9ba68daae8e7c5c95b969462bbe42e2464d60e7e2698368"},
    {file = "yarl-1.9.2-cp38-cp38-macosx_10_9_universal2.whl", hash = "sha256:5610f80cf43b6202e2c33ba3ec2ee0a2884f8f423c8f4f62906731d876ef4fac"},
    {file = "yarl-1.9.2-cp38-cp38-macosx_10_9_x86_64.whl", hash = "sha256:b9a4e67ad7b646cd6f0938c7ebfd60e481b7410f574c560e455e938d2da8e0f4"},
    {file = "yarl-1.9.2-cp38-cp38-macosx_11_0_arm64.whl", hash = "sha256:83fcc480d7549ccebe9415d96d9263e2d4226798c37ebd18c930fce43dfb9574"},
    {file = "yarl-1.9.2-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:5fcd436ea16fee7d4207c045b1e340020e58a2597301cfbcfdbe5abd2356c2fb"},
    {file = "yarl-1.9.2-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:84e0b1599334b1e1478db01b756e55937d4614f8654311eb26012091be109d59"},
    {file = "yarl-1.9.2-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:3458a24e4ea3fd8930e934c129b676c27452e4ebda80fbe47b56d8c6c7a63a9e"},
    {file = "yarl-1.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:838162460b3a08987546e881a2bfa573960bb559dfa739e7800ceeec92e64417"},
    {file = "yarl-1.9.2-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:f4e2d08f07a3d7d3e12549052eb5ad3eab1c349c53ac51c209a0e5991bbada78"},
    {file = "yarl-1.9.2-cp38-cp38-musllinux_1_1_aarch64.whl", hash = "sha256:de119f56f3c5f0e2fb4dee508531a32b069a5f2c6e827b272d1e0ff5ac040333"},
    {file = "yarl-1.9.2-cp38-cp38-musllinux_1_1_i686.whl", hash = "sha256:149ddea5abf329752ea5051b61bd6c1d979e13fbf122d3a1f9f0c8be6cb6f63c"},
    {file = "yarl-1.9.2-cp38-cp38-musllinux_1_1_ppc64le.whl", hash = "sha256:674ca19cbee4a82c9f54e0d1eee28116e63bc6fd1e96c43031d11cbab8b2afd5"},
    {file = "yarl-1.9.2-cp38-cp38-musllinux_1_1_s390x.whl", hash = "sha256:9b3152f2f5677b997ae6c804b73da05a39daa6a9e85a512e0e6823d81cdad7cc"},
    {file = "yarl-1.9.2-cp38-cp38-musllinux_1_1_x86_64.whl", hash = "sha256:5415d5a4b080dc9612b1b63cba008db84e908b95848369aa1da3686ae27b6d2b"},
    {file = "yarl-1.9.2-cp38-cp38-win32.whl", hash = "sha256:f7a3d8146575e08c29ed1cd287068e6d02f1c7bdff8970db96683b9591b86ee7"},
    {file = "yarl-1.9.2-cp38-cp38-win_amd64.whl", hash = "sha256:63c48f6cef34e6319a74c727376e95626f84ea091f92c0250a98e53e62c77c72"},
    {file = "yarl-1.9.2-cp39-cp39-macosx_10_9_universal2.whl", hash = "sha256:75df5ef94c3fdc393c6b19d80e6ef1ecc9ae2f4263c09cacb178d871c02a5ba9"},
    {file = "yarl-1.9.2-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:c027a6e96ef77d401d8d5a5c8d6bc478e8042f1e448272e8d9752cb0aff8b5c8"},
    {file = "yarl-1.9.2-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:f3b078dbe227f79be488ffcfc7a9edb3409d018e0952cf13f15fd6512847f3f7"},
    {file = "yarl-1.9.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:59723a029760079b7d991a401386390c4be5bfec1e7dd83e25a6a0881859e716"},
    {file = "yarl-1.9.2-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:b03917871bf859a81ccb180c9a2e6c1e04d2f6a51d953e6a5cdd70c93d4e5a2a"},
    {file = "yarl-1.9.2-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:c1012fa63eb6c032f3ce5d2171c267992ae0c00b9e164efe4d73db818465fac3"},
    {file = "yarl-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:a74dcbfe780e62f4b5a062714576f16c2f3493a0394e555ab141bf0d746bb955"},
    {file = "yarl-1.9.2-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:8c56986609b057b4839968ba901944af91b8e92f1725d1a2d77cbac6972b9ed1"},
    {file = "yarl-1.9.2-cp39-cp39-musllinux_1_1_aarch64.whl", hash = "sha256:2c315df3293cd521033533d242d15eab26583360b58f7ee5d9565f15fee1bef4"},
    {file = "yarl-1.9.2-cp39-cp39-musllinux_1_1_i686.whl", hash = "sha256:b7232f8dfbd225d57340e441d8caf8652a6acd06b389ea2d3222b8bc89cbfca6"},
    {file = "yarl-1.9.2-cp39-cp39-musllinux_1_1_ppc64le.whl", hash = "sha256:53338749febd28935d55b41bf0bcc79d634881195a39f6b2f767870b72514caf"},
    {file = "yarl-1.9.2-cp39-cp39-musllinux_1_1_s390x.whl", hash = "sha256:066c163aec9d3d073dc9ffe5dd3ad05069bcb03fcaab8d221290ba99f9f69ee3"},
    {file = "yarl-1.9.2-cp39-cp39-musllinux_1_1_x86_64.whl", hash = "sha256:8288d7cd28f8119b07dd49b7230d6b4562f9b61ee9a4ab02221060d21136be80"},
    {file = "yarl-1.9.2-cp39-cp39-win32.whl", hash = "sha256:b124e2a6d223b65ba8768d5706d103280914d61f5cae3afbc50fc3dfcc016623"},
    {file = "yarl-1.9.2-cp39-cp39-win_amd64.whl", hash = "sha256:61016e7d582bc46a5378ffdd02cd0314fb8ba52f40f9cf4d9a5e7dbef88dee18"},
    {file = "yarl-1.9.2.tar.gz", hash = "sha256:04ab9d4b9f587c06d801c2abfe9317b77cdf996c65a90d5e84ecc45010823571"},
]

[package.dependencies]
idna = ">=2.0"
multidict = ">=4.0"

[[package]]
name = "zipp"
version = "3.15.0"
description = "Backport of pathlib-compatible object wrapper for zip files"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "zipp-3.15.0-py3-none-any.whl", hash = "sha256:48904fc76a60e542af151aded95726c1a5c34ed43ab4134b597665c86d7ad556"},
    {file = "zipp-3.15.0.tar.gz", hash = "sha256:112929ad649da941c23de50f356a2b5570c954b65150642bccdd66bf194d224b"},
]

[package.extras]
docs = ["furo", "jaraco.packaging (>=9)", "jaraco.tidelift (>=1.4)", "rst.linker (>=1.9)", "sphinx (>=3.5)", "sphinx-lint"]
testing = ["big-O", "flake8 (<5)", "jaraco.functools", "jaraco.itertools", "more-itertools", "pytest (>=6)", "pytest-black (>=0.3.7)", "pytest-checkdocs (>=2.4)", "pytest-cov", "pytest-enabler (>=1.3)", "pytest-flake8", "pytest-mypy (>=0.9.1)"]

[[package]]
name = "zstandard"
version = "0.21.0"
description = "Zstandard bindings for Python"
category = "main"
optional = false
python-versions = ">=3.7"
files = [
    {file = "zstandard-0.21.0-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:649a67643257e3b2cff1c0a73130609679a5673bf389564bc6d4b164d822a7ce"},
    {file = "zstandard-0.21.0-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:144a4fe4be2e747bf9c646deab212666e39048faa4372abb6a250dab0f347a29"},
    {file = "zstandard-0.21.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:b72060402524ab91e075881f6b6b3f37ab715663313030d0ce983da44960a86f"},
    {file = "zstandard-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8257752b97134477fb4e413529edaa04fc0457361d304c1319573de00ba796b1"},
    {file = "zstandard-0.21.0-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_12_i686.manylinux2010_i686.whl", hash = "sha256:c053b7c4cbf71cc26808ed67ae955836232f7638444d709bfc302d3e499364fa"},
    {file = "zstandard-0.21.0-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:2769730c13638e08b7a983b32cb67775650024632cd0476bf1ba0e6360f5ac7d"},
    {file = "zstandard-0.21.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl", hash = "sha256:7d3bc4de588b987f3934ca79140e226785d7b5e47e31756761e48644a45a6766"},
    {file = "zstandard-0.21.0-cp310-cp310-win32.whl", hash = "sha256:67829fdb82e7393ca68e543894cd0581a79243cc4ec74a836c305c70a5943f07"},
    {file = "zstandard-0.21.0-cp310-cp310-win_amd64.whl", hash = "sha256:e6048a287f8d2d6e8bc67f6b42a766c61923641dd4022b7fd3f7439e17ba5a4d"},
    {file = "zstandard-0.21.0-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:7f2afab2c727b6a3d466faee6974a7dad0d9991241c498e7317e5ccf53dbc766"},
    {file = "zstandard-0.21.0-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:ff0852da2abe86326b20abae912d0367878dd0854b8931897d44cfeb18985472"},
    {file = "zstandard-0.21.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d12fa383e315b62630bd407477d750ec96a0f438447d0e6e496ab67b8b451d39"},
    {file = "zstandard-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:f1b9703fe2e6b6811886c44052647df7c37478af1b4a1a9078585806f42e5b15"},
    {file = "zstandard-0.21.0-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:df28aa5c241f59a7ab524f8ad8bb75d9a23f7ed9d501b0fed6d40ec3064784e8"},
    {file = "zstandard-0.21.0-cp311-cp311-win32.whl", hash = "sha256:0aad6090ac164a9d237d096c8af241b8dcd015524ac6dbec1330092dba151657"},
    {file = "zstandard-0.21.0-cp311-cp311-win_amd64.whl", hash = "sha256:48b6233b5c4cacb7afb0ee6b4f91820afbb6c0e3ae0fa10abbc20000acdf4f11"},
    {file = "zstandard-0.21.0-cp37-cp37m-macosx_10_9_x86_64.whl", hash = "sha256:e7d560ce14fd209db6adacce8908244503a009c6c39eee0c10f138996cd66d3e"},
    {file = "zstandard-0.21.0-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:1e6e131a4df2eb6f64961cea6f979cdff22d6e0d5516feb0d09492c8fd36f3bc"},
    {file = "zstandard-0.21.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:e1e0c62a67ff425927898cf43da2cf6b852289ebcc2054514ea9bf121bec10a5"},
    {file = "zstandard-0.21.0-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_12_i686.manylinux2010_i686.whl", hash = "sha256:1545fb9cb93e043351d0cb2ee73fa0ab32e61298968667bb924aac166278c3fc"},
    {file = "zstandard-0.21.0-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:fe6c821eb6870f81d73bf10e5deed80edcac1e63fbc40610e61f340723fd5f7c"},
    {file = "zstandard-0.21.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl", hash = "sha256:ddb086ea3b915e50f6604be93f4f64f168d3fc3cef3585bb9a375d5834392d4f"},
    {file = "zstandard-0.21.0-cp37-cp37m-win32.whl", hash = "sha256:57ac078ad7333c9db7a74804684099c4c77f98971c151cee18d17a12649bc25c"},
    {file = "zstandard-0.21.0-cp37-cp37m-win_amd64.whl", hash = "sha256:1243b01fb7926a5a0417120c57d4c28b25a0200284af0525fddba812d575f605"},
    {file = "zstandard-0.21.0-cp38-cp38-macosx_10_9_x86_64.whl", hash = "sha256:ea68b1ba4f9678ac3d3e370d96442a6332d431e5050223626bdce748692226ea"},
    {file = "zstandard-0.21.0-cp38-cp38-macosx_11_0_arm64.whl", hash = "sha256:8070c1cdb4587a8aa038638acda3bd97c43c59e1e31705f2766d5576b329e97c"},
    {file = "zstandard-0.21.0-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:4af612c96599b17e4930fe58bffd6514e6c25509d120f4eae6031b7595912f85"},
    {file = "zstandard-0.21.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:cff891e37b167bc477f35562cda1248acc115dbafbea4f3af54ec70821090965"},
    {file = "zstandard-0.21.0-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_12_i686.manylinux2010_i686.whl", hash = "sha256:a9fec02ce2b38e8b2e86079ff0b912445495e8ab0b137f9c0505f88ad0d61296"},
    {file = "zstandard-0.21.0-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:0bdbe350691dec3078b187b8304e6a9c4d9db3eb2d50ab5b1d748533e746d099"},
    {file = "zstandard-0.21.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl", hash = "sha256:b69cccd06a4a0a1d9fb3ec9a97600055cf03030ed7048d4bcb88c574f7895773"},
    {file = "zstandard-0.21.0-cp38-cp38-win32.whl", hash = "sha256:9980489f066a391c5572bc7dc471e903fb134e0b0001ea9b1d3eff85af0a6f1b"},
    {file = "zstandard-0.21.0-cp38-cp38-win_amd64.whl", hash = "sha256:0e1e94a9d9e35dc04bf90055e914077c80b1e0c15454cc5419e82529d3e70728"},
    {file = "zstandard-0.21.0-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:d2d61675b2a73edcef5e327e38eb62bdfc89009960f0e3991eae5cc3d54718de"},
    {file = "zstandard-0.21.0-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:25fbfef672ad798afab12e8fd204d122fca3bc8e2dcb0a2ba73bf0a0ac0f5f07"},
    {file = "zstandard-0.21.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:62957069a7c2626ae80023998757e27bd28d933b165c487ab6f83ad3337f773d"},
    {file = "zstandard-0.21.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:14e10ed461e4807471075d4b7a2af51f5234c8f1e2a0c1d37d5ca49aaaad49e8"},
    {file = "zstandard-0.21.0-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_12_i686.manylinux2010_i686.whl", hash = "sha256:9cff89a036c639a6a9299bf19e16bfb9ac7def9a7634c52c257166db09d950e7"},
    {file = "zstandard-0.21.0-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:52b2b5e3e7670bd25835e0e0730a236f2b0df87672d99d3bf4bf87248aa659fb"},
    {file = "zstandard-0.21.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl", hash = "sha256:b1367da0dde8ae5040ef0413fb57b5baeac39d8931c70536d5f013b11d3fc3a5"},
    {file = "zstandard-0.21.0-cp39-cp39-win32.whl", hash = "sha256:db62cbe7a965e68ad2217a056107cc43d41764c66c895be05cf9c8b19578ce9c"},
    {file = "zstandard-0.21.0-cp39-cp39-win_amd64.whl", hash = "sha256:a8d200617d5c876221304b0e3fe43307adde291b4a897e7b0617a61611dfff6a"},
    {file = "zstandard-0.21.0.tar.gz", hash = "sha256:f08e3a10d01a247877e4cb61a82a319ea746c356a3786558bed2481e6c405546"},
]

[package.dependencies]
cffi = {version = ">=1.11", markers = "platform_python_implementation == \"PyPy\""}

[package.extras]
cffi = ["cffi (>=1.11)"]

[extras]
postgresql = ["psycopg2cffi"]

[metadata]
lock-version = "2.0"
python-versions = "^3.10"
content-hash = "39179f3602509004d328d1fb7a48068c86f8e93ab0660cf18c7d7f85018cacf7"



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/pyproject.toml
================================================
[tool.poetry]
name = "chatgpt-retrieval-plugin"
version = "0.1.0"
description = ""
authors = ["isafulf <isabella@openai.com>"]
readme = "README.md"
packages = [{include = "server"}]

[tool.poetry.dependencies]
python = "^3.10"
fastapi = "^0.92.0"
uvicorn = "^0.20.0"
openai = "^0.27.5"
python-dotenv = "^0.21.1"
pydantic = "^1.10.5"
tenacity = "^8.2.1"
tiktoken = "^0.2.0"
numpy = "^1.24.2"
docx2txt = "^0.8"
PyPDF2 = "^3.0.1"
python-pptx = "^0.6.21"
python-multipart = "^0.0.6"
arrow = "^1.2.3"
chromadb = "^0.3.25"
pinecone-client = "^2.1.0"
weaviate-client = "^3.12.0"
pymilvus = "^2.2.2"
qdrant-client = {version = "^1.0.4", python = "<3.12"}
redis = "4.5.4"
supabase = "^1.0.2"
psycopg2 = "^2.9.5"
llama-index = "0.5.4"
azure-identity = "^1.12.0"
azure-search-documents = "11.4.0b8"
pgvector = "^0.1.7"
psycopg2cffi = {version = "^2.9.0", optional = true}
loguru = "^0.7.0"
elasticsearch = "8.8.2"

[tool.poetry.scripts]
start = "server.main:start"
dev = "local_server.main:start"

[tool.poetry.extras]
postgresql = ["psycopg2cffi"]

[tool.poetry.group.dev.dependencies]
httpx = "^0.23.3"
pytest = "^7.2.1"
pytest-cov = "^4.0.0"
pytest-asyncio = "^0.20.3"

[build-system]
requires = ["poetry-core>=1.0.0"]
build-backend = "poetry.core.masonry.api"

[tool.pytest.ini_options]
pythonpath = [
  "."
]
asyncio_mode="auto"



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/.dockerignore
================================================
# Ignore files that are already ignored by git
.gitignore

scripts/
tests/
examples/
local_server/
*.md
*.pyc
.dockerignore
Dockerfile



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/.gitignore
================================================
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
pip-wheel-metadata/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# .vscode files
.vscode/*

# Pycharm
.idea/

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
.python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# PEP 582; used by e.g. github.com/David-OConnor/pyflow
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/
myvenv/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# macOS .DS_Store files
.DS_Store


================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/datastore/__init__.py
================================================



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/datastore/datastore.py
================================================
from abc import ABC, abstractmethod
from typing import Dict, List, Optional
import asyncio

from models.models import (
    Document,
    DocumentChunk,
    DocumentMetadataFilter,
    Query,
    QueryResult,
    QueryWithEmbedding,
)
from services.chunks import get_document_chunks
from services.openai import get_embeddings


class DataStore(ABC):
    async def upsert(
        self, documents: List[Document], chunk_token_size: Optional[int] = None
    ) -> List[str]:
        """
        Takes in a list of documents and inserts them into the database.
        First deletes all the existing vectors with the document id (if necessary, depends on the vector db), then inserts the new ones.
        Return a list of document ids.
        """
        # Delete any existing vectors for documents with the input document ids
        await asyncio.gather(
            *[
                self.delete(
                    filter=DocumentMetadataFilter(
                        document_id=document.id,
                    ),
                    delete_all=False,
                )
                for document in documents
                if document.id
            ]
        )

        chunks = get_document_chunks(documents, chunk_token_size)

        return await self._upsert(chunks)

    @abstractmethod
    async def _upsert(self, chunks: Dict[str, List[DocumentChunk]]) -> List[str]:
        """
        Takes in a list of list of document chunks and inserts them into the database.
        Return a list of document ids.
        """

        raise NotImplementedError

    async def query(self, queries: List[Query]) -> List[QueryResult]:
        """
        Takes in a list of queries and filters and returns a list of query results with matching document chunks and scores.
        """
        # get a list of of just the queries from the Query list
        query_texts = [query.query for query in queries]
        query_embeddings = get_embeddings(query_texts)
        # hydrate the queries with embeddings
        queries_with_embeddings = [
            QueryWithEmbedding(**query.dict(), embedding=embedding)
            for query, embedding in zip(queries, query_embeddings)
        ]
        return await self._query(queries_with_embeddings)

    @abstractmethod
    async def _query(self, queries: List[QueryWithEmbedding]) -> List[QueryResult]:
        """
        Takes in a list of queries with embeddings and filters and returns a list of query results with matching document chunks and scores.
        """
        raise NotImplementedError

    @abstractmethod
    async def delete(
        self,
        ids: Optional[List[str]] = None,
        filter: Optional[DocumentMetadataFilter] = None,
        delete_all: Optional[bool] = None,
    ) -> bool:
        """
        Removes vectors by ids, filter, or everything in the datastore.
        Multiple parameters can be used at once.
        Returns whether the operation was successful.
        """
        raise NotImplementedError



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/datastore/factory.py
================================================
from datastore.datastore import DataStore
import os


async def get_datastore() -> DataStore:
    datastore = os.environ.get("DATASTORE")
    assert datastore is not None

    match datastore:
        case "chroma":
            from datastore.providers.chroma_datastore import ChromaDataStore

            return ChromaDataStore()
        case "llama":
            from datastore.providers.llama_datastore import LlamaDataStore

            return LlamaDataStore()

        case "pinecone":
            from datastore.providers.pinecone_datastore import PineconeDataStore

            return PineconeDataStore()
        case "weaviate":
            from datastore.providers.weaviate_datastore import WeaviateDataStore

            return WeaviateDataStore()
        case "milvus":
            from datastore.providers.milvus_datastore import MilvusDataStore

            return MilvusDataStore()
        case "zilliz":
            from datastore.providers.zilliz_datastore import ZillizDataStore

            return ZillizDataStore()
        case "redis":
            from datastore.providers.redis_datastore import RedisDataStore

            return await RedisDataStore.init()
        case "qdrant":
            from datastore.providers.qdrant_datastore import QdrantDataStore

            return QdrantDataStore()
        case "azuresearch":
            from datastore.providers.azuresearch_datastore import AzureSearchDataStore

            return AzureSearchDataStore()
        case "supabase":
            from datastore.providers.supabase_datastore import SupabaseDataStore

            return SupabaseDataStore()
        case "postgres":
            from datastore.providers.postgres_datastore import PostgresDataStore

            return PostgresDataStore()
        case "analyticdb":
            from datastore.providers.analyticdb_datastore import AnalyticDBDataStore

            return AnalyticDBDataStore()
        case "elasticsearch":
            from datastore.providers.elasticsearch_datastore import (
                ElasticsearchDataStore,
            )

            return ElasticsearchDataStore()
        case _:
            raise ValueError(
                f"Unsupported vector database: {datastore}. "
                f"Try one of the following: llama, elasticsearch, pinecone, weaviate, milvus, zilliz, redis, azuresearch, or qdrant"
            )



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/datastore/providers/__init__.py
================================================



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/datastore/providers/analyticdb_datastore.py
================================================
import os
import asyncio
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime
from loguru import logger

from psycopg2cffi import compat

compat.register()
import psycopg2
from psycopg2.extras import DictCursor
from psycopg2.pool import SimpleConnectionPool

from services.date import to_unix_timestamp
from datastore.datastore import DataStore
from models.models import (
    DocumentChunk,
    DocumentChunkMetadata,
    DocumentMetadataFilter,
    QueryResult,
    QueryWithEmbedding,
    DocumentChunkWithScore,
)

PG_CONFIG = {
    "collection": os.environ.get("PG_COLLECTION", "document_chunks"),
    "database": os.environ.get("PG_DATABASE", "postgres"),
    "user": os.environ.get("PG_USER", "user"),
    "password": os.environ.get("PG_PASSWORD", "password"),
    "host": os.environ.get("PG_HOST", "localhost"),
    "port": int(os.environ.get("PG_PORT", "5432")),
}
OUTPUT_DIM = 1536


class AnalyticDBDataStore(DataStore):
    def __init__(self, config: Dict[str, str] = PG_CONFIG):
        self.collection_name = config["collection"]
        self.user = config["user"]
        self.password = config["password"]
        self.database = config["database"]
        self.host = config["host"]
        self.port = config["port"]

        self.connection_pool = SimpleConnectionPool(
            minconn=1,
            maxconn=100,
            dbname=self.database,
            user=self.user,
            password=self.password,
            host=self.host,
            port=self.port,
        )

        self._initialize_db()

    def _initialize_db(self):
        conn = self.connection_pool.getconn()
        try:
            with conn.cursor() as cur:
                self._create_table(cur)
                self._create_embedding_index(cur)
                conn.commit()
        finally:
            self.connection_pool.putconn(conn)

    def _create_table(self, cur: psycopg2.extensions.cursor):
        cur.execute(
            f"""
              CREATE TABLE IF NOT EXISTS {self.collection_name} (
                id TEXT PRIMARY KEY DEFAULT uuid_generate_v4()::TEXT,
                source TEXT,
                source_id TEXT,
                content TEXT,
                document_id TEXT,
                author TEXT,
                url TEXT,
                created_at TIMESTAMPTZ DEFAULT NOW(),
                embedding real[]
            );
            """
        )

    def _create_embedding_index(self, cur: psycopg2.extensions.cursor):
        cur.execute(
            f"""
            SELECT * FROM pg_indexes WHERE tablename='{self.collection_name}';
            """
        )
        index_exists = any(
            index[2] == f"{self.collection_name}_embedding_idx"
            for index in cur.fetchall()
        )
        if not index_exists:
            cur.execute(
                f"""
                CREATE INDEX {self.collection_name}_embedding_idx
                ON {self.collection_name}
                USING ann(embedding)
                WITH (
                    distancemeasure=L2,
                    dim=OUTPUT_DIM,
                    pq_segments=64,
                    hnsw_m=100,
                    pq_centers=2048
                );
                """
            )

    async def _upsert(self, chunks: Dict[str, List[DocumentChunk]]) -> List[str]:
        """
        Takes in a dict of document_ids to list of document chunks and inserts them into the database.
        Return a list of document ids.
        """
        loop = asyncio.get_event_loop()
        tasks = [
            loop.run_in_executor(None, self._upsert_chunk, chunk)
            for document_chunks in chunks.values()
            for chunk in document_chunks
        ]
        await asyncio.gather(*tasks)

        return list(chunks.keys())

    def _upsert_chunk(self, chunk: DocumentChunk):
        created_at = (
            datetime.fromtimestamp(to_unix_timestamp(chunk.metadata.created_at))
            if chunk.metadata.created_at
            else None
        )
        data = (
            chunk.id,
            chunk.text,
            chunk.embedding,
            chunk.metadata.document_id,
            chunk.metadata.source,
            chunk.metadata.source_id,
            chunk.metadata.url,
            chunk.metadata.author,
            created_at,
        )

        conn = self.connection_pool.getconn()
        try:
            with conn.cursor() as cur:
                # Construct the SQL query and data
                query = f"""
                        INSERT INTO {self.collection_name} (id, content, embedding, document_id, source, source_id, url, author, created_at)
                        VALUES (%s::text, %s::text, %s::real[], %s::text, %s::text, %s::text, %s::text, %s::text, %s::timestamp with time zone)
                        ON CONFLICT (id) DO UPDATE SET
                            content = EXCLUDED.content,
                            embedding = EXCLUDED.embedding,
                            document_id = EXCLUDED.document_id,
                            source = EXCLUDED.source,
                            source_id = EXCLUDED.source_id,
                            url = EXCLUDED.url,
                            author = EXCLUDED.author,
                            created_at = EXCLUDED.created_at;
                """

                # Execute the query
                cur.execute(query, data)

                # Commit the transaction
                conn.commit()
        finally:
            self.connection_pool.putconn(conn)

    async def _query(self, queries: List[QueryWithEmbedding]) -> List[QueryResult]:
        """
        Takes in a list of queries with embeddings and filters and returns a list of query results with matching document chunks and scores.
        """
        query_results: List[QueryResult] = []

        def generate_query(query: QueryWithEmbedding) -> Tuple[str, List[Any]]:
            embedding = "[" + ", ".join(str(x) for x in query.embedding) + "]"
            q = f"""
                SELECT
                    id,
                    content,
                    source,
                    source_id,
                    document_id,
                    url,
                    created_at,
                    author,
                    embedding,
                    l2_distance(embedding,array{embedding}::real[]) AS similarity
                FROM
                    {self.collection_name}
            """
            where_clause, params = generate_where_clause(query.filter)
            q += where_clause
            q += f"ORDER BY embedding <-> array{embedding}::real[] LIMIT {query.top_k};"
            return q, params

        def generate_where_clause(
            query_filter: Optional[DocumentMetadataFilter],
        ) -> Tuple[str, List[Any]]:
            if query_filter is None:
                return "", []

            conditions = [
                ("document_id=%s", query_filter.document_id),
                ("source_id=%s", query_filter.source_id),
                ("source LIKE %s", query_filter.source),
                ("author LIKE %s", query_filter.author),
                ("created_at >= %s", query_filter.start_date),
                ("created_at <= %s", query_filter.end_date),
            ]

            where_clause = "WHERE " + " AND ".join(
                [cond[0] for cond in conditions if cond[1] is not None]
            )

            values = [cond[1] for cond in conditions if cond[1] is not None]

            return where_clause, values

        def fetch_data(cur, q: str, params: List[Any]):
            cur.execute(q, params)
            return cur.fetchall()

        def create_results(data):
            results = []
            for row in data:
                document_chunk = DocumentChunkWithScore(
                    id=row["id"],
                    text=row["content"],
                    score=float(row["similarity"]),
                    metadata=DocumentChunkMetadata(
                        source=row["source"],
                        source_id=row["source_id"],
                        document_id=row["document_id"],
                        url=row["url"],
                        created_at=str(row["created_at"]),
                        author=row["author"],
                    ),
                )
                results.append(document_chunk)
            return results

        conn = self.connection_pool.getconn()
        try:
            for query in queries:
                try:
                    cur = conn.cursor(cursor_factory=DictCursor)
                    for query in queries:
                        q, params = generate_query(query)
                        data = fetch_data(cur, q, params)
                        results = create_results(data)
                        query_results.append(
                            QueryResult(query=query.query, results=results)
                        )
                except Exception as e:
                    logger.error(e)
                    query_results.append(QueryResult(query=query.query, results=[]))
            return query_results
        finally:
            self.connection_pool.putconn(conn)

    async def delete(
        self,
        ids: Optional[List[str]] = None,
        filter: Optional[DocumentMetadataFilter] = None,
        delete_all: Optional[bool] = None,
    ) -> bool:
        async def execute_delete(query: str, params: Optional[List] = None) -> bool:
            conn = self.connection_pool.getconn()
            try:
                with conn.cursor() as cur:
                    if params:
                        cur.execute(query, params)
                    else:
                        cur.execute(query)
                    self.conn.commit()
                return True
            except Exception as e:
                logger.error(e)
                return False
            finally:
                self.connection_pool.putconn(conn)

        if delete_all:
            query = f"DELETE FROM {self.collection_name} WHERE document_id LIKE %s;"
            return await execute_delete(query, ["%"])
        elif ids:
            query = f"DELETE FROM {self.collection_name} WHERE document_id IN ({','.join(['%s'] * len(ids))});"
            return await execute_delete(query, ids)
        elif filter is not None:
            query, params = self._generate_delete_query(filter)
            return await execute_delete(query, params)
        else:
            return True

    def _generate_delete_query(
        self, filter: DocumentMetadataFilter
    ) -> Tuple[str, List]:
        conditions = [
            (filter.document_id, "document_id = %s"),
            (filter.source, "source = %s"),
            (filter.source_id, "source_id = %s"),
            (filter.author, "author = %s"),
            (filter.start_date, "created_at >= %s"),
            (filter.end_date, "created_at <= %s"),
        ]

        where_conditions = [f for value, f in conditions if value]
        where_values = [value for value, _ in conditions if value]

        query = f"DELETE FROM {self.collection_name} WHERE {' AND '.join(where_conditions)};"
        return query, where_values



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/datastore/providers/azuresearch_datastore.py
================================================
import asyncio
import base64
import os
import re
import time
from typing import Dict, List, Optional, Union

from azure.core.credentials import AzureKeyCredential
from azure.identity import DefaultAzureCredential as DefaultAzureCredentialSync
from azure.identity.aio import DefaultAzureCredential
from azure.search.documents.aio import SearchClient
from azure.search.documents.indexes import SearchIndexClient
from azure.search.documents.indexes.models import *
from azure.search.documents.models import QueryType, Vector
from loguru import logger

from datastore.datastore import DataStore
from models.models import (DocumentChunk, DocumentChunkMetadata,
                           DocumentChunkWithScore, DocumentMetadataFilter,
                           Query, QueryResult, QueryWithEmbedding)

AZURESEARCH_SERVICE = os.environ.get("AZURESEARCH_SERVICE")
AZURESEARCH_INDEX = os.environ.get("AZURESEARCH_INDEX")
AZURESEARCH_API_KEY = os.environ.get("AZURESEARCH_API_KEY")
AZURESEARCH_SEMANTIC_CONFIG = os.environ.get("AZURESEARCH_SEMANTIC_CONFIG")
AZURESEARCH_LANGUAGE = os.environ.get("AZURESEARCH_LANGUAGE", "en-us")
AZURESEARCH_DISABLE_HYBRID = os.environ.get("AZURESEARCH_DISABLE_HYBRID")
AZURESEARCH_DIMENSIONS = os.environ.get("AZURESEARCH_DIMENSIONS", 1536) # Default to OpenAI's ada-002 embedding model vector size
assert AZURESEARCH_SERVICE is not None
assert AZURESEARCH_INDEX is not None

# Allow overriding field names for Azure Search
FIELDS_ID = os.environ.get("AZURESEARCH_FIELDS_ID", "id")
FIELDS_TEXT = os.environ.get("AZURESEARCH_FIELDS_TEXT", "text")
FIELDS_EMBEDDING = os.environ.get("AZURESEARCH_FIELDS_EMBEDDING", "embedding")
FIELDS_DOCUMENT_ID = os.environ.get("AZURESEARCH_FIELDS_DOCUMENT_ID", "document_id")
FIELDS_SOURCE = os.environ.get("AZURESEARCH_FIELDS_SOURCE", "source")
FIELDS_SOURCE_ID = os.environ.get("AZURESEARCH_FIELDS_SOURCE_ID", "source_id")
FIELDS_URL = os.environ.get("AZURESEARCH_FIELDS_URL", "url")
FIELDS_CREATED_AT = os.environ.get("AZURESEARCH_FIELDS_CREATED_AT", "created_at")
FIELDS_AUTHOR = os.environ.get("AZURESEARCH_FIELDS_AUTHOR", "author")

MAX_UPLOAD_BATCH_SIZE = 1000
MAX_DELETE_BATCH_SIZE = 1000

class AzureSearchDataStore(DataStore):
    def __init__(self):
        self.client = SearchClient(
            endpoint=f"https://{AZURESEARCH_SERVICE}.search.windows.net",
            index_name=AZURESEARCH_INDEX,
            credential=AzureSearchDataStore._create_credentials(True),
            user_agent="retrievalplugin"
        )

        mgmt_client = SearchIndexClient(
            endpoint=f"https://{AZURESEARCH_SERVICE}.search.windows.net",
            credential=AzureSearchDataStore._create_credentials(False),
            user_agent="retrievalplugin"
        )
        if AZURESEARCH_INDEX not in [name for name in mgmt_client.list_index_names()]:
            self._create_index(mgmt_client)
        else:
            logger.info(f"Using existing index {AZURESEARCH_INDEX} in service {AZURESEARCH_SERVICE}")
    
    async def _upsert(self, chunks: Dict[str, List[DocumentChunk]]) -> List[str]:
        azdocuments: List[Dict] = []

        async def upload():
            r = await self.client.upload_documents(documents=azdocuments)
            count = sum(1 for rr in r if rr.succeeded)
            logger.info(f"Upserted {count} chunks out of {len(azdocuments)}")
            if count < len(azdocuments):
                raise Exception(f"Failed to upload {len(azdocuments) - count} chunks")

        ids = []
        for document_id, document_chunks in chunks.items():
            ids.append(document_id)
            for chunk in document_chunks:
                azdocuments.append({
                    # base64-encode the id string to stay within Azure Search's valid characters for keys
                    FIELDS_ID: base64.urlsafe_b64encode(bytes(chunk.id, "utf-8")).decode("ascii"),
                    FIELDS_TEXT: chunk.text,
                    FIELDS_EMBEDDING: chunk.embedding,
                    FIELDS_DOCUMENT_ID: document_id,
                    FIELDS_SOURCE: chunk.metadata.source,
                    FIELDS_SOURCE_ID: chunk.metadata.source_id,
                    FIELDS_URL: chunk.metadata.url,
                    FIELDS_CREATED_AT: chunk.metadata.created_at,
                    FIELDS_AUTHOR: chunk.metadata.author,
                })
            
                if len(azdocuments) >= MAX_UPLOAD_BATCH_SIZE:
                    await upload()
                    azdocuments = []

        if len(azdocuments) > 0:
            await upload()

        return ids

    async def delete(self, ids: Optional[List[str]] = None, filter: Optional[DocumentMetadataFilter] = None, delete_all: Optional[bool] = None) -> bool:
        filter = None if delete_all else self._translate_filter(filter)
        if delete_all or filter is not None:
            deleted = set()
            while True:
                search_result = await self.client.search(None, filter=filter, top=MAX_DELETE_BATCH_SIZE, include_total_count=True, select=FIELDS_ID)
                if await search_result.get_count() == 0:
                    break
                documents = [{ FIELDS_ID: d[FIELDS_ID] } async for d in search_result if d[FIELDS_ID] not in deleted]
                if len(documents) > 0:
                    logger.info(f"Deleting {len(documents)} chunks " + ("using a filter" if filter is not None else "using delete_all"))
                    del_result = await self.client.delete_documents(documents=documents)
                    if not all([rr.succeeded for rr in del_result]):
                        raise Exception("Failed to delete documents")
                    deleted.update([d[FIELDS_ID] for d in documents])
                else:
                    # All repeats, delay a bit to let the index refresh and try again
                    time.sleep(0.25)
        
        if ids is not None and len(ids) > 0:
            for id in ids:
                logger.info(f"Deleting chunks for document id {id}")
                await self.delete(filter=DocumentMetadataFilter(document_id=id))

        return True

    async def _query(self, queries: List[QueryWithEmbedding]) -> List[QueryResult]:
        """
        Takes in a list of queries with embeddings and filters and returns a list of query results with matching document chunks and scores.
        """
        return await asyncio.gather(*(self._single_query(query) for query in queries))
    
    async def _single_query(self, query: QueryWithEmbedding) -> QueryResult:
        """
        Takes in a single query and filters and returns a query result with matching document chunks and scores.
        """
        filter = self._translate_filter(query.filter) if query.filter is not None else None
        try:
            vector_top_k = query.top_k if filter is None else query.top_k * 2
            if not AZURESEARCH_DISABLE_HYBRID: vector_top_k *= 2
            q = query.query if not AZURESEARCH_DISABLE_HYBRID else None
            vector_q = Vector(value=query.embedding, k=vector_top_k, fields=FIELDS_EMBEDDING)
            if AZURESEARCH_SEMANTIC_CONFIG != None and not AZURESEARCH_DISABLE_HYBRID:
                # Ensure we're feeding a good number of candidates to the L2 reranker
                vector_top_k = max(50, vector_top_k)
                r = await self.client.search(
                        q, 
                        filter=filter, 
                        top=query.top_k, 
                        vectors=[vector_q],
                        query_type=QueryType.SEMANTIC,
                        query_language=AZURESEARCH_LANGUAGE,
                        semantic_configuration_name=AZURESEARCH_SEMANTIC_CONFIG)
            else:
                r = await self.client.search(
                        q, 
                        filter=filter, 
                        top=query.top_k, 
                        vectors=[vector_q])
            results: List[DocumentChunkWithScore] = []
            async for hit in r:
                f = lambda field: hit.get(field) if field != "-" else None
                results.append(DocumentChunkWithScore(
                    id=hit[FIELDS_ID],
                    text=hit[FIELDS_TEXT],
                    metadata=DocumentChunkMetadata(
                        document_id=f(FIELDS_DOCUMENT_ID),
                        source=f(FIELDS_SOURCE) or "file",
                        source_id=f(FIELDS_SOURCE_ID),
                        url=f(FIELDS_URL),
                        created_at=f(FIELDS_CREATED_AT),
                        author=f(FIELDS_AUTHOR)
                    ),
                    score=hit["@search.score"]
                ))
                
            return QueryResult(query=query.query, results=results)
        except Exception as e:
            raise Exception(f"Error querying the index: {e}")

    @staticmethod    
    def _translate_filter(filter: DocumentMetadataFilter) -> str:
        """
        Translates a DocumentMetadataFilter into an Azure Search filter string
        """
        if filter is None:
            return None        
        
        escape = lambda s: s.replace("'", "''")

        # regex to validate dates are in OData format
        date_re = re.compile(r"\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}Z")

        filter_list = []
        if filter.document_id is not None:
            filter_list.append(f"{FIELDS_DOCUMENT_ID} eq '{escape(filter.document_id)}'")
        if filter.source is not None:
            filter_list.append(f"{FIELDS_SOURCE} eq '{escape(filter.source)}'")
        if filter.source_id is not None:
            filter_list.append(f"{FIELDS_SOURCE_ID} eq '{escape(filter.source_id)}'")
        if filter.author is not None:
            filter_list.append(f"{FIELDS_AUTHOR} eq '{escape(filter.author)}'")
        if filter.start_date is not None:
            if not date_re.match(filter.start_date):
                raise ValueError(f"start_date must be in OData format, got {filter.start_date}")
            filter_list.append(f"{FIELDS_CREATED_AT} ge {filter.start_date}")
        if filter.end_date is not None:
            if not date_re.match(filter.end_date):
                raise ValueError(f"end_date must be in OData format, got {filter.end_date}")
            filter_list.append(f"{FIELDS_CREATED_AT} le {filter.end_date}")
        return " and ".join(filter_list) if len(filter_list) > 0 else None
    
    def _create_index(self, mgmt_client: SearchIndexClient):
        """
        Creates an Azure Cognitive Search index, including a semantic search configuration if a name is specified for it
        """
        logger.info(
            f"Creating index {AZURESEARCH_INDEX} in service {AZURESEARCH_SERVICE}" +
            (f" with semantic search configuration {AZURESEARCH_SEMANTIC_CONFIG}" if AZURESEARCH_SEMANTIC_CONFIG is not None else "")
        )
        mgmt_client.create_index(
            SearchIndex(
                name=AZURESEARCH_INDEX,
                fields=[
                    SimpleField(name=FIELDS_ID, type=SearchFieldDataType.String, key=True),
                    SearchableField(name=FIELDS_TEXT, type=SearchFieldDataType.String, analyzer_name="standard.lucene"),
                    SearchField(name=FIELDS_EMBEDDING, type=SearchFieldDataType.Collection(SearchFieldDataType.Single), 
                                hidden=False, searchable=True, filterable=False, sortable=False, facetable=False,
                                vector_search_dimensions=AZURESEARCH_DIMENSIONS, vector_search_configuration="default"),
                    SimpleField(name=FIELDS_DOCUMENT_ID, type=SearchFieldDataType.String, filterable=True, sortable=True),
                    SimpleField(name=FIELDS_SOURCE, type=SearchFieldDataType.String, filterable=True, sortable=True),
                    SimpleField(name=FIELDS_SOURCE_ID, type=SearchFieldDataType.String, filterable=True, sortable=True),
                    SimpleField(name=FIELDS_URL, type=SearchFieldDataType.String),
                    SimpleField(name=FIELDS_CREATED_AT, type=SearchFieldDataType.DateTimeOffset, filterable=True, sortable=True),
                    SimpleField(name=FIELDS_AUTHOR, type=SearchFieldDataType.String, filterable=True, sortable=True)
                ],
                semantic_settings=None if AZURESEARCH_SEMANTIC_CONFIG is None else SemanticSettings(
                    configurations=[SemanticConfiguration(
                        name=AZURESEARCH_SEMANTIC_CONFIG,
                        prioritized_fields=PrioritizedFields(
                            title_field=None, prioritized_content_fields=[SemanticField(field_name=FIELDS_TEXT)]
                        )
                    )]
                ),
                vector_search=VectorSearch(
                    algorithm_configurations=[
                        HnswVectorSearchAlgorithmConfiguration(
                            name="default",
                            kind="hnsw",
                            # Could change to dotproduct for OpenAI's embeddings since they normalize vectors to unit length
                            hnsw_parameters=HnswParameters(metric="cosine") 
                        )
                    ]
                )
            )
        )

    @staticmethod
    def _create_credentials(use_async: bool) -> Union[AzureKeyCredential, DefaultAzureCredential, DefaultAzureCredentialSync]:
        if AZURESEARCH_API_KEY is None:
            logger.info("Using DefaultAzureCredential for Azure Search, make sure local identity or managed identity are set up appropriately")
            credential = DefaultAzureCredential() if use_async else DefaultAzureCredentialSync()
        else:
            logger.info("Using an API key to authenticate with Azure Search")
            credential = AzureKeyCredential(AZURESEARCH_API_KEY)
        return credential



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/datastore/providers/chroma_datastore.py
================================================
"""
Chroma datastore support for the ChatGPT retrieval plugin.

Consult the Chroma docs and GitHub repo for more information:
- https://docs.trychroma.com/usage-guide?lang=py
- https://github.com/chroma-core/chroma
- https://www.trychroma.com/
"""

import os
from datetime import datetime
from typing import Dict, List, Optional

import chromadb

from datastore.datastore import DataStore
from models.models import (
    Document,
    DocumentChunk,
    DocumentChunkMetadata,
    DocumentChunkWithScore,
    DocumentMetadataFilter,
    QueryResult,
    QueryWithEmbedding,
    Source,
)
from services.chunks import get_document_chunks

CHROMA_IN_MEMORY = os.environ.get("CHROMA_IN_MEMORY", "True")
CHROMA_PERSISTENCE_DIR = os.environ.get("CHROMA_PERSISTENCE_DIR", "openai")
CHROMA_HOST = os.environ.get("CHROMA_HOST", "http://127.0.0.1")
CHROMA_PORT = os.environ.get("CHROMA_PORT", "8000")
CHROMA_COLLECTION = os.environ.get("CHROMA_COLLECTION", "openaiembeddings")


class ChromaDataStore(DataStore):
    def __init__(
        self,
        in_memory: bool = CHROMA_IN_MEMORY,  # type: ignore
        persistence_dir: Optional[str] = CHROMA_PERSISTENCE_DIR,
        collection_name: str = CHROMA_COLLECTION,
        host: str = CHROMA_HOST,
        port: str = CHROMA_PORT,
        client: Optional[chromadb.Client] = None,
    ):
        if client:
            self._client = client
        else:
            if in_memory:
                settings = (
                    chromadb.config.Settings(
                        chroma_db_impl="duckdb+parquet",
                        persist_directory=persistence_dir,
                    )
                    if persistence_dir
                    else chromadb.config.Settings()
                )

                self._client = chromadb.Client(settings=settings)
            else:
                self._client = chromadb.Client(
                    settings=chromadb.config.Settings(
                        chroma_api_impl="rest",
                        chroma_server_host=host,
                        chroma_server_http_port=port,
                    )
                )
        self._collection = self._client.get_or_create_collection(
            name=collection_name,
            embedding_function=None,
        )

    async def upsert(
        self, documents: List[Document], chunk_token_size: Optional[int] = None
    ) -> List[str]:
        """
        Takes in a list of documents and inserts them into the database. If an id already exists, the document is updated.
        Return a list of document ids.
        """

        chunks = get_document_chunks(documents, chunk_token_size)

        # Chroma has a true upsert, so we don't need to delete first
        return await self._upsert(chunks)

    async def _upsert(self, chunks: Dict[str, List[DocumentChunk]]) -> List[str]:
        """
        Takes in a list of list of document chunks and inserts them into the database.
        Return a list of document ids.
        """

        self._collection.upsert(
            ids=[chunk.id for chunk_list in chunks.values() for chunk in chunk_list],
            embeddings=[
                chunk.embedding
                for chunk_list in chunks.values()
                for chunk in chunk_list
            ],
            documents=[
                chunk.text for chunk_list in chunks.values() for chunk in chunk_list
            ],
            metadatas=[
                self._process_metadata_for_storage(chunk.metadata)
                for chunk_list in chunks.values()
                for chunk in chunk_list
            ],
        )
        return list(chunks.keys())

    def _where_from_query_filter(self, query_filter: DocumentMetadataFilter) -> Dict:
        output = {
            k: v
            for (k, v) in query_filter.dict().items()
            if v is not None and k != "start_date" and k != "end_date" and k != "source"
        }
        if query_filter.source:
            output["source"] = query_filter.source.value
        if query_filter.start_date and query_filter.end_date:
            output["$and"] = [
                {
                    "created_at": {
                        "$gte": int(
                            datetime.fromisoformat(query_filter.start_date).timestamp()
                        )
                    }
                },
                {
                    "created_at": {
                        "$lte": int(
                            datetime.fromisoformat(query_filter.end_date).timestamp()
                        )
                    }
                },
            ]
        elif query_filter.start_date:
            output["created_at"] = {
                "$gte": int(datetime.fromisoformat(query_filter.start_date).timestamp())
            }
        elif query_filter.end_date:
            output["created_at"] = {
                "$lte": int(datetime.fromisoformat(query_filter.end_date).timestamp())
            }

        return output

    def _process_metadata_for_storage(self, metadata: DocumentChunkMetadata) -> Dict:
        stored_metadata = {}
        if metadata.source:
            stored_metadata["source"] = metadata.source.value
        if metadata.source_id:
            stored_metadata["source_id"] = metadata.source_id
        if metadata.url:
            stored_metadata["url"] = metadata.url
        if metadata.created_at:
            stored_metadata["created_at"] = int(
                datetime.fromisoformat(metadata.created_at).timestamp()
            )
        if metadata.author:
            stored_metadata["author"] = metadata.author
        if metadata.document_id:
            stored_metadata["document_id"] = metadata.document_id

        return stored_metadata

    def _process_metadata_from_storage(self, metadata: Dict) -> DocumentChunkMetadata:
        return DocumentChunkMetadata(
            source=Source(metadata["source"]) if "source" in metadata else None,
            source_id=metadata.get("source_id", None),
            url=metadata.get("url", None),
            created_at=datetime.fromtimestamp(metadata["created_at"]).isoformat()
            if "created_at" in metadata
            else None,
            author=metadata.get("author", None),
            document_id=metadata.get("document_id", None),
        )

    async def _query(self, queries: List[QueryWithEmbedding]) -> List[QueryResult]:
        """
        Takes in a list of queries with embeddings and filters and returns a list of query results with matching document chunks and scores.
        """
        results = [
            self._collection.query(
                query_embeddings=[query.embedding],
                include=["documents", "distances", "metadatas"],  # embeddings
                n_results=min(query.top_k, self._collection.count()),  # type: ignore
                where=(
                    self._where_from_query_filter(query.filter) if query.filter else {}
                ),
            )
            for query in queries
        ]

        output = []
        for query, result in zip(queries, results):
            inner_results = []
            (ids,) = result["ids"]
            # (embeddings,) = result["embeddings"]
            (documents,) = result["documents"]
            (metadatas,) = result["metadatas"]
            (distances,) = result["distances"]
            for id_, text, metadata, distance in zip(
                ids,
                documents,
                metadatas,
                distances,  # embeddings (https://github.com/openai/chatgpt-retrieval-plugin/pull/59#discussion_r1154985153)
            ):
                inner_results.append(
                    DocumentChunkWithScore(
                        id=id_,
                        text=text,
                        metadata=self._process_metadata_from_storage(metadata),
                        # embedding=embedding,
                        score=distance,
                    )
                )
            output.append(QueryResult(query=query.query, results=inner_results))

        return output

    async def delete(
        self,
        ids: Optional[List[str]] = None,
        filter: Optional[DocumentMetadataFilter] = None,
        delete_all: Optional[bool] = None,
    ) -> bool:
        """
        Removes vectors by ids, filter, or everything in the datastore.
        Multiple parameters can be used at once.
        Returns whether the operation was successful.
        """
        if delete_all:
            self._collection.delete()
            return True

        if ids and len(ids) > 0:
            if len(ids) > 1:
                where_clause = {"$or": [{"document_id": id_} for id_ in ids]}
            else:
                (id_,) = ids
                where_clause = {"document_id": id_}

            if filter:
                where_clause = {
                    "$and": [self._where_from_query_filter(filter), where_clause]
                }
        elif filter:
            where_clause = self._where_from_query_filter(filter)

        self._collection.delete(where=where_clause)
        return True



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/datastore/providers/elasticsearch_datastore.py
================================================
import os
from typing import Dict, List, Any, Optional

import elasticsearch
from elasticsearch import Elasticsearch, helpers
from loguru import logger

from datastore.datastore import DataStore
from models.models import (
    DocumentChunk,
    DocumentChunkWithScore,
    DocumentMetadataFilter,
    QueryResult,
    QueryWithEmbedding,
)
from services.date import to_unix_timestamp

ELASTICSEARCH_URL = os.environ.get("ELASTICSEARCH_URL", "http://localhost:9200")
ELASTICSEARCH_CLOUD_ID = os.environ.get("ELASTICSEARCH_CLOUD_ID")
ELASTICSEARCH_USERNAME = os.environ.get("ELASTICSEARCH_USERNAME")
ELASTICSEARCH_PASSWORD = os.environ.get("ELASTICSEARCH_PASSWORD")
ELASTICSEARCH_API_KEY = os.environ.get("ELASTICSEARCH_API_KEY")

ELASTICSEARCH_INDEX = os.environ.get("ELASTICSEARCH_INDEX")
ELASTICSEARCH_REPLICAS = int(os.environ.get("ELASTICSEARCH_REPLICAS", "1"))
ELASTICSEARCH_SHARDS = int(os.environ.get("ELASTICSEARCH_SHARDS", "1"))

VECTOR_SIZE = 1536
UPSERT_BATCH_SIZE = 100


class ElasticsearchDataStore(DataStore):
    def __init__(
        self,
        index_name: Optional[str] = None,
        vector_size: int = VECTOR_SIZE,
        similarity: str = "cosine",
        replicas: int = ELASTICSEARCH_REPLICAS,
        shards: int = ELASTICSEARCH_SHARDS,
        recreate_index: bool = True,
    ):
        """
        Args:
            index_name: Name of the index to be used
            vector_size: Size of the embedding stored in a collection
            similarity:
                Any of "cosine" / "l2_norm" / "dot_product".

        """
        assert similarity in [
            "cosine",
            "l2_norm",
            "dot_product",
        ], "Similarity must be one of 'cosine' / 'l2_norm' / 'dot_product'."
        assert replicas > 0, "Replicas must be greater than or equal to 0."
        assert shards > 0, "Shards must be greater than or equal to 0."

        self.client = connect_to_elasticsearch(
            ELASTICSEARCH_URL,
            ELASTICSEARCH_CLOUD_ID,
            ELASTICSEARCH_API_KEY,
            ELASTICSEARCH_USERNAME,
            ELASTICSEARCH_PASSWORD,
        )
        assert (
            index_name != "" or ELASTICSEARCH_INDEX != ""
        ), "Please provide an index name."
        self.index_name = index_name or ELASTICSEARCH_INDEX or ""

        replicas = replicas or ELASTICSEARCH_REPLICAS
        shards = shards or ELASTICSEARCH_SHARDS

        # Set up the collection so the documents might be inserted or queried
        self._set_up_index(vector_size, similarity, replicas, shards, recreate_index)

    async def _upsert(self, chunks: Dict[str, List[DocumentChunk]]) -> List[str]:
        """
        Takes in a list of document chunks and inserts them into the database.
        Return a list of document ids.
        """
        actions = []
        for _, chunkList in chunks.items():
            for chunk in chunkList:
                actions = (
                    actions
                    + self._convert_document_chunk_to_es_document_operation(chunk)
                )

        self.client.bulk(operations=actions, index=self.index_name)
        return list(chunks.keys())

    async def _query(
        self,
        queries: List[QueryWithEmbedding],
    ) -> List[QueryResult]:
        """
        Takes in a list of queries with embeddings and filters and returns a list of query results with matching document chunks and scores.
        """
        searches = self._convert_queries_to_msearch_query(queries)
        results = self.client.msearch(searches=searches)
        return [
            QueryResult(
                query=query.query,
                results=[
                    self._convert_hit_to_document_chunk_with_score(hit)
                    for hit in result["hits"]["hits"]
                ],
            )
            for query, result in zip(queries, results["responses"])
        ]

    async def delete(
        self,
        ids: Optional[List[str]] = None,
        filter: Optional[DocumentMetadataFilter] = None,
        delete_all: Optional[bool] = None,
    ) -> bool:
        """
        Removes vectors by ids, filter, or everything in the datastore.
        Returns whether the operation was successful.
        """

        # Delete all vectors from the index if delete_all is True
        if delete_all:
            try:
                logger.info(f"Deleting all vectors from index")
                self.client.delete_by_query(
                    index=self.index_name, query={"match_all": {}}
                )
                logger.info(f"Deleted all vectors successfully")
                return True
            except Exception as e:
                logger.error(f"Error deleting all vectors: {e}")
                raise e

        # Convert the metadata filter object to a dict with elasticsearch filter expressions
        es_filters = self._get_es_filters(filter)
        # Delete vectors that match the filter from the index if the filter is not empty
        if es_filters != {}:
            try:
                logger.info(f"Deleting vectors with filter {es_filters}")
                self.client.delete_by_query(index=self.index_name, query=es_filters)
                logger.info(f"Deleted vectors with filter successfully")
            except Exception as e:
                logger.error(f"Error deleting vectors with filter: {e}")
                raise e

        if ids:
            try:
                documents_to_delete = [doc_id for doc_id in ids]
                logger.info(f"Deleting {len(documents_to_delete)} documents")
                res = self.client.delete_by_query(
                    index=self.index_name,
                    query={"terms": {"metadata.document_id": documents_to_delete}},
                )
                logger.info(f"Deleted documents successfully")
            except Exception as e:
                logger.error(f"Error deleting documents: {e}")
                raise e

        return True

    def _get_es_filters(
        self, filter: Optional[DocumentMetadataFilter] = None
    ) -> Dict[str, Any]:
        if filter is None:
            return {}

        es_filters = {
            "bool": {
                "must": [],
            }
        }

        # For each field in the MetadataFilter, check if it has a value and add the corresponding pinecone filter expression
        # For start_date and end_date, uses the range query - gte and lte operators respectively
        # For other fields, uses the term query
        for field, value in filter.dict().items():
            if value is not None:
                if field == "start_date":
                    es_filters["bool"]["must"].append(
                        {"range": {"created_at": {"gte": to_unix_timestamp(value)}}}
                    )
                elif field == "end_date":
                    es_filters["bool"]["must"].append(
                        {"range": {"created_at": {"lte": to_unix_timestamp(value)}}}
                    )
                else:
                    es_filters["bool"]["must"].append(
                        {"term": {f"metadata.{field}": value}}
                    )

        return es_filters

    def _convert_document_chunk_to_es_document_operation(
        self, document_chunk: DocumentChunk
    ) -> List[Dict]:
        created_at = (
            to_unix_timestamp(document_chunk.metadata.created_at)
            if document_chunk.metadata.created_at is not None
            else None
        )

        action_and_metadata = {
            "index": {
                "_index": self.index_name,
                "_id": document_chunk.id,
            }
        }

        source = {
            "id": document_chunk.id,
            "text": document_chunk.text,
            "metadata": document_chunk.metadata.dict(),
            "created_at": created_at,
            "embedding": document_chunk.embedding,
        }

        return [action_and_metadata, source]

    def _convert_queries_to_msearch_query(self, queries: List[QueryWithEmbedding]):
        searches = []

        for query in queries:
            searches.append({"index": self.index_name})
            searches.append(
                {
                    "_source": True,
                    "knn": {
                        "field": "embedding",
                        "query_vector": query.embedding,
                        "k": query.top_k,
                        "num_candidates": query.top_k,
                    },
                    "size": query.top_k,
                }
            )

        return searches

    def _convert_hit_to_document_chunk_with_score(self, hit) -> DocumentChunkWithScore:
        return DocumentChunkWithScore(
            id=hit["_id"],
            text=hit["_source"]["text"],  # type: ignore
            metadata=hit["_source"]["metadata"],  # type: ignore
            embedding=hit["_source"]["embedding"],  # type: ignore
            score=hit["_score"],
        )

    def _set_up_index(
        self,
        vector_size: int,
        similarity: str,
        replicas: int,
        shards: int,
        recreate_index: bool,
    ) -> None:
        if recreate_index:
            self._recreate_index(similarity, vector_size, replicas, shards)

        try:
            index_mapping = self.client.indices.get_mapping(index=self.index_name)
            current_similarity = index_mapping[self.index_name]["mappings"]["properties"]["embedding"]["similarity"]  # type: ignore
            current_vector_size = index_mapping[self.index_name]["mappings"]["properties"]["embedding"]["dims"]  # type: ignore

            if current_similarity != similarity:
                raise ValueError(
                    f"Collection '{self.index_name}' already exists in Elasticsearch, "
                    f"but it is configured with a similarity '{current_similarity}'. "
                    f"If you want to use that collection, but with a different "
                    f"similarity, please set `recreate_index=True` argument."
                )

            if current_vector_size != vector_size:
                raise ValueError(
                    f"Collection '{self.index_name}' already exists in Elasticsearch, "
                    f"but it is configured with a vector size '{current_vector_size}'. "
                    f"If you want to use that collection, but with a different "
                    f"vector size, please set `recreate_index=True` argument."
                )
        except elasticsearch.exceptions.NotFoundError:
            self._recreate_index(similarity, vector_size, replicas, shards)

    def _recreate_index(
        self, similarity: str, vector_size: int, replicas: int, shards: int
    ) -> None:
        settings = {
            "index": {
                "number_of_shards": shards,
                "number_of_replicas": replicas,
                "refresh_interval": "1s",
            }
        }
        mappings = {
            "properties": {
                "embedding": {
                    "type": "dense_vector",
                    "dims": vector_size,
                    "index": True,
                    "similarity": similarity,
                }
            }
        }

        self.client.indices.delete(
            index=self.index_name, ignore_unavailable=True, allow_no_indices=True
        )
        self.client.indices.create(
            index=self.index_name, mappings=mappings, settings=settings
        )


def connect_to_elasticsearch(
    elasticsearch_url=None, cloud_id=None, api_key=None, username=None, password=None
):
    # Check if both elasticsearch_url and cloud_id are defined
    if elasticsearch_url and cloud_id:
        raise ValueError(
            "Both elasticsearch_url and cloud_id are defined. Please provide only one."
        )

    # Initialize connection parameters dictionary
    connection_params = {}

    # Define the connection based on the provided parameters
    if elasticsearch_url:
        connection_params["hosts"] = [elasticsearch_url]
    elif cloud_id:
        connection_params["cloud_id"] = cloud_id
    else:
        raise ValueError("Please provide either elasticsearch_url or cloud_id.")

    # Add authentication details based on the provided parameters
    if api_key:
        connection_params["api_key"] = api_key
    elif username and password:
        connection_params["basic_auth"] = (username, password)
    else:
        logger.warning(
            "No authentication details provided. Please consider using an api_key or username and password to secure your connection."
        )

    # Establish the Elasticsearch client connection
    es_client = Elasticsearch(**connection_params)
    try:
        es_client.info()
    except Exception as e:
        logger.error(f"Error connecting to Elasticsearch: {e}")
        raise e

    return es_client



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/datastore/providers/llama_datastore.py
================================================
import json
import os
from typing import Dict, List, Optional, Type
from loguru import logger
from datastore.datastore import DataStore
from models.models import DocumentChunk, DocumentChunkMetadata, DocumentChunkWithScore, DocumentMetadataFilter, Query, QueryResult, QueryWithEmbedding

from llama_index.indices.base import BaseGPTIndex
from llama_index.indices.vector_store.base import GPTVectorStoreIndex
from llama_index.indices.query.schema import QueryBundle
from llama_index.response.schema import Response
from llama_index.data_structs.node_v2 import Node, DocumentRelationship, NodeWithScore
from llama_index.indices.registry import INDEX_STRUCT_TYPE_TO_INDEX_CLASS
from llama_index.data_structs.struct_type import IndexStructType
from llama_index.indices.response.builder import ResponseMode

INDEX_STRUCT_TYPE_STR = os.environ.get('LLAMA_INDEX_TYPE', IndexStructType.SIMPLE_DICT.value)
INDEX_JSON_PATH = os.environ.get('LLAMA_INDEX_JSON_PATH', None)
QUERY_KWARGS_JSON_PATH = os.environ.get('LLAMA_QUERY_KWARGS_JSON_PATH', None)
RESPONSE_MODE = os.environ.get('LLAMA_RESPONSE_MODE', ResponseMode.NO_TEXT.value)

EXTERNAL_VECTOR_STORE_INDEX_STRUCT_TYPES = [
    IndexStructType.DICT,
    IndexStructType.WEAVIATE,
    IndexStructType.PINECONE,
    IndexStructType.QDRANT,
    IndexStructType.CHROMA,
    IndexStructType.VECTOR_STORE,
]

def _create_or_load_index(
    index_type_str: Optional[str] = None,
    index_json_path: Optional[str] = None,
    index_type_to_index_cls: Optional[dict[str, Type[BaseGPTIndex]]] = None,
) -> BaseGPTIndex:
    """Create or load index from json path."""
    index_json_path = index_json_path or INDEX_JSON_PATH
    index_type_to_index_cls = index_type_to_index_cls or INDEX_STRUCT_TYPE_TO_INDEX_CLASS
    index_type_str = index_type_str or INDEX_STRUCT_TYPE_STR
    index_type = IndexStructType(index_type_str)

    if index_type not in index_type_to_index_cls:
        raise ValueError(f'Unknown index type: {index_type}')

    if index_type in EXTERNAL_VECTOR_STORE_INDEX_STRUCT_TYPES:
        raise ValueError('Please use vector store directly.')

    index_cls = index_type_to_index_cls[index_type]
    if index_json_path is None:
        return index_cls(nodes=[])  # Create empty index
    else:
        return index_cls.load_from_disk(index_json_path) # Load index from disk

def _create_or_load_query_kwargs(query_kwargs_json_path: Optional[str] = None) -> Optional[dict]:
    """Create or load query kwargs from json path."""
    query_kwargs_json_path= query_kwargs_json_path or QUERY_KWARGS_JSON_PATH
    query_kargs: Optional[dict] = None
    if  query_kwargs_json_path is not None:
        with open(INDEX_JSON_PATH, 'r') as f:
            query_kargs = json.load(f)
    return query_kargs


def _doc_chunk_to_node(doc_chunk: DocumentChunk, source_doc_id: str) -> Node:
    """Convert document chunk to Node"""
    return Node(
        doc_id=doc_chunk.id,
        text=doc_chunk.text,
        embedding=doc_chunk.embedding,
        extra_info=doc_chunk.metadata.dict(),
        relationships={
            DocumentRelationship.SOURCE: source_doc_id
        }
    )

def _query_with_embedding_to_query_bundle(query: QueryWithEmbedding) -> QueryBundle:
    return QueryBundle(
        query_str = query.query,
        embedding=query.embedding,
    )

def _source_node_to_doc_chunk_with_score(node_with_score: NodeWithScore) -> DocumentChunkWithScore:
    node = node_with_score.node
    if node.extra_info is not None:
        metadata = DocumentChunkMetadata(**node.extra_info)
    else:
        metadata = DocumentChunkMetadata()

    return DocumentChunkWithScore(
        id=node.doc_id,
        text=node.text,
        score=node_with_score.score if node_with_score.score is not None else 1.,
        metadata=metadata,
    )

def _response_to_query_result(response: Response, query: QueryWithEmbedding) -> QueryResult:
    results = [_source_node_to_doc_chunk_with_score(node) for node in response.source_nodes]
    return QueryResult(query=query.query, results=results,)

class LlamaDataStore(DataStore):
    def __init__(self, index: Optional[BaseGPTIndex] = None, query_kwargs: Optional[dict] = None):
        self._index = index or _create_or_load_index()
        self._query_kwargs = query_kwargs or _create_or_load_query_kwargs()

    async def _upsert(self, chunks: Dict[str, List[DocumentChunk]]) -> List[str]:
        """
        Takes in a list of list of document chunks and inserts them into the database.
        Return a list of document ids.
        """
        doc_ids = []
        for doc_id, doc_chunks in chunks.items():
            logger.debug(f"Upserting {doc_id} with {len(doc_chunks)} chunks")

            nodes = [
                _doc_chunk_to_node(doc_chunk=doc_chunk, source_doc_id=doc_id)
                for doc_chunk in doc_chunks
            ]
                
            self._index.insert_nodes(nodes)
            doc_ids.append(doc_id)
        return doc_ids

    async def _query(
        self,
        queries: List[QueryWithEmbedding],
    ) -> List[QueryResult]:
        """
        Takes in a list of queries with embeddings and filters and
        returns a list of query results with matching document chunks and scores.
        """
        query_result_all = []
        for query in queries:
            if query.filter is not None:
                logger.warning('Filters are not supported yet, ignoring for now.')

            query_bundle = _query_with_embedding_to_query_bundle(query)

            # Setup query kwargs
            if self._query_kwargs is not None:
                query_kwargs = self._query_kwargs
            else:
                query_kwargs = {}
            # TODO: support top_k for other indices
            if isinstance(self._index, GPTVectorStoreIndex):
                query_kwargs['similarity_top_k'] = query.top_k

            response = await self._index.aquery(query_bundle, response_mode=RESPONSE_MODE, **query_kwargs)
            
            query_result = _response_to_query_result(response, query)
            query_result_all.append(query_result)
        
        return query_result_all

    async def delete(
        self,
        ids: Optional[List[str]] = None,
        filter: Optional[DocumentMetadataFilter] = None,
        delete_all: Optional[bool] = None,
    ) -> bool:
        """
        Removes vectors by ids, filter, or everything in the datastore.
        Returns whether the operation was successful.
        """
        if delete_all:
            logger.warning('Delete all not supported yet.')
            return False
        
        if filter is not None:
            logger.warning('Filters are not supported yet.')
            return False

        if ids is not None:
            for id_ in ids:
                try:
                    self._index.delete(id_)
                except NotImplementedError:
                    # NOTE: some indices does not support delete yet.
                    logger.warning(f'{type(self._index)} does not support delete yet.')
                    return False

        return True


================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/datastore/providers/milvus_datastore.py
================================================
import json
import os
import asyncio

from loguru import logger
from typing import Dict, List, Optional
from pymilvus import (
    Collection,
    connections,
    utility,
    FieldSchema,
    DataType,
    CollectionSchema,
    MilvusException,
)
from uuid import uuid4


from services.date import to_unix_timestamp
from datastore.datastore import DataStore
from models.models import (
    DocumentChunk,
    DocumentChunkMetadata,
    Source,
    DocumentMetadataFilter,
    QueryResult,
    QueryWithEmbedding,
    DocumentChunkWithScore,
)

MILVUS_COLLECTION = os.environ.get("MILVUS_COLLECTION") or "c" + uuid4().hex
MILVUS_HOST = os.environ.get("MILVUS_HOST") or "localhost"
MILVUS_PORT = os.environ.get("MILVUS_PORT") or 19530
MILVUS_USER = os.environ.get("MILVUS_USER")
MILVUS_PASSWORD = os.environ.get("MILVUS_PASSWORD")
MILVUS_USE_SECURITY = False if MILVUS_PASSWORD is None else True

MILVUS_INDEX_PARAMS = os.environ.get("MILVUS_INDEX_PARAMS")
MILVUS_SEARCH_PARAMS = os.environ.get("MILVUS_SEARCH_PARAMS")
MILVUS_CONSISTENCY_LEVEL = os.environ.get("MILVUS_CONSISTENCY_LEVEL")

UPSERT_BATCH_SIZE = 100
OUTPUT_DIM = 1536
EMBEDDING_FIELD = "embedding"


class Required:
    pass

# The fields names that we are going to be storing within Milvus, the field declaration for schema creation, and the default value
SCHEMA_V1 = [
    (
        "pk",
        FieldSchema(name="pk", dtype=DataType.INT64, is_primary=True, auto_id=True),
        Required,
    ),
    (
        EMBEDDING_FIELD,
        FieldSchema(name=EMBEDDING_FIELD, dtype=DataType.FLOAT_VECTOR, dim=OUTPUT_DIM),
        Required,
    ),
    (
        "text",
        FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=65535),
        Required,
    ),
    (
        "document_id",
        FieldSchema(name="document_id", dtype=DataType.VARCHAR, max_length=65535),
        "",
    ),
    (
        "source_id",
        FieldSchema(name="source_id", dtype=DataType.VARCHAR, max_length=65535),
        "",
    ),
    (
        "id",
        FieldSchema(
            name="id",
            dtype=DataType.VARCHAR,
            max_length=65535,
        ),
        "",
    ),
    (
        "source",
        FieldSchema(name="source", dtype=DataType.VARCHAR, max_length=65535),
        "",
    ),
    ("url", FieldSchema(name="url", dtype=DataType.VARCHAR, max_length=65535), ""),
    ("created_at", FieldSchema(name="created_at", dtype=DataType.INT64), -1),
    (
        "author",
        FieldSchema(name="author", dtype=DataType.VARCHAR, max_length=65535),
        "",
    ),
]

# V2 schema, remomve the "pk" field
SCHEMA_V2 = SCHEMA_V1[1:]
SCHEMA_V2[4][1].is_primary = True


class MilvusDataStore(DataStore):
    def __init__(
        self,
        create_new: Optional[bool] = False,
        consistency_level: str = "Bounded",
    ):
        """Create a Milvus DataStore.

        The Milvus Datastore allows for storing your indexes and metadata within a Milvus instance.

        Args:
            create_new (Optional[bool], optional): Whether to overwrite if collection already exists. Defaults to True.
            consistency_level(str, optional): Specify the collection consistency level.
                                                Defaults to "Bounded" for search performance.
                                                Set to "Strong" in test cases for result validation.
        """
        # Overwrite the default consistency level by MILVUS_CONSISTENCY_LEVEL
        self._consistency_level = MILVUS_CONSISTENCY_LEVEL or consistency_level
        self._create_connection()

        self._create_collection(MILVUS_COLLECTION, create_new)  # type: ignore
        self._create_index()

    def _get_schema(self):
        return SCHEMA_V1 if self._schema_ver == "V1" else SCHEMA_V2

    def _create_connection(self):
        try:
            self.alias = ""
            # Check if the connection already exists
            for x in connections.list_connections():
                addr = connections.get_connection_addr(x[0])
                if x[1] and ('address' in addr) and (addr['address'] == "{}:{}".format(MILVUS_HOST, MILVUS_PORT)):
                    self.alias = x[0]
                    logger.info("Reuse connection to Milvus server '{}:{}' with alias '{:s}'"
                                     .format(MILVUS_HOST, MILVUS_PORT, self.alias))
                    break

            # Connect to the Milvus instance using the passed in Environment variables
            if len(self.alias) == 0:
                self.alias = uuid4().hex
                connections.connect(
                    alias=self.alias,
                    host=MILVUS_HOST,
                    port=MILVUS_PORT,
                    user=MILVUS_USER,  # type: ignore
                    password=MILVUS_PASSWORD,  # type: ignore
                    secure=MILVUS_USE_SECURITY,
                )
                logger.info("Create connection to Milvus server '{}:{}' with alias '{:s}'"
                                 .format(MILVUS_HOST, MILVUS_PORT, self.alias))
        except Exception as e:
            logger.error("Failed to create connection to Milvus server '{}:{}', error: {}"
                            .format(MILVUS_HOST, MILVUS_PORT, e))

    def _create_collection(self, collection_name, create_new: bool) -> None:
        """Create a collection based on environment and passed in variables.

        Args:
            create_new (bool): Whether to overwrite if collection already exists.
        """
        try:
            self._schema_ver = "V1"
            # If the collection exists and create_new is True, drop the existing collection
            if utility.has_collection(collection_name, using=self.alias) and create_new:
                utility.drop_collection(collection_name, using=self.alias)

            # Check if the collection doesnt exist
            if utility.has_collection(collection_name, using=self.alias) is False:
                # If it doesnt exist use the field params from init to create a new schem
                schema = [field[1] for field in SCHEMA_V2]
                schema = CollectionSchema(schema)
                # Use the schema to create a new collection
                self.col = Collection(
                    collection_name,
                    schema=schema,
                    using=self.alias,
                    consistency_level=self._consistency_level,
                )
                self._schema_ver = "V2"
                logger.info("Create Milvus collection '{}' with schema {} and consistency level {}"
                                 .format(collection_name, self._schema_ver, self._consistency_level))
            else:
                # If the collection exists, point to it
                self.col = Collection(
                    collection_name, using=self.alias
                )  # type: ignore
                # Which sechma is used
                for field in self.col.schema.fields:
                    if field.name == "id" and field.is_primary:
                        self._schema_ver = "V2"
                        break
                logger.info("Milvus collection '{}' already exists with schema {}"
                                 .format(collection_name, self._schema_ver))
        except Exception as e:
            logger.error("Failed to create collection '{}', error: {}".format(collection_name, e))

    def _create_index(self):
        # TODO: verify index/search params passed by os.environ
        self.index_params = MILVUS_INDEX_PARAMS or None
        self.search_params = MILVUS_SEARCH_PARAMS or None
        try:
            # If no index on the collection, create one
            if len(self.col.indexes) == 0:
                if self.index_params is not None:
                    # Convert the string format to JSON format parameters passed by MILVUS_INDEX_PARAMS
                    self.index_params = json.loads(self.index_params)
                    logger.info("Create Milvus index: {}".format(self.index_params))
                    # Create an index on the 'embedding' field with the index params found in init
                    self.col.create_index(EMBEDDING_FIELD, index_params=self.index_params)
                else:
                    # If no index param supplied, to first create an HNSW index for Milvus
                    try:
                        i_p = {
                            "metric_type": "IP",
                            "index_type": "HNSW",
                            "params": {"M": 8, "efConstruction": 64},
                        }
                        logger.info("Attempting creation of Milvus '{}' index".format(i_p["index_type"]))
                        self.col.create_index(EMBEDDING_FIELD, index_params=i_p)
                        self.index_params = i_p
                        logger.info("Creation of Milvus '{}' index successful".format(i_p["index_type"]))
                    # If create fails, most likely due to being Zilliz Cloud instance, try to create an AutoIndex
                    except MilvusException:
                        logger.info("Attempting creation of Milvus default index")
                        i_p = {"metric_type": "IP", "index_type": "AUTOINDEX", "params": {}}
                        self.col.create_index(EMBEDDING_FIELD, index_params=i_p)
                        self.index_params = i_p
                        logger.info("Creation of Milvus default index successful")
            # If an index already exists, grab its params
            else:
                # How about if the first index is not vector index?
                for index in self.col.indexes:
                    idx = index.to_dict()
                    if idx["field"] == EMBEDDING_FIELD:
                        logger.info("Index already exists: {}".format(idx))
                        self.index_params = idx['index_param']
                        break

            self.col.load()

            if self.search_params is not None:
                # Convert the string format to JSON format parameters passed by MILVUS_SEARCH_PARAMS
                self.search_params = json.loads(self.search_params)
            else:
                # The default search params
                metric_type = "IP"
                if "metric_type" in self.index_params:
                    metric_type = self.index_params["metric_type"]
                default_search_params = {
                    "IVF_FLAT": {"metric_type": metric_type, "params": {"nprobe": 10}},
                    "IVF_SQ8": {"metric_type": metric_type, "params": {"nprobe": 10}},
                    "IVF_PQ": {"metric_type": metric_type, "params": {"nprobe": 10}},
                    "HNSW": {"metric_type": metric_type, "params": {"ef": 10}},
                    "RHNSW_FLAT": {"metric_type": metric_type, "params": {"ef": 10}},
                    "RHNSW_SQ": {"metric_type": metric_type, "params": {"ef": 10}},
                    "RHNSW_PQ": {"metric_type": metric_type, "params": {"ef": 10}},
                    "IVF_HNSW": {"metric_type": metric_type, "params": {"nprobe": 10, "ef": 10}},
                    "ANNOY": {"metric_type": metric_type, "params": {"search_k": 10}},
                    "AUTOINDEX": {"metric_type": metric_type, "params": {}},
                }
                # Set the search params
                self.search_params = default_search_params[self.index_params["index_type"]]
            logger.info("Milvus search parameters: {}".format(self.search_params))
        except Exception as e:
            logger.error("Failed to create index, error: {}".format(e))

    async def _upsert(self, chunks: Dict[str, List[DocumentChunk]]) -> List[str]:
        """Upsert chunks into the datastore.

        Args:
            chunks (Dict[str, List[DocumentChunk]]): A list of DocumentChunks to insert

        Raises:
            e: Error in upserting data.

        Returns:
            List[str]: The document_id's that were inserted.
        """
        try:
            # The doc id's to return for the upsert
            doc_ids: List[str] = []
            # List to collect all the insert data, skip the "pk" for schema V1
            offset = 1 if self._schema_ver == "V1" else 0
            insert_data = [[] for _ in range(len(self._get_schema()) - offset)]

            # Go through each document chunklist and grab the data
            for doc_id, chunk_list in chunks.items():
                # Append the doc_id to the list we are returning
                doc_ids.append(doc_id)
                # Examine each chunk in the chunklist
                for chunk in chunk_list:
                    # Extract data from the chunk
                    list_of_data = self._get_values(chunk)
                    # Check if the data is valid
                    if list_of_data is not None:
                        # Append each field to the insert_data
                        for x in range(len(insert_data)):
                            insert_data[x].append(list_of_data[x])
            # Slice up our insert data into batches
            batches = [
                insert_data[i : i + UPSERT_BATCH_SIZE]
                for i in range(0, len(insert_data), UPSERT_BATCH_SIZE)
            ]

            # Attempt to insert each batch into our collection
            # batch data can work with both V1 and V2 schema
            for batch in batches:
                if len(batch[0]) != 0:
                    try:
                        logger.info(f"Upserting batch of size {len(batch[0])}")
                        self.col.insert(batch)
                        logger.info(f"Upserted batch successfully")
                    except Exception as e:
                        logger.error(f"Failed to insert batch records, error: {e}")
                        raise e

            # This setting perfoms flushes after insert. Small insert == bad to use
            # self.col.flush()
            return doc_ids
        except Exception as e:
            logger.error("Failed to insert records, error: {}".format(e))
            return []


    def _get_values(self, chunk: DocumentChunk) -> List[any] | None:  # type: ignore
        """Convert the chunk into a list of values to insert whose indexes align with fields.

        Args:
            chunk (DocumentChunk): The chunk to convert.

        Returns:
            List (any): The values to insert.
        """
        # Convert DocumentChunk and its sub models to dict
        values = chunk.dict()
        # Unpack the metadata into the same dict
        meta = values.pop("metadata")
        values.update(meta)

        # Convert date to int timestamp form
        if values["created_at"]:
            values["created_at"] = to_unix_timestamp(values["created_at"])

        # If source exists, change from Source object to the string value it holds
        if values["source"]:
            values["source"] = values["source"].value
        # List to collect data we will return
        ret = []
        # Grab data responding to each field, excluding the hidden auto pk field for schema V1
        offset = 1 if self._schema_ver == "V1" else 0
        for key, _, default in self._get_schema()[offset:]:
            # Grab the data at the key and default to our defaults set in init
            x = values.get(key) or default
            # If one of our required fields is missing, ignore the entire entry
            if x is Required:
                logger.info("Chunk " + values["id"] + " missing " + key + " skipping")
                return None
            # Add the corresponding value if it passes the tests
            ret.append(x)
        return ret

    async def _query(
        self,
        queries: List[QueryWithEmbedding],
    ) -> List[QueryResult]:
        """Query the QueryWithEmbedding against the MilvusDocumentSearch

        Search the embedding and its filter in the collection.

        Args:
            queries (List[QueryWithEmbedding]): The list of searches to perform.

        Returns:
            List[QueryResult]: Results for each search.
        """
        # Async to perform the query, adapted from pinecone implementation
        async def _single_query(query: QueryWithEmbedding) -> QueryResult:
            try:
                filter = None
                # Set the filter to expression that is valid for Milvus
                if query.filter is not None:
                    # Either a valid filter or None will be returned
                    filter = self._get_filter(query.filter)

                # Perform our search
                return_from = 2 if self._schema_ver == "V1" else 1
                res = self.col.search(
                    data=[query.embedding],
                    anns_field=EMBEDDING_FIELD,
                    param=self.search_params,
                    limit=query.top_k,
                    expr=filter,
                    output_fields=[
                        field[0] for field in self._get_schema()[return_from:]
                    ],  # Ignoring pk, embedding
                )
                # Results that will hold our DocumentChunkWithScores
                results = []
                # Parse every result for our search
                for hit in res[0]:  # type: ignore
                    # The distance score for the search result, falls under DocumentChunkWithScore
                    score = hit.score
                    # Our metadata info, falls under DocumentChunkMetadata
                    metadata = {}
                    # Grab the values that correspond to our fields, ignore pk and embedding.
                    for x in [field[0] for field in self._get_schema()[return_from:]]:
                        metadata[x] = hit.entity.get(x)
                    # If the source isn't valid, convert to None
                    if metadata["source"] not in Source.__members__:
                        metadata["source"] = None
                    # Text falls under the DocumentChunk
                    text = metadata.pop("text")
                    # Id falls under the DocumentChunk
                    ids = metadata.pop("id")
                    chunk = DocumentChunkWithScore(
                        id=ids,
                        score=score,
                        text=text,
                        metadata=DocumentChunkMetadata(**metadata),
                    )
                    results.append(chunk)

                # TODO: decide on doing queries to grab the embedding itself, slows down performance as double query occurs

                return QueryResult(query=query.query, results=results)
            except Exception as e:
                logger.error("Failed to query, error: {}".format(e))
                return QueryResult(query=query.query, results=[])

        results: List[QueryResult] = await asyncio.gather(
            *[_single_query(query) for query in queries]
        )
        return results

    async def delete(
        self,
        ids: Optional[List[str]] = None,
        filter: Optional[DocumentMetadataFilter] = None,
        delete_all: Optional[bool] = None,
    ) -> bool:
        """Delete the entities based either on the chunk_id of the vector,

        Args:
            ids (Optional[List[str]], optional): The document_ids to delete. Defaults to None.
            filter (Optional[DocumentMetadataFilter], optional): The filter to delete by. Defaults to None.
            delete_all (Optional[bool], optional): Whether to drop the collection and recreate it. Defaults to None.
        """
        # If deleting all, drop and create the new collection
        if delete_all:
            coll_name = self.col.name
            logger.info("Delete the entire collection {} and create new one".format(coll_name))
            # Release the collection from memory
            self.col.release()
            # Drop the collection
            self.col.drop()
            # Recreate the new collection
            self._create_collection(coll_name, True)
            self._create_index()
            return True

        # Keep track of how many we have deleted for later printing
        delete_count = 0
        batch_size = 100
        pk_name = "pk" if self._schema_ver == "V1" else "id"
        try:
            # According to the api design, the ids is a list of document_id,
            # document_id is not primary key, use query+delete to workaround,
            # in future version we can delete by expression
            if (ids is not None) and len(ids) > 0:
                # Add quotation marks around the string format id
                ids = ['"' + str(id) + '"' for id in ids]
                # Query for the pk's of entries that match id's
                ids = self.col.query(f"document_id in [{','.join(ids)}]")
                # Convert to list of pks
                pks = [str(entry[pk_name]) for entry in ids]  # type: ignore
                # for schema V2, the "id" is varchar, rewrite the expression
                if self._schema_ver != "V1":
                    pks = ['"' + pk + '"' for pk in pks]

                # Delete by ids batch by batch(avoid too long expression)
                logger.info("Apply {:d} deletions to schema {:s}".format(len(pks), self._schema_ver))
                while len(pks) > 0:
                    batch_pks = pks[:batch_size]
                    pks = pks[batch_size:]
                    # Delete the entries batch by batch
                    res = self.col.delete(f"{pk_name} in [{','.join(batch_pks)}]")
                    # Increment our deleted count
                    delete_count += int(res.delete_count)  # type: ignore
        except Exception as e:
            logger.error("Failed to delete by ids, error: {}".format(e))

        try:
            # Check if empty filter
            if filter is not None:
                # Convert filter to milvus expression
                filter = self._get_filter(filter)  # type: ignore
                # Check if there is anything to filter
                if len(filter) != 0:  # type: ignore
                    # Query for the pk's of entries that match filter
                    res = self.col.query(filter)  # type: ignore
                    # Convert to list of pks
                    pks = [str(entry[pk_name]) for entry in res]  # type: ignore
                    # for schema V2, the "id" is varchar, rewrite the expression
                    if self._schema_ver != "V1":
                        pks = ['"' + pk + '"' for pk in pks]
                    # Check to see if there are valid pk's to delete, delete batch by batch(avoid too long expression)
                    while len(pks) > 0:  # type: ignore
                        batch_pks = pks[:batch_size]
                        pks = pks[batch_size:]
                        # Delete the entries batch by batch
                        res = self.col.delete(f"{pk_name} in [{','.join(batch_pks)}]")  # type: ignore
                        # Increment our delete count
                        delete_count += int(res.delete_count)  # type: ignore
        except Exception as e:
            logger.error("Failed to delete by filter, error: {}".format(e))

        logger.info("{:d} records deleted".format(delete_count))

        # This setting performs flushes after delete. Small delete == bad to use
        # self.col.flush()

        return True

    def _get_filter(self, filter: DocumentMetadataFilter) -> Optional[str]:
        """Converts a DocumentMetdataFilter to the expression that Milvus takes.

        Args:
            filter (DocumentMetadataFilter): The Filter to convert to Milvus expression.

        Returns:
            Optional[str]: The filter if valid, otherwise None.
        """
        filters = []
        # Go through all the fields and their values
        for field, value in filter.dict().items():
            # Check if the Value is empty
            if value is not None:
                # Convert start_date to int and add greater than or equal logic
                if field == "start_date":
                    filters.append(
                        "(created_at >= " + str(to_unix_timestamp(value)) + ")"
                    )
                # Convert end_date to int and add less than or equal logic
                elif field == "end_date":
                    filters.append(
                        "(created_at <= " + str(to_unix_timestamp(value)) + ")"
                    )
                # Convert Source to its string value and check equivalency
                elif field == "source":
                    filters.append("(" + field + ' == "' + str(value.value) + '")')
                # Check equivalency of rest of string fields
                else:
                    filters.append("(" + field + ' == "' + str(value) + '")')
        # Join all our expressions with `and``
        return " and ".join(filters)



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/datastore/providers/pgvector_datastore.py
================================================
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional
from datetime import datetime
from loguru import logger

from services.date import to_unix_timestamp
from datastore.datastore import DataStore
from models.models import (
    DocumentChunk,
    DocumentChunkMetadata,
    DocumentMetadataFilter,
    QueryResult,
    QueryWithEmbedding,
    DocumentChunkWithScore,
)


# interface for Postgres client to implement pg based Datastore providers
class PGClient(ABC):
    @abstractmethod
    async def upsert(self, table: str, json: dict[str, Any]) -> None:
        """
        Takes in a list of documents and inserts them into the table.
        """
        raise NotImplementedError

    @abstractmethod
    async def rpc(self, function_name: str, params: dict[str, Any]) -> Any:
        """
        Calls a stored procedure in the database with the given parameters.
        """
        raise NotImplementedError

    @abstractmethod
    async def delete_like(self, table: str, column: str, pattern: str) -> None:
        """
        Deletes rows in the table that match the pattern.
        """
        raise NotImplementedError

    @abstractmethod
    async def delete_in(self, table: str, column: str, ids: List[str]) -> None:
        """
        Deletes rows in the table that match the ids.
        """
        raise NotImplementedError

    @abstractmethod
    async def delete_by_filters(
        self, table: str, filter: DocumentMetadataFilter
    ) -> None:
        """
        Deletes rows in the table that match the filter.
        """
        raise NotImplementedError


# abstract class for Postgres based Datastore providers that implements DataStore interface
class PgVectorDataStore(DataStore):
    def __init__(self):
        self.client = self.create_db_client()

    @abstractmethod
    def create_db_client(self) -> PGClient:
        """
        Create db client, can be accessing postgres database via different APIs.
        Can be supabase client or psycopg2 based client.
        Return a client for postgres DB.
        """

        raise NotImplementedError

    async def _upsert(self, chunks: Dict[str, List[DocumentChunk]]) -> List[str]:
        """
        Takes in a dict of document_ids to list of document chunks and inserts them into the database.
        Return a list of document ids.
        """
        for document_id, document_chunks in chunks.items():
            for chunk in document_chunks:
                json = {
                    "id": chunk.id,
                    "content": chunk.text,
                    "embedding": chunk.embedding,
                    "document_id": document_id,
                    "source": chunk.metadata.source,
                    "source_id": chunk.metadata.source_id,
                    "url": chunk.metadata.url,
                    "author": chunk.metadata.author,
                }
                if chunk.metadata.created_at:
                    json["created_at"] = (
                        datetime.fromtimestamp(
                            to_unix_timestamp(chunk.metadata.created_at)
                        ),
                    )
                await self.client.upsert("documents", json)

        return list(chunks.keys())

    async def _query(self, queries: List[QueryWithEmbedding]) -> List[QueryResult]:
        """
        Takes in a list of queries with embeddings and filters and returns a list of query results with matching document chunks and scores.
        """
        query_results: List[QueryResult] = []
        for query in queries:
            # get the top 3 documents with the highest cosine similarity using rpc function in the database called "match_page_sections"
            params = {
                "in_embedding": query.embedding,
            }
            if query.top_k:
                params["in_match_count"] = query.top_k
            if query.filter:
                if query.filter.document_id:
                    params["in_document_id"] = query.filter.document_id
                if query.filter.source:
                    params["in_source"] = query.filter.source.value
                if query.filter.source_id:
                    params["in_source_id"] = query.filter.source_id
                if query.filter.author:
                    params["in_author"] = query.filter.author
                if query.filter.start_date:
                    params["in_start_date"] = datetime.fromtimestamp(
                        to_unix_timestamp(query.filter.start_date)
                    )
                if query.filter.end_date:
                    params["in_end_date"] = datetime.fromtimestamp(
                        to_unix_timestamp(query.filter.end_date)
                    )
            try:
                data = await self.client.rpc("match_page_sections", params=params)
                results: List[DocumentChunkWithScore] = []
                for row in data:
                    document_chunk = DocumentChunkWithScore(
                        id=row["id"],
                        text=row["content"],
                        # TODO: add embedding to the response ?
                        # embedding=row["embedding"],
                        score=float(row["similarity"]),
                        metadata=DocumentChunkMetadata(
                            source=row["source"],
                            source_id=row["source_id"],
                            document_id=row["document_id"],
                            url=row["url"],
                            created_at=row["created_at"],
                            author=row["author"],
                        ),
                    )
                    results.append(document_chunk)
                query_results.append(QueryResult(query=query.query, results=results))
            except Exception as e:
                logger.error(e)
                query_results.append(QueryResult(query=query.query, results=[]))
        return query_results

    async def delete(
        self,
        ids: Optional[List[str]] = None,
        filter: Optional[DocumentMetadataFilter] = None,
        delete_all: Optional[bool] = None,
    ) -> bool:
        """
        Removes vectors by ids, filter, or everything in the datastore.
        Multiple parameters can be used at once.
        Returns whether the operation was successful.
        """
        if delete_all:
            try:
                await self.client.delete_like("documents", "document_id", "%")
            except:
                return False
        elif ids:
            try:
                await self.client.delete_in("documents", "document_id", ids)
            except:
                return False
        elif filter:
            try:
                await self.client.delete_by_filters("documents", filter)
            except:
                return False
        return True



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/datastore/providers/pinecone_datastore.py
================================================
import os
from typing import Any, Dict, List, Optional
import pinecone
from tenacity import retry, wait_random_exponential, stop_after_attempt
import asyncio
from loguru import logger

from datastore.datastore import DataStore
from models.models import (
    DocumentChunk,
    DocumentChunkMetadata,
    DocumentChunkWithScore,
    DocumentMetadataFilter,
    QueryResult,
    QueryWithEmbedding,
    Source,
)
from services.date import to_unix_timestamp

# Read environment variables for Pinecone configuration
PINECONE_API_KEY = os.environ.get("PINECONE_API_KEY")
PINECONE_ENVIRONMENT = os.environ.get("PINECONE_ENVIRONMENT")
PINECONE_INDEX = os.environ.get("PINECONE_INDEX")
assert PINECONE_API_KEY is not None
assert PINECONE_ENVIRONMENT is not None
assert PINECONE_INDEX is not None

# Initialize Pinecone with the API key and environment
pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)

# Set the batch size for upserting vectors to Pinecone
UPSERT_BATCH_SIZE = 100


class PineconeDataStore(DataStore):
    def __init__(self):
        # Check if the index name is specified and exists in Pinecone
        if PINECONE_INDEX and PINECONE_INDEX not in pinecone.list_indexes():

            # Get all fields in the metadata object in a list
            fields_to_index = list(DocumentChunkMetadata.__fields__.keys())

            # Create a new index with the specified name, dimension, and metadata configuration
            try:
                logger.info(
                    f"Creating index {PINECONE_INDEX} with metadata config {fields_to_index}"
                )
                pinecone.create_index(
                    PINECONE_INDEX,
                    dimension=1536,  # dimensionality of OpenAI ada v2 embeddings
                    metadata_config={"indexed": fields_to_index},
                )
                self.index = pinecone.Index(PINECONE_INDEX)
                logger.info(f"Index {PINECONE_INDEX} created successfully")
            except Exception as e:
                logger.error(f"Error creating index {PINECONE_INDEX}: {e}")
                raise e
        elif PINECONE_INDEX and PINECONE_INDEX in pinecone.list_indexes():
            # Connect to an existing index with the specified name
            try:
                logger.info(f"Connecting to existing index {PINECONE_INDEX}")
                self.index = pinecone.Index(PINECONE_INDEX)
                logger.info(f"Connected to index {PINECONE_INDEX} successfully")
            except Exception as e:
                logger.error(f"Error connecting to index {PINECONE_INDEX}: {e}")
                raise e

    @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(3))
    async def _upsert(self, chunks: Dict[str, List[DocumentChunk]]) -> List[str]:
        """
        Takes in a dict from document id to list of document chunks and inserts them into the index.
        Return a list of document ids.
        """
        # Initialize a list of ids to return
        doc_ids: List[str] = []
        # Initialize a list of vectors to upsert
        vectors = []
        # Loop through the dict items
        for doc_id, chunk_list in chunks.items():
            # Append the id to the ids list
            doc_ids.append(doc_id)
            logger.info(f"Upserting document_id: {doc_id}")
            for chunk in chunk_list:
                # Create a vector tuple of (id, embedding, metadata)
                # Convert the metadata object to a dict with unix timestamps for dates
                pinecone_metadata = self._get_pinecone_metadata(chunk.metadata)
                # Add the text and document id to the metadata dict
                pinecone_metadata["text"] = chunk.text
                pinecone_metadata["document_id"] = doc_id
                vector = (chunk.id, chunk.embedding, pinecone_metadata)
                vectors.append(vector)

        # Split the vectors list into batches of the specified size
        batches = [
            vectors[i : i + UPSERT_BATCH_SIZE]
            for i in range(0, len(vectors), UPSERT_BATCH_SIZE)
        ]
        # Upsert each batch to Pinecone
        for batch in batches:
            try:
                logger.info(f"Upserting batch of size {len(batch)}")
                self.index.upsert(vectors=batch)
                logger.info(f"Upserted batch successfully")
            except Exception as e:
                logger.error(f"Error upserting batch: {e}")
                raise e

        return doc_ids

    @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(3))
    async def _query(
        self,
        queries: List[QueryWithEmbedding],
    ) -> List[QueryResult]:
        """
        Takes in a list of queries with embeddings and filters and returns a list of query results with matching document chunks and scores.
        """

        # Define a helper coroutine that performs a single query and returns a QueryResult
        async def _single_query(query: QueryWithEmbedding) -> QueryResult:
            logger.debug(f"Query: {query.query}")

            # Convert the metadata filter object to a dict with pinecone filter expressions
            pinecone_filter = self._get_pinecone_filter(query.filter)

            try:
                # Query the index with the query embedding, filter, and top_k
                query_response = self.index.query(
                    # namespace=namespace,
                    top_k=query.top_k,
                    vector=query.embedding,
                    filter=pinecone_filter,
                    include_metadata=True,
                )
            except Exception as e:
                logger.error(f"Error querying index: {e}")
                raise e

            query_results: List[DocumentChunkWithScore] = []
            for result in query_response.matches:
                score = result.score
                metadata = result.metadata
                # Remove document id and text from metadata and store it in a new variable
                metadata_without_text = (
                    {key: value for key, value in metadata.items() if key != "text"}
                    if metadata
                    else None
                )

                # If the source is not a valid Source in the Source enum, set it to None
                if (
                    metadata_without_text
                    and "source" in metadata_without_text
                    and metadata_without_text["source"] not in Source.__members__
                ):
                    metadata_without_text["source"] = None

                # Create a document chunk with score object with the result data
                result = DocumentChunkWithScore(
                    id=result.id,
                    score=score,
                    text=metadata["text"] if metadata and "text" in metadata else None,
                    metadata=metadata_without_text,
                )
                query_results.append(result)
            return QueryResult(query=query.query, results=query_results)

        # Use asyncio.gather to run multiple _single_query coroutines concurrently and collect their results
        results: List[QueryResult] = await asyncio.gather(
            *[_single_query(query) for query in queries]
        )

        return results

    @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(3))
    async def delete(
        self,
        ids: Optional[List[str]] = None,
        filter: Optional[DocumentMetadataFilter] = None,
        delete_all: Optional[bool] = None,
    ) -> bool:
        """
        Removes vectors by ids, filter, or everything from the index.
        """
        # Delete all vectors from the index if delete_all is True
        if delete_all:
            try:
                logger.info(f"Deleting all vectors from index")
                self.index.delete(delete_all=True)
                logger.info(f"Deleted all vectors successfully")
                return True
            except Exception as e:
                logger.error(f"Error deleting all vectors: {e}")
                raise e

        # Convert the metadata filter object to a dict with pinecone filter expressions
        pinecone_filter = self._get_pinecone_filter(filter)
        # Delete vectors that match the filter from the index if the filter is not empty
        if pinecone_filter != {}:
            try:
                logger.info(f"Deleting vectors with filter {pinecone_filter}")
                self.index.delete(filter=pinecone_filter)
                logger.info(f"Deleted vectors with filter successfully")
            except Exception as e:
                logger.error(f"Error deleting vectors with filter: {e}")
                raise e

        # Delete vectors that match the document ids from the index if the ids list is not empty
        if ids is not None and len(ids) > 0:
            try:
                logger.info(f"Deleting vectors with ids {ids}")
                pinecone_filter = {"document_id": {"$in": ids}}
                self.index.delete(filter=pinecone_filter)  # type: ignore
                logger.info(f"Deleted vectors with ids successfully")
            except Exception as e:
                logger.error(f"Error deleting vectors with ids: {e}")
                raise e

        return True

    def _get_pinecone_filter(
        self, filter: Optional[DocumentMetadataFilter] = None
    ) -> Dict[str, Any]:
        if filter is None:
            return {}

        pinecone_filter = {}

        # For each field in the MetadataFilter, check if it has a value and add the corresponding pinecone filter expression
        # For start_date and end_date, uses the $gte and $lte operators respectively
        # For other fields, uses the $eq operator
        for field, value in filter.dict().items():
            if value is not None:
                if field == "start_date":
                    pinecone_filter["created_at"] = pinecone_filter.get("created_at", {})
                    pinecone_filter["created_at"]["$gte"] = to_unix_timestamp(value)
                elif field == "end_date":
                    pinecone_filter["created_at"] = pinecone_filter.get("created_at", {})
                    pinecone_filter["created_at"]["$lte"] = to_unix_timestamp(value)
                else:
                    pinecone_filter[field] = value

        return pinecone_filter

    def _get_pinecone_metadata(
        self, metadata: Optional[DocumentChunkMetadata] = None
    ) -> Dict[str, Any]:
        if metadata is None:
            return {}

        pinecone_metadata = {}

        # For each field in the Metadata, check if it has a value and add it to the pinecone metadata dict
        # For fields that are dates, convert them to unix timestamps
        for field, value in metadata.dict().items():
            if value is not None:
                if field in ["created_at"]:
                    pinecone_metadata[field] = to_unix_timestamp(value)
                else:
                    pinecone_metadata[field] = value

        return pinecone_metadata



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/datastore/providers/postgres_datastore.py
================================================
import os
from typing import Any, List
from datetime import datetime
import numpy as np

from psycopg2 import connect
from psycopg2.extras import DictCursor
from pgvector.psycopg2 import register_vector

from services.date import to_unix_timestamp
from datastore.providers.pgvector_datastore import PGClient, PgVectorDataStore
from models.models import (
    DocumentMetadataFilter,
)

PG_HOST = os.environ.get("PG_HOST", "localhost")
PG_PORT = int(os.environ.get("PG_PORT", 5432))
PG_DB = os.environ.get("PG_DB", "postgres")
PG_USER = os.environ.get("PG_USER", "postgres")
PG_PASSWORD = os.environ.get("PG_PASSWORD", "postgres")


# class that implements the DataStore interface for Postgres Datastore provider
class PostgresDataStore(PgVectorDataStore):
    def create_db_client(self):
        return PostgresClient()


class PostgresClient(PGClient):
    def __init__(self) -> None:
        super().__init__()
        self.client = connect(
            dbname=PG_DB, user=PG_USER, password=PG_PASSWORD, host=PG_HOST, port=PG_PORT
        )
        register_vector(self.client)

    def __del__(self):
        # close the connection when the client is destroyed
        self.client.close()

    async def upsert(self, table: str, json: dict[str, Any]):
        """
        Takes in a list of documents and inserts them into the table.
        """
        with self.client.cursor() as cur:
            if not json.get("created_at"):
                json["created_at"] = datetime.now()
            json["embedding"] = np.array(json["embedding"])
            cur.execute(
                f"INSERT INTO {table} (id, content, embedding, document_id, source, source_id, url, author, created_at) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s) ON CONFLICT (id) DO UPDATE SET content = %s, embedding = %s, document_id = %s, source = %s, source_id = %s, url = %s, author = %s, created_at = %s",
                (
                    json["id"],
                    json["content"],
                    json["embedding"],
                    json["document_id"],
                    json["source"],
                    json["source_id"],
                    json["url"],
                    json["author"],
                    json["created_at"],
                    json["content"],
                    json["embedding"],
                    json["document_id"],
                    json["source"],
                    json["source_id"],
                    json["url"],
                    json["author"],
                    json["created_at"],
                ),
            )
            self.client.commit()

    async def rpc(self, function_name: str, params: dict[str, Any]):
        """
        Calls a stored procedure in the database with the given parameters.
        """
        data = []
        params["in_embedding"] = np.array(params["in_embedding"])
        with self.client.cursor(cursor_factory=DictCursor) as cur:
            cur.callproc(function_name, params)
            rows = cur.fetchall()
            self.client.commit()
            for row in rows:
                row["created_at"] = to_unix_timestamp(row["created_at"])
                data.append(dict(row))
        return data

    async def delete_like(self, table: str, column: str, pattern: str):
        """
        Deletes rows in the table that match the pattern.
        """
        with self.client.cursor() as cur:
            cur.execute(
                f"DELETE FROM {table} WHERE {column} LIKE %s",
                (f"%{pattern}%",),
            )
            self.client.commit()

    async def delete_in(self, table: str, column: str, ids: List[str]):
        """
        Deletes rows in the table that match the ids.
        """
        with self.client.cursor() as cur:
            cur.execute(
                f"DELETE FROM {table} WHERE {column} IN %s",
                (tuple(ids),),
            )
            self.client.commit()

    async def delete_by_filters(self, table: str, filter: DocumentMetadataFilter):
        """
        Deletes rows in the table that match the filter.
        """

        filters = "WHERE"
        if filter.document_id:
            filters += f" document_id = '{filter.document_id}' AND"
        if filter.source:
            filters += f" source = '{filter.source}' AND"
        if filter.source_id:
            filters += f" source_id = '{filter.source_id}' AND"
        if filter.author:
            filters += f" author = '{filter.author}' AND"
        if filter.start_date:
            filters += f" created_at >= '{filter.start_date}' AND"
        if filter.end_date:
            filters += f" created_at <= '{filter.end_date}' AND"
        filters = filters[:-4]

        with self.client.cursor() as cur:
            cur.execute(f"DELETE FROM {table} {filters}")
            self.client.commit()



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/datastore/providers/qdrant_datastore.py
================================================
import os
import uuid
from typing import Dict, List, Optional

from grpc._channel import _InactiveRpcError
from qdrant_client.http.exceptions import UnexpectedResponse
from qdrant_client.http.models import PayloadSchemaType

from datastore.datastore import DataStore
from models.models import (
    DocumentChunk,
    DocumentMetadataFilter,
    QueryResult,
    QueryWithEmbedding,
    DocumentChunkWithScore,
)
from qdrant_client.http import models as rest

import qdrant_client

from services.date import to_unix_timestamp

QDRANT_URL = os.environ.get("QDRANT_URL", "http://localhost")
QDRANT_PORT = os.environ.get("QDRANT_PORT", "6333")
QDRANT_GRPC_PORT = os.environ.get("QDRANT_GRPC_PORT", "6334")
QDRANT_API_KEY = os.environ.get("QDRANT_API_KEY")
QDRANT_COLLECTION = os.environ.get("QDRANT_COLLECTION", "document_chunks")


class QdrantDataStore(DataStore):
    UUID_NAMESPACE = uuid.UUID("3896d314-1e95-4a3a-b45a-945f9f0b541d")

    def __init__(
        self,
        collection_name: Optional[str] = None,
        vector_size: int = 1536,
        distance: str = "Cosine",
        recreate_collection: bool = False,
    ):
        """
        Args:
            collection_name: Name of the collection to be used
            vector_size: Size of the embedding stored in a collection
            distance:
                Any of "Cosine" / "Euclid" / "Dot". Distance function to measure
                similarity
        """
        self.client = qdrant_client.QdrantClient(
            url=QDRANT_URL,
            port=int(QDRANT_PORT),
            grpc_port=int(QDRANT_GRPC_PORT),
            api_key=QDRANT_API_KEY,
            prefer_grpc=True,
            timeout=10,
        )
        self.collection_name = collection_name or QDRANT_COLLECTION

        # Set up the collection so the points might be inserted or queried
        self._set_up_collection(vector_size, distance, recreate_collection)

    async def _upsert(self, chunks: Dict[str, List[DocumentChunk]]) -> List[str]:
        """
        Takes in a list of document chunks and inserts them into the database.
        Return a list of document ids.
        """
        points = [
            self._convert_document_chunk_to_point(chunk)
            for _, chunks in chunks.items()
            for chunk in chunks
        ]
        self.client.upsert(
            collection_name=self.collection_name,
            points=points,  # type: ignore
            wait=True,
        )
        return list(chunks.keys())

    async def _query(
        self,
        queries: List[QueryWithEmbedding],
    ) -> List[QueryResult]:
        """
        Takes in a list of queries with embeddings and filters and returns a list of query results with matching document chunks and scores.
        """
        search_requests = [
            self._convert_query_to_search_request(query) for query in queries
        ]
        results = self.client.search_batch(
            collection_name=self.collection_name,
            requests=search_requests,
        )
        return [
            QueryResult(
                query=query.query,
                results=[
                    self._convert_scored_point_to_document_chunk_with_score(point)
                    for point in result
                ],
            )
            for query, result in zip(queries, results)
        ]

    async def delete(
        self,
        ids: Optional[List[str]] = None,
        filter: Optional[DocumentMetadataFilter] = None,
        delete_all: Optional[bool] = None,
    ) -> bool:
        """
        Removes vectors by ids, filter, or everything in the datastore.
        Returns whether the operation was successful.
        """
        if ids is None and filter is None and delete_all is None:
            raise ValueError(
                "Please provide one of the parameters: ids, filter or delete_all."
            )

        if delete_all:
            points_selector = rest.Filter()
        else:
            points_selector = self._convert_metadata_filter_to_qdrant_filter(
                filter, ids
            )

        response = self.client.delete(
            collection_name=self.collection_name,
            points_selector=points_selector,  # type: ignore
        )
        return "COMPLETED" == response.status

    def _convert_document_chunk_to_point(
        self, document_chunk: DocumentChunk
    ) -> rest.PointStruct:
        created_at = (
            to_unix_timestamp(document_chunk.metadata.created_at)
            if document_chunk.metadata.created_at is not None
            else None
        )
        return rest.PointStruct(
            id=self._create_document_chunk_id(document_chunk.id),
            vector=document_chunk.embedding,  # type: ignore
            payload={
                "id": document_chunk.id,
                "text": document_chunk.text,
                "metadata": document_chunk.metadata.dict(),
                "created_at": created_at,
            },
        )

    def _create_document_chunk_id(self, external_id: Optional[str]) -> str:
        if external_id is None:
            return uuid.uuid4().hex
        return uuid.uuid5(self.UUID_NAMESPACE, external_id).hex

    def _convert_query_to_search_request(
        self, query: QueryWithEmbedding
    ) -> rest.SearchRequest:
        return rest.SearchRequest(
            vector=query.embedding,
            filter=self._convert_metadata_filter_to_qdrant_filter(query.filter),
            limit=query.top_k,  # type: ignore
            with_payload=True,
            with_vector=False,
        )

    def _convert_metadata_filter_to_qdrant_filter(
        self,
        metadata_filter: Optional[DocumentMetadataFilter] = None,
        ids: Optional[List[str]] = None,
    ) -> Optional[rest.Filter]:
        if metadata_filter is None and ids is None:
            return None

        must_conditions, should_conditions = [], []

        # Filtering by document ids
        if ids and len(ids) > 0:
            for document_id in ids:
                should_conditions.append(
                    rest.FieldCondition(
                        key="metadata.document_id",
                        match=rest.MatchValue(value=document_id),
                    )
                )

        # Equality filters for the payload attributes
        if metadata_filter:
            meta_attributes_keys = {
                "document_id": "metadata.document_id",
                "source": "metadata.source",
                "source_id": "metadata.source_id",
                "author": "metadata.author",
            }

            for meta_attr_name, payload_key in meta_attributes_keys.items():
                attr_value = getattr(metadata_filter, meta_attr_name)
                if attr_value is None:
                    continue

                must_conditions.append(
                    rest.FieldCondition(
                        key=payload_key, match=rest.MatchValue(value=attr_value)
                    )
                )

            # Date filters use range filtering
            start_date = metadata_filter.start_date
            end_date = metadata_filter.end_date
            if start_date or end_date:
                gte_filter = (
                    to_unix_timestamp(start_date) if start_date is not None else None
                )
                lte_filter = (
                    to_unix_timestamp(end_date) if end_date is not None else None
                )
                must_conditions.append(
                    rest.FieldCondition(
                        key="created_at",
                        range=rest.Range(
                            gte=gte_filter,
                            lte=lte_filter,
                        ),
                    )
                )

        if 0 == len(must_conditions) and 0 == len(should_conditions):
            return None

        return rest.Filter(must=must_conditions, should=should_conditions)

    def _convert_scored_point_to_document_chunk_with_score(
        self, scored_point: rest.ScoredPoint
    ) -> DocumentChunkWithScore:
        payload = scored_point.payload or {}
        return DocumentChunkWithScore(
            id=payload.get("id"),
            text=scored_point.payload.get("text"),  # type: ignore
            metadata=scored_point.payload.get("metadata"),  # type: ignore
            embedding=scored_point.vector,  # type: ignore
            score=scored_point.score,
        )

    def _set_up_collection(
        self, vector_size: int, distance: str, recreate_collection: bool
    ):
        distance = rest.Distance[distance.upper()]

        if recreate_collection:
            self._recreate_collection(distance, vector_size)

        try:
            collection_info = self.client.get_collection(self.collection_name)
            current_distance = collection_info.config.params.vectors.distance  # type: ignore
            current_vector_size = collection_info.config.params.vectors.size  # type: ignore

            if current_distance != distance:
                raise ValueError(
                    f"Collection '{self.collection_name}' already exists in Qdrant, "
                    f"but it is configured with a similarity '{current_distance.name}'. "
                    f"If you want to use that collection, but with a different "
                    f"similarity, please set `recreate_collection=True` argument."
                )

            if current_vector_size != vector_size:
                raise ValueError(
                    f"Collection '{self.collection_name}' already exists in Qdrant, "
                    f"but it is configured with a vector size '{current_vector_size}'. "
                    f"If you want to use that collection, but with a different "
                    f"vector size, please set `recreate_collection=True` argument."
                )
        except (UnexpectedResponse, _InactiveRpcError):
            self._recreate_collection(distance, vector_size)

    def _recreate_collection(self, distance: rest.Distance, vector_size: int):
        self.client.recreate_collection(
            self.collection_name,
            vectors_config=rest.VectorParams(
                size=vector_size,
                distance=distance,
            ),
        )

        # Create the payload index for the document_id metadata attribute, as it is
        # used to delete the document related entries
        self.client.create_payload_index(
            self.collection_name,
            field_name="metadata.document_id",
            field_type=PayloadSchemaType.KEYWORD,
        )

        # Create the payload index for the created_at attribute, to make the lookup
        # by range filters faster
        self.client.create_payload_index(
            self.collection_name,
            field_name="created_at",
            field_schema=PayloadSchemaType.INTEGER,
        )



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/datastore/providers/redis_datastore.py
================================================
import asyncio
import os
import re
import json
import redis.asyncio as redis
import numpy as np

from redis.commands.search.query import Query as RediSearchQuery
from redis.commands.search.indexDefinition import IndexDefinition, IndexType
from redis.commands.search.field import (
    TagField,
    TextField,
    NumericField,
    VectorField,
)
from loguru import logger
from typing import Dict, List, Optional
from datastore.datastore import DataStore
from models.models import (
    DocumentChunk,
    DocumentMetadataFilter,
    DocumentChunkWithScore,
    DocumentMetadataFilter,
    QueryResult,
    QueryWithEmbedding,
)
from services.date import to_unix_timestamp

# Read environment variables for Redis
REDIS_HOST = os.environ.get("REDIS_HOST", "localhost")
REDIS_PORT = int(os.environ.get("REDIS_PORT", 6379))
REDIS_PASSWORD = os.environ.get("REDIS_PASSWORD")
REDIS_INDEX_NAME = os.environ.get("REDIS_INDEX_NAME", "index")
REDIS_DOC_PREFIX = os.environ.get("REDIS_DOC_PREFIX", "doc")
REDIS_DISTANCE_METRIC = os.environ.get("REDIS_DISTANCE_METRIC", "COSINE")
REDIS_INDEX_TYPE = os.environ.get("REDIS_INDEX_TYPE", "FLAT")
assert REDIS_INDEX_TYPE in ("FLAT", "HNSW")

# OpenAI Ada Embeddings Dimension
VECTOR_DIMENSION = 1536

# RediSearch constants
REDIS_REQUIRED_MODULES = [
    {"name": "search", "ver": 20600},
    {"name": "ReJSON", "ver": 20404}
]

REDIS_DEFAULT_ESCAPED_CHARS = re.compile(r"[,.<>{}\[\]\\\"\':;!@#$%^&()\-+=~\/ ]")

# Helper functions
def unpack_schema(d: dict):
    for v in d.values():
        if isinstance(v, dict):
            yield from unpack_schema(v)
        else:
            yield v

async def _check_redis_module_exist(client: redis.Redis, modules: List[dict]):
    installed_modules = (await client.info()).get("modules", [])
    installed_modules = {module["name"]: module for module in installed_modules}
    for module in modules:
        if module["name"] not in installed_modules or int(installed_modules[module["name"]]["ver"]) < int(module["ver"]):
            error_message = "You must add the RediSearch (>= 2.6) and ReJSON (>= 2.4) modules from Redis Stack. " \
                "Please refer to Redis Stack docs: https://redis.io/docs/stack/"
            logger.error(error_message)
            raise AttributeError(error_message)


class RedisDataStore(DataStore):
    def __init__(self, client: redis.Redis, redisearch_schema: dict):
        self.client = client
        self._schema = redisearch_schema
        # Init default metadata with sentinel values in case the document written has no metadata
        self._default_metadata = {
            field: (0 if field == "created_at" else "_null_") for field in redisearch_schema["metadata"]
        }

    ### Redis Helper Methods ###

    @classmethod
    async def init(cls, **kwargs):
        """
        Setup the index if it does not exist.
        """
        try:
            # Connect to the Redis Client
            logger.info("Connecting to Redis")
            client = redis.Redis(
                host=REDIS_HOST, port=REDIS_PORT, password=REDIS_PASSWORD
            )
        except Exception as e:
            logger.error(f"Error setting up Redis: {e}")
            raise e

        await _check_redis_module_exist(client, modules=REDIS_REQUIRED_MODULES)

        dim = kwargs.get("dim", VECTOR_DIMENSION)
        redisearch_schema = {
            "metadata": {
                "document_id": TagField("$.metadata.document_id", as_name="document_id"),
                "source_id": TagField("$.metadata.source_id", as_name="source_id"),
                "source": TagField("$.metadata.source", as_name="source"),
                "author": TextField("$.metadata.author", as_name="author"),
                "created_at": NumericField("$.metadata.created_at", as_name="created_at"),
            },
            "embedding": VectorField(
                "$.embedding",
                REDIS_INDEX_TYPE,
                {
                    "TYPE": "FLOAT64",
                    "DIM": dim,
                    "DISTANCE_METRIC": REDIS_DISTANCE_METRIC,
                },
                as_name="embedding",
            ),
        }
        try:
            # Check for existence of RediSearch Index
            await client.ft(REDIS_INDEX_NAME).info()
            logger.info(f"RediSearch index {REDIS_INDEX_NAME} already exists")
        except:
            # Create the RediSearch Index
            logger.info(f"Creating new RediSearch index {REDIS_INDEX_NAME}")
            definition = IndexDefinition(
                prefix=[REDIS_DOC_PREFIX], index_type=IndexType.JSON
            )
            fields = list(unpack_schema(redisearch_schema))
            logger.info(f"Creating index with fields: {fields}")
            await client.ft(REDIS_INDEX_NAME).create_index(
                fields=fields, definition=definition
            )
        return cls(client, redisearch_schema)

    @staticmethod
    def _redis_key(document_id: str, chunk_id: str) -> str:
        """
        Create the JSON key for document chunks in Redis.

        Args:
            document_id (str): Document Identifier
            chunk_id (str): Chunk Identifier

        Returns:
            str: JSON key string.
        """
        return f"doc:{document_id}:chunk:{chunk_id}"

    @staticmethod
    def _escape(value: str) -> str:
        """
        Escape filter value.

        Args:
            value (str): Value to escape.

        Returns:
            str: Escaped filter value for RediSearch.
        """

        def escape_symbol(match) -> str:
            value = match.group(0)
            return f"\\{value}"

        return REDIS_DEFAULT_ESCAPED_CHARS.sub(escape_symbol, value)

    def _get_redis_chunk(self, chunk: DocumentChunk) -> dict:
        """
        Convert DocumentChunk into a JSON object for storage
        in Redis.

        Args:
            chunk (DocumentChunk): Chunk of a Document.

        Returns:
            dict: JSON object for storage in Redis.
        """
        # Convert chunk -> dict
        data = chunk.__dict__
        metadata = chunk.metadata.__dict__
        data["chunk_id"] = data.pop("id")

        # Prep Redis Metadata
        redis_metadata = dict(self._default_metadata)
        if metadata:
            for field, value in metadata.items():
                if value:
                    if field == "created_at":
                        redis_metadata[field] = to_unix_timestamp(value)  # type: ignore
                    else:
                        redis_metadata[field] = value
        data["metadata"] = redis_metadata
        return data

    def _get_redis_query(self, query: QueryWithEmbedding) -> RediSearchQuery:
        """
        Convert a QueryWithEmbedding into a RediSearchQuery.

        Args:
            query (QueryWithEmbedding): Search query.

        Returns:
            RediSearchQuery: Query for RediSearch.
        """
        filter_str: str = ""

        # RediSearch field type to query string
        def _typ_to_str(typ, field, value) -> str:  # type: ignore
            if isinstance(typ, TagField):
                return f"@{field}:{{{self._escape(value)}}} "
            elif isinstance(typ, TextField):
                return f"@{field}:{value} "
            elif isinstance(typ, NumericField):
                num = to_unix_timestamp(value)
                match field:
                    case "start_date":
                        return f"@{field}:[{num} +inf] "
                    case "end_date":
                        return f"@{field}:[-inf {num}] "

        # Build filter
        if query.filter:
            redisearch_schema = self._schema
            for field, value in query.filter.__dict__.items():
                if not value:
                    continue
                if field in redisearch_schema:
                    filter_str += _typ_to_str(redisearch_schema[field], field, value)
                elif field in redisearch_schema["metadata"]:
                    if field == "source":  # handle the enum
                        value = value.value
                    filter_str += _typ_to_str(
                        redisearch_schema["metadata"][field], field, value
                    )
                elif field in ["start_date", "end_date"]:
                    filter_str += _typ_to_str(
                        redisearch_schema["metadata"]["created_at"], field, value
                    )

        # Postprocess filter string
        filter_str = filter_str.strip()
        filter_str = filter_str if filter_str else "*"

        # Prepare query string
        query_str = (
            f"({filter_str})=>[KNN {query.top_k} @embedding $embedding as score]"
        )
        return (
            RediSearchQuery(query_str)
            .sort_by("score")
            .paging(0, query.top_k)
            .dialect(2)
        )

    async def _redis_delete(self, keys: List[str]):
        """
        Delete a list of keys from Redis.

        Args:
            keys (List[str]): List of keys to delete.
        """
        # Delete the keys
        await asyncio.gather(*[self.client.delete(key) for key in keys])

    #######

    async def _upsert(self, chunks: Dict[str, List[DocumentChunk]]) -> List[str]:
        """
        Takes in a list of list of document chunks and inserts them into the database.
        Return a list of document ids.
        """
        # Initialize a list of ids to return
        doc_ids: List[str] = []

        # Loop through the dict items
        for doc_id, chunk_list in chunks.items():

            # Append the id to the ids list
            doc_ids.append(doc_id)

            # Write chunks in a pipelines
            async with self.client.pipeline(transaction=False) as pipe:
                for chunk in chunk_list:
                    key = self._redis_key(doc_id, chunk.id)
                    data = self._get_redis_chunk(chunk)
                    await pipe.json().set(key, "$", data)
                await pipe.execute()

        return doc_ids

    async def _query(
        self,
        queries: List[QueryWithEmbedding],
    ) -> List[QueryResult]:
        """
        Takes in a list of queries with embeddings and filters and
        returns a list of query results with matching document chunks and scores.
        """
        # Prepare query responses and results object
        results: List[QueryResult] = []

        # Gather query results in a pipeline
        logger.info(f"Gathering {len(queries)} query results")
        for query in queries:

            logger.debug(f"Query: {query.query}")
            query_results: List[DocumentChunkWithScore] = []

            # Extract Redis query
            redis_query: RediSearchQuery = self._get_redis_query(query)
            embedding = np.array(query.embedding, dtype=np.float64).tobytes()

            # Perform vector search
            query_response = await self.client.ft(REDIS_INDEX_NAME).search(
                redis_query, {"embedding": embedding}
            )

            # Iterate through the most similar documents
            for doc in query_response.docs:
                # Load JSON data
                doc_json = json.loads(doc.json)
                # Create document chunk object with score
                result = DocumentChunkWithScore(
                    id=doc_json["metadata"]["document_id"],
                    score=doc.score,
                    text=doc_json["text"],
                    metadata=doc_json["metadata"]
                )
                query_results.append(result)

            # Add to overall results
            results.append(QueryResult(query=query.query, results=query_results))

        return results

    async def _find_keys(self, pattern: str) -> List[str]:
        return [key async for key in self.client.scan_iter(pattern)]

    async def delete(
        self,
        ids: Optional[List[str]] = None,
        filter: Optional[DocumentMetadataFilter] = None,
        delete_all: Optional[bool] = None,
    ) -> bool:
        """
        Removes vectors by ids, filter, or everything in the datastore.
        Returns whether the operation was successful.
        """
        # Delete all vectors from the index if delete_all is True
        if delete_all:
            try:
                logger.info(f"Deleting all documents from index")
                await self.client.ft(REDIS_INDEX_NAME).dropindex(True)
                logger.info(f"Deleted all documents successfully")
                return True
            except Exception as e:
                logger.error(f"Error deleting all documents: {e}")
                raise e

        # Delete by filter
        if filter:
            # TODO - extend this to work with other metadata filters?
            if filter.document_id:
                try:
                    keys = await self._find_keys(
                        f"{REDIS_DOC_PREFIX}:{filter.document_id}:*"
                    )
                    await self._redis_delete(keys)
                    logger.info(f"Deleted document {filter.document_id} successfully")
                except Exception as e:
                    logger.error(f"Error deleting document {filter.document_id}: {e}")
                    raise e

        # Delete by explicit ids (Redis keys)
        if ids:
            try:
                logger.info(f"Deleting document ids {ids}")
                keys = []
                # find all keys associated with the document ids
                for document_id in ids:
                    doc_keys = await self._find_keys(
                        pattern=f"{REDIS_DOC_PREFIX}:{document_id}:*"
                    )
                    keys.extend(doc_keys)
                # delete all keys
                logger.info(f"Deleting {len(keys)} keys from Redis")
                await self._redis_delete(keys)
            except Exception as e:
                logger.error(f"Error deleting ids: {e}")
                raise e

        return True



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/datastore/providers/supabase_datastore.py
================================================
import os
from typing import Any, List
from datetime import datetime

from supabase import Client

from datastore.providers.pgvector_datastore import PGClient, PgVectorDataStore
from models.models import (
    DocumentMetadataFilter,
)

SUPABASE_URL = os.environ.get("SUPABASE_URL")
assert SUPABASE_URL is not None, "SUPABASE_URL is not set"
SUPABASE_ANON_KEY = os.environ.get("SUPABASE_ANON_KEY")
# use service role key if you want this app to be able to bypass your Row Level Security policies
SUPABASE_SERVICE_ROLE_KEY = os.environ.get("SUPABASE_SERVICE_ROLE_KEY")
assert (
    SUPABASE_ANON_KEY is not None or SUPABASE_SERVICE_ROLE_KEY is not None
), "SUPABASE_ANON_KEY or SUPABASE_SERVICE_ROLE_KEY must be set"


# class that implements the DataStore interface for Supabase Datastore provider
class SupabaseDataStore(PgVectorDataStore):
    def create_db_client(self):
        return SupabaseClient()


class SupabaseClient(PGClient):
    def __init__(self) -> None:
        super().__init__()
        if not SUPABASE_SERVICE_ROLE_KEY:
            self.client = Client(SUPABASE_URL, SUPABASE_ANON_KEY)
        else:
            self.client = Client(SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY)

    async def upsert(self, table: str, json: dict[str, Any]):
        """
        Takes in a list of documents and inserts them into the table.
        """
        if "created_at" in json:
            json["created_at"] = json["created_at"][0].isoformat()

        self.client.table(table).upsert(json).execute()

    async def rpc(self, function_name: str, params: dict[str, Any]):
        """
        Calls a stored procedure in the database with the given parameters.
        """
        if "in_start_date" in params:
            params["in_start_date"] = params["in_start_date"].isoformat()
        if "in_end_date" in params:
            params["in_end_date"] = params["in_end_date"].isoformat()

        response = self.client.rpc(function_name, params=params).execute()
        return response.data

    async def delete_like(self, table: str, column: str, pattern: str):
        """
        Deletes rows in the table that match the pattern.
        """
        self.client.table(table).delete().like(column, pattern).execute()

    async def delete_in(self, table: str, column: str, ids: List[str]):
        """
        Deletes rows in the table that match the ids.
        """
        self.client.table(table).delete().in_(column, ids).execute()

    async def delete_by_filters(self, table: str, filter: DocumentMetadataFilter):
        """
        Deletes rows in the table that match the filter.
        """
        builder = self.client.table(table).delete()
        if filter.document_id:
            builder = builder.eq(
                "document_id",
                filter.document_id,
            )
        if filter.source:
            builder = builder.eq("source", filter.source)
        if filter.source_id:
            builder = builder.eq("source_id", filter.source_id)
        if filter.author:
            builder = builder.eq("author", filter.author)
        if filter.start_date:
            builder = builder.gte(
                "created_at",
                filter.start_date[0].isoformat(),
            )
        if filter.end_date:
            builder = builder.lte(
                "created_at",
                filter.end_date[0].isoformat(),
            )
        builder.execute()



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/datastore/providers/weaviate_datastore.py
================================================
import asyncio
import os
import re
import uuid
from typing import Dict, List, Optional

import weaviate
from loguru import logger
from weaviate import Client
from weaviate.util import generate_uuid5

from datastore.datastore import DataStore
from models.models import (
    DocumentChunk,
    DocumentChunkMetadata,
    DocumentChunkWithScore,
    DocumentMetadataFilter,
    QueryResult,
    QueryWithEmbedding,
    Source,
)

WEAVIATE_URL_DEFAULT = "http://localhost:8080"
WEAVIATE_CLASS = os.environ.get("WEAVIATE_CLASS", "OpenAIDocument")

WEAVIATE_BATCH_SIZE = int(os.environ.get("WEAVIATE_BATCH_SIZE", 20))
WEAVIATE_BATCH_DYNAMIC = os.environ.get("WEAVIATE_BATCH_DYNAMIC", False)
WEAVIATE_BATCH_TIMEOUT_RETRIES = int(os.environ.get("WEAVIATE_TIMEOUT_RETRIES", 3))
WEAVIATE_BATCH_NUM_WORKERS = int(os.environ.get("WEAVIATE_BATCH_NUM_WORKERS", 1))

SCHEMA = {
    "class": WEAVIATE_CLASS,
    "description": "The main class",
    "properties": [
        {
            "name": "chunk_id",
            "dataType": ["string"],
            "description": "The chunk id",
        },
        {
            "name": "document_id",
            "dataType": ["string"],
            "description": "The document id",
        },
        {
            "name": "text",
            "dataType": ["text"],
            "description": "The chunk's text",
        },
        {
            "name": "source",
            "dataType": ["string"],
            "description": "The source of the data",
        },
        {
            "name": "source_id",
            "dataType": ["string"],
            "description": "The source id",
        },
        {
            "name": "url",
            "dataType": ["string"],
            "description": "The source url",
        },
        {
            "name": "created_at",
            "dataType": ["date"],
            "description": "Creation date of document",
        },
        {
            "name": "author",
            "dataType": ["string"],
            "description": "Document author",
        },
    ],
}


def extract_schema_properties(schema):
    properties = schema["properties"]

    return {property["name"] for property in properties}


class WeaviateDataStore(DataStore):
    def handle_errors(self, results: Optional[List[dict]]) -> List[str]:
        if not self or not results:
            return []

        error_messages = []
        for result in results:
            if (
                "result" not in result
                or "errors" not in result["result"]
                or "error" not in result["result"]["errors"]
            ):
                continue
            for message in result["result"]["errors"]["error"]:
                error_messages.append(message["message"])
                logger.error(message["message"])

        return error_messages

    def __init__(self):
        auth_credentials = self._build_auth_credentials()

        url = os.environ.get("WEAVIATE_URL", WEAVIATE_URL_DEFAULT)

        logger.debug(
            f"Connecting to weaviate instance at {url} with credential type {type(auth_credentials).__name__}"
        )
        self.client = Client(url, auth_client_secret=auth_credentials)
        self.client.batch.configure(
            batch_size=WEAVIATE_BATCH_SIZE,
            dynamic=WEAVIATE_BATCH_DYNAMIC,  # type: ignore
            callback=self.handle_errors,  # type: ignore
            timeout_retries=WEAVIATE_BATCH_TIMEOUT_RETRIES,
            num_workers=WEAVIATE_BATCH_NUM_WORKERS,
        )

        if self.client.schema.contains(SCHEMA):
            current_schema = self.client.schema.get(WEAVIATE_CLASS)
            current_schema_properties = extract_schema_properties(current_schema)

            logger.debug(
                f"Found index {WEAVIATE_CLASS} with properties {current_schema_properties}"
            )
            logger.debug("Will reuse this schema")
        else:
            new_schema_properties = extract_schema_properties(SCHEMA)
            logger.debug(
                f"Creating collection {WEAVIATE_CLASS} with properties {new_schema_properties}"
            )
            self.client.schema.create_class(SCHEMA)

    @staticmethod
    def _build_auth_credentials():
        url = os.environ.get("WEAVIATE_URL", WEAVIATE_URL_DEFAULT)

        if WeaviateDataStore._is_wcs_domain(url):
            api_key = os.environ.get("WEAVIATE_API_KEY")
            if api_key is not None:
                return weaviate.auth.AuthApiKey(api_key=api_key)
            else:
                raise ValueError("WEAVIATE_API_KEY environment variable is not set")
        else:
            return None

    async def _upsert(self, chunks: Dict[str, List[DocumentChunk]]) -> List[str]:
        """
        Takes in a list of list of document chunks and inserts them into the database.
        Return a list of document ids.
        """
        doc_ids = []

        with self.client.batch as batch:
            for doc_id, doc_chunks in chunks.items():
                logger.debug(f"Upserting {doc_id} with {len(doc_chunks)} chunks")
                for doc_chunk in doc_chunks:
                    # we generate a uuid regardless of the format of the document_id because
                    # weaviate needs a uuid to store each document chunk and
                    # a document chunk cannot share the same uuid
                    doc_uuid = generate_uuid5(doc_chunk, WEAVIATE_CLASS)
                    metadata = doc_chunk.metadata
                    doc_chunk_dict = doc_chunk.dict()
                    doc_chunk_dict.pop("metadata")
                    for key, value in metadata.dict().items():
                        doc_chunk_dict[key] = value
                    doc_chunk_dict["chunk_id"] = doc_chunk_dict.pop("id")
                    doc_chunk_dict["source"] = (
                        doc_chunk_dict.pop("source").value
                        if doc_chunk_dict["source"]
                        else None
                    )
                    embedding = doc_chunk_dict.pop("embedding")

                    batch.add_data_object(
                        uuid=doc_uuid,
                        data_object=doc_chunk_dict,
                        class_name=WEAVIATE_CLASS,
                        vector=embedding,
                    )

                doc_ids.append(doc_id)
            batch.flush()
        return doc_ids

    async def _query(
        self,
        queries: List[QueryWithEmbedding],
    ) -> List[QueryResult]:
        """
        Takes in a list of queries with embeddings and filters and returns a list of query results with matching document chunks and scores.
        """

        async def _single_query(query: QueryWithEmbedding) -> QueryResult:
            logger.debug(f"Query: {query.query}")
            if not hasattr(query, "filter") or not query.filter:
                result = (
                    self.client.query.get(
                        WEAVIATE_CLASS,
                        [
                            "chunk_id",
                            "document_id",
                            "text",
                            "source",
                            "source_id",
                            "url",
                            "created_at",
                            "author",
                        ],
                    )
                    .with_hybrid(query=query.query, alpha=0.5, vector=query.embedding)
                    .with_limit(query.top_k)  # type: ignore
                    .with_additional(["score", "vector"])
                    .do()
                )
            else:
                filters_ = self.build_filters(query.filter)
                result = (
                    self.client.query.get(
                        WEAVIATE_CLASS,
                        [
                            "chunk_id",
                            "document_id",
                            "text",
                            "source",
                            "source_id",
                            "url",
                            "created_at",
                            "author",
                        ],
                    )
                    .with_hybrid(query=query.query, alpha=0.5, vector=query.embedding)
                    .with_where(filters_)
                    .with_limit(query.top_k)  # type: ignore
                    .with_additional(["score", "vector"])
                    .do()
                )

            query_results: List[DocumentChunkWithScore] = []
            response = result["data"]["Get"][WEAVIATE_CLASS]

            for resp in response:
                result = DocumentChunkWithScore(
                    id=resp["chunk_id"],
                    text=resp["text"],
                    # embedding=resp["_additional"]["vector"],
                    score=resp["_additional"]["score"],
                    metadata=DocumentChunkMetadata(
                        document_id=resp["document_id"] if resp["document_id"] else "",
                        source=Source(resp["source"]) if resp["source"] else None,
                        source_id=resp["source_id"],
                        url=resp["url"],
                        created_at=resp["created_at"],
                        author=resp["author"],
                    ),
                )
                query_results.append(result)
            return QueryResult(query=query.query, results=query_results)

        return await asyncio.gather(*[_single_query(query) for query in queries])

    async def delete(
        self,
        ids: Optional[List[str]] = None,
        filter: Optional[DocumentMetadataFilter] = None,
        delete_all: Optional[bool] = None,
    ) -> bool:
        # TODO
        """
        Removes vectors by ids, filter, or everything in the datastore.
        Returns whether the operation was successful.
        """
        if delete_all:
            logger.debug(f"Deleting all vectors in index {WEAVIATE_CLASS}")
            self.client.schema.delete_all()
            return True

        if ids:
            operands = [
                {"path": ["document_id"], "operator": "Equal", "valueString": id}
                for id in ids
            ]

            where_clause = {"operator": "Or", "operands": operands}

            logger.debug(f"Deleting vectors from index {WEAVIATE_CLASS} with ids {ids}")
            result = self.client.batch.delete_objects(
                class_name=WEAVIATE_CLASS, where=where_clause, output="verbose"
            )

            if not bool(result["results"]["successful"]):
                logger.debug(
                    f"Failed to delete the following objects: {result['results']['objects']}"
                )

        if filter:
            where_clause = self.build_filters(filter)

            logger.debug(
                f"Deleting vectors from index {WEAVIATE_CLASS} with filter {where_clause}"
            )
            result = self.client.batch.delete_objects(
                class_name=WEAVIATE_CLASS, where=where_clause
            )

            if not bool(result["results"]["successful"]):
                logger.debug(
                    f"Failed to delete the following objects: {result['results']['objects']}"
                )

        return True

    @staticmethod
    def build_filters(filter):
        if filter.source:
            filter.source = filter.source.value

        operands = []
        filter_conditions = {
            "source": {
                "operator": "Equal",
                "value": "query.filter.source.value",
                "value_key": "valueString",
            },
            "start_date": {"operator": "GreaterThanEqual", "value_key": "valueDate"},
            "end_date": {"operator": "LessThanEqual", "value_key": "valueDate"},
            "default": {"operator": "Equal", "value_key": "valueString"},
        }

        for attr, value in filter.__dict__.items():
            if value is not None:
                filter_condition = filter_conditions.get(
                    attr, filter_conditions["default"]
                )
                value_key = filter_condition["value_key"]

                operand = {
                    "path": [
                        attr
                        if not (attr == "start_date" or attr == "end_date")
                        else "created_at"
                    ],
                    "operator": filter_condition["operator"],
                    value_key: value,
                }

                operands.append(operand)

        return {"operator": "And", "operands": operands}

    @staticmethod
    def _is_valid_weaviate_id(candidate_id: str) -> bool:
        """
        Check if candidate_id is a valid UUID for weaviate's use

        Weaviate supports UUIDs of version 3, 4 and 5. This function checks if the candidate_id is a valid UUID of one of these versions.
        See https://weaviate.io/developers/weaviate/more-resources/faq#q-are-there-restrictions-on-uuid-formatting-do-i-have-to-adhere-to-any-standards
        for more information.
        """
        acceptable_version = [3, 4, 5]

        try:
            result = uuid.UUID(candidate_id)
            if result.version not in acceptable_version:
                return False
            else:
                return True
        except ValueError:
            return False

    @staticmethod
    def _is_wcs_domain(url: str) -> bool:
        """
        Check if the given URL ends with ".weaviate.network" or ".weaviate.network/".

        Args:
            url (str): The URL to check.

        Returns:
            bool: True if the URL ends with the specified strings, False otherwise.
        """
        pattern = r"\.(weaviate\.cloud|weaviate\.network)(/)?$"
        return bool(re.search(pattern, url))



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/datastore/providers/zilliz_datastore.py
================================================
import os

from loguru import logger
from typing import Optional
from pymilvus import (
    connections,
)
from uuid import uuid4

from datastore.providers.milvus_datastore import (
    MilvusDataStore,
)


ZILLIZ_COLLECTION = os.environ.get("ZILLIZ_COLLECTION") or "c" + uuid4().hex
ZILLIZ_URI = os.environ.get("ZILLIZ_URI")
ZILLIZ_USER = os.environ.get("ZILLIZ_USER")
ZILLIZ_PASSWORD = os.environ.get("ZILLIZ_PASSWORD")
ZILLIZ_USE_SECURITY = False if ZILLIZ_PASSWORD is None else True

ZILLIZ_CONSISTENCY_LEVEL = os.environ.get("ZILLIZ_CONSISTENCY_LEVEL")

class ZillizDataStore(MilvusDataStore):
    def __init__(self, create_new: Optional[bool] = False):
        """Create a Zilliz DataStore.

        The Zilliz Datastore allows for storing your indexes and metadata within a Zilliz Cloud instance.

        Args:
            create_new (Optional[bool], optional): Whether to overwrite if collection already exists. Defaults to True.
        """
        # Overwrite the default consistency level by MILVUS_CONSISTENCY_LEVEL
        self._consistency_level = ZILLIZ_CONSISTENCY_LEVEL or "Bounded"
        self._create_connection()

        self._create_collection(ZILLIZ_COLLECTION, create_new)  # type: ignore
        self._create_index()

    def _create_connection(self):
        # Check if the connection already exists
        try:
            i = [
                connections.get_connection_addr(x[0])
                for x in connections.list_connections()
            ].index({"address": ZILLIZ_URI, "user": ZILLIZ_USER})
            self.alias = connections.list_connections()[i][0]
        except ValueError:
            # Connect to the Zilliz instance using the passed in Environment variables
            self.alias = uuid4().hex
            connections.connect(alias=self.alias, uri=ZILLIZ_URI, user=ZILLIZ_USER, password=ZILLIZ_PASSWORD, secure=ZILLIZ_USE_SECURITY)  # type: ignore
            logger.info("Connect to zilliz cloud server")

    def _create_index(self):
        try:
            # If no index on the collection, create one
            if len(self.col.indexes) == 0:
                self.index_params = {"metric_type": "IP", "index_type": "AUTOINDEX", "params": {}}
                self.col.create_index("embedding", index_params=self.index_params)

            self.col.load()
            self.search_params = {"metric_type": "IP", "params": {}}
        except Exception as e:
            logger.error("Failed to create index, error: {}".format(e))





================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/docs/deployment/flyio.md
================================================
# Deploying to Fly.io

## Removing Unused Dependencies

Before deploying your app, you might want to remove unused dependencies from your [pyproject.toml](/pyproject.toml) file to reduce the size of your app and improve its performance. Depending on the vector database provider you choose, you can remove the packages that are not needed for your specific provider.

Find the packages you can remove for each vector database provider [here](removing-unused-dependencies.md).

After removing the unnecessary packages from the `pyproject.toml` file, you don't need to run `poetry lock` and `poetry install` manually. The provided Dockerfile takes care of installing the required dependencies using the `requirements.txt` file generated by the `poetry export` command.

## Deployment

To deploy the Docker container from this repository to Fly.io, follow
these steps:

[Install Docker](https://docs.docker.com/engine/install/) on your local machine if it is not already installed.

Install the [Fly.io CLI](https://fly.io/docs/getting-started/installing-flyctl/) on your local machine.

Clone the repository from GitHub:

```
git clone https://github.com/openai/chatgpt-retrieval-plugin.git
```

Navigate to the cloned repository directory:

```
cd path/to/chatgpt-retrieval-plugin
```

Log in to the Fly.io CLI:

```
flyctl auth login
```

Create and launch your Fly.io app:

```
flyctl launch
```

Follow the instructions in your terminal:

- Choose your app name
- Choose your app region
- Don't add any databases
- Don't deploy yet (if you do, the first deploy might fail as the environment variables are not yet set)

Set the required environment variables:

```
flyctl secrets set DATASTORE=your_datastore \
OPENAI_API_KEY=your_openai_api_key \
BEARER_TOKEN=your_bearer_token \
<Add the environment variables for your chosen vector DB here>
```

Alternatively, you could set environment variables in the [Fly.io Console](https://fly.io/dashboard).

At this point, you can change the plugin url in your plugin manifest file [here](/.well-known/ai-plugin.json), and in your OpenAPI schema [here](/.well-known/openapi.yaml) to the url for your Fly.io app, which will be `https://your-app-name.fly.dev`.

Deploy your app with:

```
flyctl deploy
```

After completing these steps, your Docker container should be deployed to Fly.io and running with the necessary environment variables set. You can view your app by running:

```
flyctl open
```

which will open your app url. You should be able to find the OpenAPI schema at `<your_app_url>/.well-known/openapi.yaml` and the manifest at `<your_app_url>/.well-known/ai-plugin.json`.

To view your app logs:

```
flyctl logs
```

Now, make sure you have changed the plugin url in your plugin manifest file [here](/.well-known/ai-plugin.json), and in your OpenAPI schema [here](/.well-known/openapi.yaml), and redeploy with `flyctl deploy`. This url will be `https://<your-app-name>.fly.dev`.

**Debugging tips:**
Fly.io uses port 8080 by default.

If your app fails to deploy, check if the environment variables are set correctly, and then check if your port is configured correctly. You could also try using the [`-e` flag](https://fly.io/docs/flyctl/launch/) with the `flyctl launch` command to set the environment variables at launch.



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/docs/deployment/heroku.md
================================================
# Deploying to Heroku

## Removing Unused Dependencies

Before deploying your app, you might want to remove unused dependencies from your [pyproject.toml](/pyproject.toml) file to reduce the size of your app and improve its performance. Depending on the vector database provider you choose, you can remove the packages that are not needed for your specific provider.

Find the packages you can remove for each vector database provider [here](removing-unused-dependencies.md).

After removing the unnecessary packages from the `pyproject.toml` file, you don't need to run `poetry lock` and `poetry install` manually. The provided Dockerfile takes care of installing the required dependencies using the `requirements.txt` file generated by the `poetry export` command.

## Deployment

To deploy the Docker container from this repository to Heroku and set the required environment variables, follow these steps:

[Install Docker](https://docs.docker.com/engine/install/) on your local machine if it is not already installed.

Install the [Heroku CLI](https://devcenter.heroku.com/articles/heroku-cli) on your local machine.

Clone the repository from GitHub:

```
git clone https://github.com/openai/chatgpt-retrieval-plugin.git
```

Navigate to the cloned repository directory:

```
cd path/to/chatgpt-retrieval-plugin
```

Log in to the Heroku CLI:

```
heroku login
```

Create a Heroku app:

```
heroku create [app-name]
```

Log in to the Heroku Container Registry:

```
heroku container:login
```

Alternatively, you can use a command from the Makefile to log in to the Heroku Container Registry by running:

```
make heroku-login
```

Build the Docker image using the Dockerfile:

```
docker buildx build --platform linux/amd64 -t [image-name] .
```

(Replace `[image-name]` with the name you want to give your Docker image)

Push the Docker image to the Heroku Container Registry, and release the newly pushed image to your Heroku app.

```
docker tag [image-name] registry.heroku.com/[app-name]/web
docker push registry.heroku.com/[app-name]/web
heroku container:release web -a [app-name]
```

Alternatively, you can use a command from the to push the Docker image to the Heroku Container Registry by running:

```
make heroku-push
```

**Note:** You will need to edit the Makefile and replace `<your app name>` with your actual app name.

Set the required environment variables for your Heroku app:

```
heroku config:set DATASTORE=your_datastore \
OPENAI_API_KEY=your_openai_api_key \
BEARER_TOKEN=your_bearer_token \
<Add the environment variables for your chosen vector DB here> \
-a [app-name]
```

You could also set environment variables in the [Heroku Console](https://dashboard.heroku.com/apps).

After completing these steps, your Docker container should be deployed to Heroku and running with the necessary environment variables set. You can view your app by running:

```
heroku open -a [app-name]
```

which will open your app url. You should be able to find the OpenAPI schema at `<your_app_url>/.well-known/openapi.yaml` and the manifest at `<your_app_url>/.well-known/ai-plugin.json`.

To view your app logs:

```
heroku logs --tail -a [app-name]
```

Now make sure to change the plugin url in your plugin manifest file [here](/.well-known/ai-plugin.json), and in your OpenAPI schema [here](/.well-known/openapi.yaml), and redeploy with `make heroku-push`. This url will be `https://your-app-name.herokuapp.com`.



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/docs/deployment/other-options.md
================================================
# Other Deployment Options

Some possible other options for deploying the app are:

- **Azure Container Apps**: This is a cloud platform that allows you to deploy and manage web apps using Docker containers. You can use the Azure CLI or the Azure Portal to create and configure your app service, and then push your Docker image to a container registry and deploy it to your app service. You can also set environment variables and scale your app using the Azure Portal. Learn more [here](https://learn.microsoft.com/en-us/azure/container-apps/get-started-existing-container-image-portal?pivots=container-apps-private-registry).
- **Google Cloud Run**: This is a serverless platform that allows you to run stateless web apps using Docker containers. You can use the Google Cloud Console or the gcloud command-line tool to create and deploy your Cloud Run service, and then push your Docker image to the Google Container Registry and deploy it to your service. You can also set environment variables and scale your app using the Google Cloud Console. Learn more [here](https://cloud.google.com/run/docs/quickstarts/build-and-deploy).
- **AWS Elastic Container Service**: This is a cloud platform that allows you to run and manage web apps using Docker containers. You can use the AWS CLI or the AWS Management Console to create and configure your ECS cluster, and then push your Docker image to the Amazon Elastic Container Registry and deploy it to your cluster. You can also set environment variables and scale your app using the AWS Management Console. Learn more [here](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/docker-basics.html).

After you create your app, make sure to change the plugin url in your plugin manifest file [here](/.well-known/ai-plugin.json), and in your OpenAPI schema [here](/.well-known/openapi.yaml), and redeploy.

## Removing Unused Dependencies

Before deploying your app, you might want to remove unused dependencies from your [pyproject.toml](/pyproject.toml) file to reduce the size of your app and improve its performance. Depending on the vector database provider you choose, you can remove the packages that are not needed for your specific provider.

Find the packages you can remove for each vector database provider [here](removing_unused_dependencies.md).

After removing the unnecessary packages from the `pyproject.toml` file, you don't need to run `poetry lock` and `poetry install` manually. The provided Dockerfile takes care of installing the required dependencies using the `requirements.txt` file generated by the `poetry export` command.



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/docs/deployment/removing-unused-dependencies.md
================================================
# Removing Unused Dependencies

Before deploying your app, you might want to remove unused dependencies from your [pyproject.toml](/pyproject.toml) file to reduce the size of your app and improve its performance. Depending on the vector database provider you choose, you can remove the packages that are not needed for your specific provider.

Here are the packages you can remove for each vector database provider:

- **Pinecone:** Remove `weaviate-client`, `pymilvus`, `qdrant-client`, `redis`, `chromadb`, `llama-index`, `azure-identity`, `azure-search-documents`, `supabase`, `psycopg2`+`pgvector`, and `psycopg2cffi`.
- **Weaviate:** Remove `pinecone-client`, `pymilvus`, `qdrant-client`, `redis`, `chromadb`, `llama-index`, `azure-identity` and `azure-search-documents`, `supabase`, `psycopg2`+`pgvector`, `psycopg2cffi`.
- **Zilliz:** Remove `pinecone-client`, `weaviate-client`, `qdrant-client`, `redis`, `chromadb`, `llama-index`, `azure-identity` and `azure-search-documents`, `supabase`, `psycopg2`+`pgvector`, and `psycopg2cffi`.
- **Milvus:** Remove `pinecone-client`, `weaviate-client`, `qdrant-client`, `redis`, `chromadb`, `llama-index`, `azure-identity` and `azure-search-documents`, `supabase`, `psycopg2`+`pgvector`, and `psycopg2cffi`.
- **Qdrant:** Remove `pinecone-client`, `weaviate-client`, `pymilvus`, `redis`, `chromadb`, `llama-index`, `azure-identity` and `azure-search-documents`, `supabase`, `psycopg2`+`pgvector`, and `psycopg2cffi`.
- **Redis:** Remove `pinecone-client`, `weaviate-client`, `pymilvus`, `qdrant-client`, `chromadb`, `llama-index`, `azure-identity` and `azure-search-documents`, `supabase`, `psycopg2`+`pgvector`, and `psycopg2cffi`.
- **LlamaIndex:** Remove `pinecone-client`, `weaviate-client`, `pymilvus`, `qdrant-client`, `chromadb`, `redis`, `azure-identity` and `azure-search-documents`, `supabase`, `psycopg2`+`pgvector`, and `psycopg2cffi`.
- **Chroma:**: Remove `pinecone-client`, `weaviate-client`, `pymilvus`, `qdrant-client`, `llama-index`, `redis`, `azure-identity` and `azure-search-documents`, `supabase`, `psycopg2`+`pgvector`, and `psycopg2cffi`.
- **Azure Cognitive Search**: Remove `pinecone-client`, `weaviate-client`, `pymilvus`, `qdrant-client`, `llama-index`, `redis` and `chromadb`, `supabase`, `psycopg2`+`pgvector`, and `psycopg2cffi`.
- **Supabase:** Remove `pinecone-client`, `weaviate-client`, `pymilvus`, `qdrant-client`, `redis`, `llama-index`, `azure-identity` and `azure-search-documents`, `psycopg2`+`pgvector`, and `psycopg2cffi`.
- **Postgres:** Remove `pinecone-client`, `weaviate-client`, `pymilvus`, `qdrant-client`, `redis`, `llama-index`, `azure-identity` and `azure-search-documents`, `supabase`, and `psycopg2cffi`.
- **AnalyticDB:** Remove `pinecone-client`, `weaviate-client`, `pymilvus`, `qdrant-client`, `redis`, `llama-index`, `azure-identity` and `azure-search-documents`, `supabase`, and `psycopg2`+`pgvector`.

After removing the unnecessary packages from the `pyproject.toml` file, you don't need to run `poetry lock` and `poetry install` manually. The provided Dockerfile takes care of installing the required dependencies using the `requirements.txt` file generated by the `poetry export` command.



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/docs/deployment/render.md
================================================
# Deploying to Render

## Removing Unused Dependencies

Before deploying your app, you might want to remove unused dependencies from your [pyproject.toml](/pyproject.toml) file to reduce the size of your app and improve its performance. Depending on the vector database provider you choose, you can remove the packages that are not needed for your specific provider.

Find the packages you can remove for each vector database provider [here](removing-unused-dependencies.md).

After removing the unnecessary packages from the `pyproject.toml` file, you don't need to run `poetry lock` and `poetry install` manually. The provided Dockerfile takes care of installing the required dependencies using the `requirements.txt` file generated by the `poetry export` command.

## Deployment

Render maintains a [fork](https://github.com/render-examples/chatgpt-retrieval-plugin/) of this repository with a few small changes that facilitate easy deployment. The source code is unchanged. To deploy both the Docker container from this repository and a self-hosted Weaviate database to back it, just click the button below. Enter your OpenAI API key when prompted.

[<img src="https://render.com/images/deploy-to-render-button.svg" alt="Deploy to Render" />](https://render.com/deploy?repo=https://github.com/render-examples/chatgpt-retrieval-plugin/tree/main)

The bearer token will be randomly generated for you. You can view it in in the "Environment" tab on the [Render dashboard](https://dashboard.render.com) page for your server. For more guidance, consult the [README in Render's fork](https://github.com/render-examples/chatgpt-retrieval-plugin/blob/main/README.md), [Render's documentation](https://render.com/docs), or the screen recording linked below.

[![Deploy to Render screen recording](render-thumbnail.png)](https://vimeo.com/823610578)



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/docs/providers/analyticdb/setup.md
================================================
# AnalyticDB

[AnalyticDB](https://www.alibabacloud.com/help/en/analyticdb-for-postgresql/latest/product-introduction-overview) is a distributed cloud-native vector database designed for storing documents and vector embeddings. It is a high-performance vector database that is fully compatible with PostgreSQL syntax, making it easy to use. Managed by Alibaba Cloud, AnalyticDB offers a powerful vector compute engine, processing billions of data vectors and providing a wide range of features, including indexing algorithms, structured and unstructured data capabilities, real-time updates, distance metrics, scalar filtering, and time travel searches. Additionally, it offers full OLAP database functionality and an SLA commitment for production use.

## Install Requirements

Run the following command to install the required packages, including the `psycopg2cffi` package:

```
poetry install --extras "postgresql"
```

If you encounter the `Error: pg_config executable not found.` issue, you need to install the PostgreSQL development package on your system. Follow the instructions for your specific Linux distribution:

1. Debian-based systems (e.g., Ubuntu):

```bash
sudo apt-get update
sudo apt-get install libpq-dev
```

2. RHEL-based systems (e.g., CentOS, Fedora):

```bash
sudo yum install postgresql-devel
```

3. Arch-based systems (e.g., Manjaro, Arch Linux):

```bash
sudo pacman -S postgresql-libs
```

4. macOS:

```bash
brew install postgresql
```

After installing the required package, try to install `psycopg2cffi` again. If the `pg_config` executable is still not found, add its location to your system's `PATH` variable. You can typically find the `pg_config` executable in the `bin` directory of your PostgreSQL installation, for example `/usr/pgsql-13/bin/pg_config`. To add it to your `PATH` variable, use the following command (replace the path with the correct one for your system):

```bash
export PATH=$PATH:/usr/pgsql-13/bin
```

Now, try installing `psycopg2cffi` again using Poetry.

**Environment Variables:**

| Name             | Required | Description                         | Default           |
| ---------------- | -------- | ----------------------------------- | ----------------- |
| `DATASTORE`      | Yes      | Datastore name, set to `analyticdb` |                   |
| `BEARER_TOKEN`   | Yes      | Secret token                        |                   |
| `OPENAI_API_KEY` | Yes      | OpenAI API key                      |                   |
| `PG_HOST`        | Yes      | AnalyticDB instance URL             | `localhost`       |
| `PG_USER`        | Yes      | Database user                       | `user`            |
| `PG_PASSWORD`    | Yes      | Database password                   | `password`        |
| `PG_PORT`        | Optional | Port for AnalyticDB communication   | `5432`            |
| `PG_DATABASE`    | Optional | Database name                       | `postgres`        |
| `PG_COLLECTION`  | Optional | AnalyticDB relation name            | `document_chunks` |

## AnalyticDB Cloud

For a hosted [AnalyticDB Cloud](https://cloud.qdrant.io/) version, provide the AnalyticDB instance URL:

**Example:**

```bash
PG_HOST="https://YOUR-CLUSTER-URL.gpdb.rds.aliyuncs.com"
PG_USER="YOUR-USER-NAME"
PG_PASSWORD="YOUR-PASSWORD"
```

The other parameters are optional and can be changed if needed.

## Running AnalyticDB Integration Tests

A suite of integration tests verifies the AnalyticDB integration. Launch the test suite with this command:

```bash
pytest ./tests/datastore/providers/analyticdb/test_analyticdb_datastore.py
```



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/docs/providers/azuresearch/setup.md
================================================
# Azure Cognitive Search

[Azure Cognitive Search](https://azure.microsoft.com/products/search/) is a complete retrieval cloud service that supports vector search, text search, and hybrid (vectors + text combined to yield the best of the two approaches). Azure Cognitive Search also offers an [optional L2 re-ranking step](https://learn.microsoft.com/azure/search/semantic-search-overview) to further improve results quality. 

You can find the Azure Cognitive Search documentation [here](https://learn.microsoft.com/azure/search/search-what-is-azure-search). If you don't have an Azure account, you can start setting one up [here](https://azure.microsoft.com/).

## Signing up for vector search

Azure Cognitive Search supports searching using pure vectors, pure text, or hybrid mode where both are combined. For the vector-based cases, you'll need to sign up for vector search private preview. To sign up, please fill in this form: https://aka.ms/VectorSearchSignUp

## Environment variables

| Name                         | Required | Description                                                                           | Default             |
| ---------------------------- | -------- | ------------------------------------------------------------------------------------- | ------------------- |
| `DATASTORE`                  | Yes      | Datastore name, set to `azuresearch`                                                  |                     |
| `BEARER_TOKEN`               | Yes      | Secret token                                                                          |                     |
| `OPENAI_API_KEY`             | Yes      | OpenAI API key                                                                        |                     |
| `AZURESEARCH_SERVICE`        | Yes      | Name of your search service                                                           |                     |
| `AZURESEARCH_INDEX`          | Yes      | Name of your search index                                                             |                     |
| `AZURESEARCH_API_KEY`        | No       | Your API key, if using key-based auth instead of Azure managed identity               |Uses managed identity|
| `AZURESEARCH_DISABLE_HYBRID` | No       | Disable hybrid search and only use vector similarity                                  |Use hybrid search    |
| `AZURESEARCH_SEMANTIC_CONFIG`| No       | Enable L2 re-ranking with this configuration name [see re-ranking below](#re-ranking) |L2 not enabled       |
| `AZURESEARCH_LANGUAGE`       | No       | If using L2 re-ranking, language for queries/documents (valid values [listed here](https://learn.microsoft.com/rest/api/searchservice/preview-api/search-documents#queryLanguage))     |`en-us`              |
| `AZURESEARCH_DIMENSIONS`     | No       | Vector size for embeddings                                                            |1536 (OpenAI's Ada002)|

## Authentication Options

* API key: this is enabled by default; you can obtain the key in the Azure Portal or using the Azure CLI.
* Managed identity: If the plugin is running in Azure, you can enable managed identity for the host and give that identity access to the service, without having to manage keys (avoiding secret storage, rotation, etc.). More details [here](https://learn.microsoft.com/azure/search/search-security-rbac). 

## Re-ranking

Azure Cognitive Search offers the option to enable a second (L2) ranking step after retrieval to further improve results quality. This only applies when using text or hybrid search. Since it has latency and cost implications, if you want to try this option you need to explicitly [enable "semantic search"](https://learn.microsoft.com/azure/search/semantic-search-overview#enable-semantic-search) in your Cognitive Search service, and [create a semantic search configuration](https://learn.microsoft.com/azure/search/semantic-how-to-query-request#2---create-a-semantic-configuration) for your index.

## Using existing search indexes

If an existing index has fields that align with what's needed by the retrieval plugin but just differ in names, you can map your fields to the plugin fields using the following environment variables:

|Plugin field name|Environment variable to override it|
|-----------------|-----------------------------------|
|id               |AZURESEARCH_FIELDS_ID              |
|text             |AZURESEARCH_FIELDS_TEXT            |
|embedding        |AZURESEARCH_FIELDS_EMBEDDING       |
|document_id      |AZURESEARCH_FIELDS_DOCUMENT_ID     |
|source           |AZURESEARCH_FIELDS_SOURCE          |
|source_id        |AZURESEARCH_FIELDS_SOURCE_ID       |
|url              |AZURESEARCH_FIELDS_URL             |
|created_at       |AZURESEARCH_FIELDS_CREATED_AT      |
|author           |AZURESEARCH_FIELDS_AUTHOR          |



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/docs/providers/chroma/setup.md
================================================
[Chroma](https://trychroma.com) is an AI-native open-source embedding database designed to make it easy to work with embeddings. Chroma runs in-memory, or in a client-server setup.

Install Chroma by running `pip install chromadb`. Once installed, the core API consists of four essential commands for creating collections, adding embeddings, documents, and metadata, and querying embeddings to find similar documents. Get started with Chroma by visiting the [Getting Started](https://docs.trychroma.com) page on their documentation website, or explore the open-source code on their [GitHub repository](https://github.com/chroma-core/chroma).

**Chroma Environment Variables**

To set up Chroma and start using it as your vector database provider, you need to define some environment variables to connect to your Chroma instance.

**Chroma Datastore Environment Variables**

Chroma runs _in-memory_ by default, with local persistence. It can also run in [self-hosted](https://docs.trychroma.com/usage-guide#running-chroma-in-clientserver-mode) client-server mode, with a fully managed hosted version coming soon.

| Name                     | Required | Description                                                                                        | Default          |
| ------------------------ | -------- | -------------------------------------------------------------------------------------------------- | ---------------- |
| `DATASTORE`              | Yes      | Datastore name. Set this to `chroma`                                                               |                  |
| `BEARER_TOKEN`           | Yes      | Your secret token for authenticating requests to the API                                           |                  |
| `OPENAI_API_KEY`         | Yes      | Your OpenAI API key for generating embeddings                                                      |                  |
| `CHROMA_COLLECTION`      | Optional | Your chosen Chroma collection name to store your embeddings                                        | openaiembeddings |
| `CHROMA_IN_MEMORY`       | Optional | If set to `True`, ignore `CHROMA_HOST` and `CHROMA_PORT` and just use an in-memory Chroma instance | `True`           |
| `CHROMA_PERSISTENCE_DIR` | Optional | If set, and `CHROMA_IN_MEMORY` is set, persist to and load from this directory.                    | `openai`         |

To run Chroma in self-hosted client-server mode, st the following variables:

| Name          | Required | Description                                         | Default            |
| ------------- | -------- | --------------------------------------------------- | ------------------ |
| `CHROMA_HOST` | Optional | Your Chroma instance host address (see notes below) | `http://127.0.0.1` |
| `CHROMA_PORT` | Optional | Your Chroma port number                             | `8000`             |

> For **self-hosted instances**, if your instance is not at 127.0.0.1:8000, set `CHROMA_HOST` and `CHROMA_PORT` accordingly. For example: `CHROMA_HOST=http://localhost/` and `CHROMA_PORT=8080`.



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/docs/providers/elasticsearch/setup.md
================================================
# Elasticsearch

Elasticsearch is a search engine based on the Lucene library. It provides a distributed, full-text and vector search engine with an HTTP web interface and schema-free JSON documents. To use Elasticsearch as your vector database, start by [installing Elasticsearch](https://www.elastic.co/guide/en/elasticsearch/reference/current/install-elasticsearch.html) or signing up for a free trial of [Elastic Cloud](https://www.elastic.co/cloud/).

The app will create an Elasticsearch index for you automatically when you run it for the first time. Just pick a name for your index and set it as an environment variable.

**Environment Variables:**

| Name                  | Required | Description                                                                                                          |
| --------------------- | -------- | -------------------------------------------------------------------------------------------------------------------- |
| `DATASTORE`           | Yes      | Datastore name, set this to `elasticsearch`                                                                          |
| `BEARER_TOKEN`        | Yes      | Your secret token for authenticating requests to the API                                                             |
| `OPENAI_API_KEY`      | Yes      | Your OpenAI API key for generating embeddings with the `text-embedding-ada-002` model                                |
| `ELASTICSEARCH_INDEX` | Yes      | Your chosen Elasticsearch index name. **Note:** Index name must consist of lower case alphanumeric characters or '-' |

**Connection Evironment Variables:**
Depending on your Elasticsearch setup, you may need to set one of the following environment variables to connect to your Elasticsearch instance. If you are using Elastic Cloud, you can connect via `ELASTICSEARCH_CLOUD_ID`. If you are using a local instance of Elasticsearch, you will need to set `ELASTICSEARCH_URL`.

You can authenticate to Elasticsearch using either `ELASTICSEARCH_USERNAME` and `ELASTICSEARCH_PASSWORD` or `ELASTICSEARCH_API_KEY`. If you are using Elastic Cloud, you can find this in Kibana.

| Name                     | Required | Description                                                                                      |
| ------------------------ | -------- | ------------------------------------------------------------------------------------------------ |
| `ELASTICSEARCH_URL`      | Yes      | Your Elasticsearch URL. If installed locally, this would be https://localhost:9200               |
| `ELASTICSEARCH_CLOUD_ID` | Yes      | Your cloud id, linked to your deployment. This can be found in the deployment's console          |
| `ELASTICSEARCH_USERNAME` | Yes      | Your username for authenticating requests to the API. Commonly 'elastic'.                        |
| `ELASTICSEARCH_PASSWORD` | Yes      | Your password for authenticating requests to the API                                             |
| `ELASTICSEARCH_API_KEY`  | Yes      | Alternatively you can authenticate using api-key. This can be created in Kibana stack management |

## Running Elasticsearch Integration Tests

A suite of integration tests is available to verify the Elasticsearch integration. To run the tests, run the docker compose found in the `examples/docker/elasticsearch` folder with `docker-compose up`. This will start Elasticsearch in single node, security off mode, listening on `http://localhost:9200`.

Then, launch the test suite with this command:

```bash
pytest ./tests/datastore/providers/elasticsearch/test_elasticsearch_datastore.py
```



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/docs/providers/llama/setup.md
================================================

# LlamaIndex

[LlamaIndex](https://github.com/jerryjliu/llama_index) is a central interface to connect your LLM's with external data.
It provides a suite of in-memory indices over your unstructured and structured data for use with ChatGPT.
Unlike standard vector databases, LlamaIndex supports a wide range of indexing strategies (e.g. tree, keyword table, knowledge graph) optimized for different use-cases.
It is light-weight, easy-to-use, and requires no additional deployment.
All you need to do is specifying a few environment variables (optionally point to an existing saved Index json file).
Note that metadata filters in queries are not yet supported.

## Setup
Currently, LlamaIndex requires no additional deployment
and runs as a part of the Retrieval Plugin.
It is super easy to setup and great for quick prototyping
with ChatGPT and your external data.

**Retrieval App Environment Variables**

| Name             | Required | Description                         |
|------------------|----------|-------------------------------------|
| `DATASTORE`      | Yes      | Datastore name. Set this to `llama` |
| `BEARER_TOKEN`   | Yes      | Your secret token                   |
| `OPENAI_API_KEY` | Yes      | Your OpenAI API key                 |

**Llama Datastore Environment Variables**

| Name                           | Required | Description                          | Default       |
|--------------------------------|----------|--------------------------------------|---------------|
| `LLAMA_INDEX_TYPE`             | Optional | Index type (see below for details)   | `simple_dict` |
| `LLAMA_INDEX_JSON_PATH`        | Optional | Path to saved Index json file        | None          |
| `LLAMA_QUERY_KWARGS_JSON_PATH` | Optional | Path to saved query kwargs json file | None          |
| `LLAMA_RESPONSE_MODE`          | Optional | Response mode for query              | `no_text`     | 


**Different Index Types**
By default, we use a `GPTVectorStoreIndex` to store document chunks in memory, 
and retrieve top-k nodes by embedding similarity.
Different index types are optimized for different data and query use-cases.
See this guide on [How Each Index Works](https://gpt-index.readthedocs.io/en/latest/guides/primer/index_guide.html) to learn more.
You can configure the index type via the `LLAMA_INDEX_TYPE`, see [here](https://gpt-index.readthedocs.io/en/latest/reference/indices/composability_query.html#gpt_index.data_structs.struct_type.IndexStructType) for the full list of accepted index type identifiers.


Read more details on [readthedocs](https://gpt-index.readthedocs.io/en/latest/), 
and engage with the community on [discord](https://discord.com/invite/dGcwcsnxhU).

## Running Tests
You can launch the test suite with this command:

```bash
pytest ./tests/datastore/providers/llama/test_llama_datastore.py
```



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/docs/providers/milvus/setup.md
================================================
# Milvus

[Milvus](https://milvus.io/) is the open-source, cloud-native vector database that scales to billions of vectors. It's the open-source version of Zilliz. It supports:

- Various indexing algorithms and distance metrics
- Scalar filtering and time travel searches
- Rollback and snapshots
- Multi-language SDKs
- Storage and compute separation
- Cloud scalability
- A developer-first community with multi-language support

Visit the [Github](https://github.com/milvus-io/milvus) to learn more.

## Deploying the Database

You can deploy and manage Milvus using Docker Compose, Helm, K8's Operator, or Ansible. Follow the instructions [here](https://milvus.io/docs) to get started.

**Environment Variables:**

| Name                       | Required | Description                                                                                                                                  |
|----------------------------| -------- |----------------------------------------------------------------------------------------------------------------------------------------------|
| `DATASTORE`                | Yes      | Datastore name, set to `milvus`                                                                                                              |
| `BEARER_TOKEN`             | Yes      | Your bearer token                                                                                                                            |
| `OPENAI_API_KEY`           | Yes      | Your OpenAI API key                                                                                                                          |
| `MILVUS_COLLECTION`        | Optional | Milvus collection name, defaults to a random UUID                                                                                            |
| `MILVUS_HOST`              | Optional | Milvus host IP, defaults to `localhost`                                                                                                      |
| `MILVUS_PORT`              | Optional | Milvus port, defaults to `19530`                                                                                                             |
| `MILVUS_USER`              | Optional | Milvus username if RBAC is enabled, defaults to `None`                                                                                       |
| `MILVUS_PASSWORD`          | Optional | Milvus password if required, defaults to `None`                                                                                              |
| `MILVUS_INDEX_PARAMS`      | Optional | Custom index options for the collection, defaults to `{"metric_type": "IP", "index_type": "HNSW", "params": {"M": 8, "efConstruction": 64}}` |
| `MILVUS_SEARCH_PARAMS`     | Optional | Custom search options for the collection, defaults to `{"metric_type": "IP", "params": {"ef": 10}}`                                          |
| `MILVUS_CONSISTENCY_LEVEL` | Optional | Data consistency level for the collection, defaults to `Bounded`                                                                             |

## Running Milvus Integration Tests

A suite of integration tests is available to verify the Milvus integration. To run the tests, run the milvus docker compose found in the examples folder.

Then, launch the test suite with this command:

```bash
pytest ./tests/datastore/providers/milvus/test_milvus_datastore.py
```



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/docs/providers/pinecone/setup.md
================================================
# Pinecone

[Pinecone](https://www.pinecone.io) is a managed vector database built for speed, scale, and shipping to production sooner. To use Pinecone as your vector database provider, first get an API key by [signing up for an account](https://app.pinecone.io/). You can access your API key from the "API Keys" section in the sidebar of your dashboard. Pinecone also supports hybrid search and at the time of writing is the only datastore to support SPLADE sparse vectors natively.

A full Jupyter notebook walkthrough for the Pinecone flavor of the retrieval plugin can be found [here](https://github.com/openai/chatgpt-retrieval-plugin/blob/main/examples/providers/pinecone/semantic-search.ipynb). There is also a [video walkthrough here](https://youtu.be/hpePPqKxNq8).

The app will create a Pinecone index for you automatically when you run it for the first time. Just pick a name for your index and set it as an environment variable.

**Environment Variables:**

| Name                   | Required | Description                                                                                                                      |
| ---------------------- | -------- | -------------------------------------------------------------------------------------------------------------------------------- |
| `DATASTORE`            | Yes      | Datastore name, set this to `pinecone`                                                                                           |
| `BEARER_TOKEN`         | Yes      | Your secret token for authenticating requests to the API                                                                         |
| `OPENAI_API_KEY`       | Yes      | Your OpenAI API key for generating embeddings with the `text-embedding-ada-002` model                                            |
| `PINECONE_API_KEY`     | Yes      | Your Pinecone API key, found in the [Pinecone console](https://app.pinecone.io/)                                                 |
| `PINECONE_ENVIRONMENT` | Yes      | Your Pinecone environment, found in the [Pinecone console](https://app.pinecone.io/), e.g. `us-west1-gcp`, `us-east-1-aws`, etc. |
| `PINECONE_INDEX`       | Yes      | Your chosen Pinecone index name. **Note:** Index name must consist of lower case alphanumeric characters or '-'                  |

If you want to create your own index with custom configurations, you can do so using the Pinecone SDK, API, or web interface ([see docs](https://docs.pinecone.io/docs/manage-indexes)). Make sure to use a dimensionality of 1536 for the embeddings and avoid indexing on the text field in the metadata, as this will reduce the performance significantly.

```python
# Creating index with Pinecone SDK - use only if you wish to create the index manually.

import os, pinecone

pinecone.init(api_key=os.environ['PINECONE_API_KEY'],
              environment=os.environ['PINECONE_ENVIRONMENT'])

pinecone.create_index(name=os.environ['PINECONE_INDEX'],
                      dimension=1536,
                      metric='cosine',
                      metadata_config={
                          "indexed": ['source', 'source_id', 'url', 'created_at', 'author', 'document_id']})
```



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/docs/providers/postgres/setup.md
================================================
# Postgres

Postgres Database offers an easy and efficient way to store vectors via [pgvector](https://github.com/pgvector/pgvector) extension. To use pgvector, you will need to set up a PostgreSQL database with the pgvector extension enabled or use a managed solution that provides pgvector. For a hosted/managed solution, you can use any of the cloud vendors which support [pgvector](https://github.com/pgvector/pgvector#hosted-postgres).

- The database needs the `pgvector` extension.
- To apply required migrations you may use any tool you are more familiar with like [pgAdmin](https://www.pgadmin.org/), [DBeaver](https://dbeaver.io/), [DataGrip](https://www.jetbrains.com/datagrip/), or `psql` cli.

**Retrieval App Environment Variables**

| Name             | Required | Description                            |
| ---------------- | -------- | -------------------------------------- |
| `DATASTORE`      | Yes      | Datastore name. Set this to `postgres` |
| `BEARER_TOKEN`   | Yes      | Your secret token                      |
| `OPENAI_API_KEY` | Yes      | Your OpenAI API key                    |

**Postgres Datastore Environment Variables**

| Name          | Required | Description       | Default    |
| ------------- | -------- | ----------------- | ---------- |
| `PG_HOST`     | Optional | Postgres host     | localhost  |
| `PG_PORT`     | Optional | Postgres port     | `5432`     |
| `PG_PASSWORD` | Optional | Postgres password | `postgres` |
| `PG_USER`     | Optional | Postgres username | `postgres` |
| `PG_DB`       | Optional | Postgres database | `postgres` |

## Postgres Datastore local development & testing

In order to test your changes to the Postgres Datastore, you can run the following:

1. You can run local or self-hosted instance of PostgreSQL with `pgvector` enabled using Docker.

```bash
docker pull ankane/pgvector
```

```bash
docker run --name pgvector -e POSTGRES_PASSWORD=mysecretpassword -d postgres
```

Check PostgreSQL [official docker image](https://github.com/docker-library/docs/blob/master/postgres/README.md) for more options.

2. Apply migrations using any tool you like most [pgAdmin](https://www.pgadmin.org/), [DBeaver](https://dbeaver.io/), [DataGrip](https://www.jetbrains.com/datagrip/), or `psql` cli.

```bash
# apply migrations using psql cli
psql -h localhost -p 5432 -U postgres -d postgres -f examples/providers/supabase/migrations/20230414142107_init_pg_vector.sql
```

3. Export environment variables required for the Postgres Datastore

```bash
export PG_HOST=localhost
export PG_PORT=54322
export PG_PASSWORD=mysecretpassword
```

4. Run the Postgres datastore tests from the project's root directory

```bash
# Run the Postgres datastore tests
# go to project's root directory and run
poetry run pytest -s ./tests/datastore/providers/postgres/test_postgres_datastore.py
```

5. When going to prod don't forget to set the password for the `postgres` user to something more secure and apply migrations.

6. You may want to remove RLS (Row Level Security) from the `documents` table. If you are not using RLS, it is not required in this setup. But it may be useful if you want to separate documents by user or group of users, or if you want to give permissions to insert or query documents to different users. And RLS is especially important if you are willing to use PostgREST. To do so you can just remove the following statement from the `20230414142107_init_pg_vector.sql` migration file: `alter table documents enable row level security;`.

## Indexes for Postgres

By default, pgvector performs exact nearest neighbor search. To speed up the vector comparison, you may want to create indexes for the `embedding` column in the `documents` table. You should do this **only** after a few thousand records are inserted.

As datasotre is using inner product for similarity search, you can add index as follows:

```sql
create index on documents using ivfflat (embedding vector_ip_ops) with (lists = 100);
```

To choose `lists` constant - a good place to start is records / 1000 for up to 1M records and sqrt(records) for over 1M records

For more information about indexes, see [pgvector docs](https://github.com/pgvector/pgvector#indexing).



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/docs/providers/qdrant/setup.md
================================================
# Qdrant

[Qdrant](https://qdrant.tech/) is a vector database that can store documents and vector embeddings. It can run as a self-hosted version or a managed [Qdrant Cloud](https://cloud.qdrant.io/)
solution. The configuration is almost identical for both options, except for the API key that [Qdrant Cloud](https://cloud.qdrant.io/) provides.

**Environment Variables:**

| Name                | Required | Description                                                 | Default            |
| ------------------- | -------- | ----------------------------------------------------------- | ------------------ |
| `DATASTORE`         | Yes      | Datastore name, set to `qdrant`                             |                    |
| `BEARER_TOKEN`      | Yes      | Secret token                                                |                    |
| `OPENAI_API_KEY`    | Yes      | OpenAI API key                                              |                    |
| `QDRANT_URL`        | Yes      | Qdrant instance URL                                         | `http://localhost` |
| `QDRANT_PORT`       | Optional | TCP port for Qdrant HTTP communication                      | `6333`             |
| `QDRANT_GRPC_PORT`  | Optional | TCP port for Qdrant GRPC communication                      | `6334`             |
| `QDRANT_API_KEY`    | Optional | Qdrant API key for [Qdrant Cloud](https://cloud.qdrant.io/) |                    |
| `QDRANT_COLLECTION` | Optional | Qdrant collection name                                      | `document_chunks`  |

## Qdrant Cloud

For a hosted [Qdrant Cloud](https://cloud.qdrant.io/) version, provide the Qdrant instance
URL and the API key from the [Qdrant Cloud UI](https://cloud.qdrant.io/).

**Example:**

```bash
QDRANT_URL="https://YOUR-CLUSTER-URL.aws.cloud.qdrant.io"
QDRANT_API_KEY="<YOUR_QDRANT_CLOUD_CLUSTER_API_KEY>"
```

The other parameters are optional and can be changed if needed.

## Self-hosted Qdrant Instance

For a self-hosted version, use Docker containers or the official Helm chart for deployment. The only
required parameter is the `QDRANT_URL` that points to the Qdrant server URL.

**Example:**

```bash
QDRANT_URL="http://YOUR_HOST.example.com:6333"
```

The other parameters are optional and can be changed if needed.

## Running Qdrant Integration Tests

A suite of integration tests verifies the Qdrant integration. To run it, start a local Qdrant instance in a Docker container.

```bash
docker run -p "6333:6333" -p "6334:6334" qdrant/qdrant:v1.0.3
```

Then, launch the test suite with this command:

```bash
pytest ./tests/datastore/providers/qdrant/test_qdrant_datastore.py
```



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/docs/providers/redis/setup.md
================================================
# Redis

[Redis](https://redis.com/solutions/use-cases/vector-database/) is a real-time data platform that supports a variety of use cases for everyday applications as well as AI/ML workloads. Use Redis as a low-latency vector engine by creating a Redis database with the [Redis Stack docker container](/examples/docker/redis/docker-compose.yml). For a hosted/managed solution, try [Redis Cloud](https://app.redislabs.com/#/). See more helpful examples of Redis as a vector database [here](https://github.com/RedisVentures/redis-ai-resources).

- The database **needs the RediSearch module (>=v2.6) and RedisJSON**, which are included in the self-hosted docker compose above.
- Run the App with the Redis docker image: `docker compose up -d` in [this dir](/examples/docker/redis/).
- The app automatically creates a Redis vector search index on the first run. Optionally, create a custom index with a specific name and set it as an environment variable (see below).
- To enable more hybrid searching capabilities, adjust the document schema [here](/datastore/providers/redis_datastore.py).

**Environment Variables:**

| Name                    | Required | Description                                                                                                            | Default     |
| ----------------------- | -------- | ---------------------------------------------------------------------------------------------------------------------- | ----------- |
| `DATASTORE`             | Yes      | Datastore name, set to `redis`                                                                                         |             |
| `BEARER_TOKEN`          | Yes      | Secret token                                                                                                           |             |
| `OPENAI_API_KEY`        | Yes      | OpenAI API key                                                                                                         |             |
| `REDIS_HOST`            | Optional | Redis host url                                                                                                         | `localhost` |
| `REDIS_PORT`            | Optional | Redis port                                                                                                             | `6379`      |
| `REDIS_PASSWORD`        | Optional | Redis password                                                                                                         | none        |
| `REDIS_INDEX_NAME`      | Optional | Redis vector index name                                                                                                | `index`     |
| `REDIS_DOC_PREFIX`      | Optional | Redis key prefix for the index                                                                                         | `doc`       |
| `REDIS_DISTANCE_METRIC` | Optional | Vector similarity distance metric                                                                                      | `COSINE`    |
| `REDIS_INDEX_TYPE`      | Optional | [Vector index algorithm type](https://redis.io/docs/stack/search/reference/vectors/#creation-attributes-per-algorithm) | `FLAT`      |


## Redis Datastore development & testing
In order to test your changes to the Redis Datastore, you can run the following commands:

```bash
# Run the Redis stack docker image
docker run -it --rm -p 6379:6379 redis/redis-stack-server:latest
```
    
```bash
# Run the Redis datastore tests
poetry run pytest -s ./tests/datastore/providers/redis/test_redis_datastore.py
```


================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/docs/providers/supabase/setup.md
================================================
# Supabase

[Supabase](https://supabase.com/blog/openai-embeddings-postgres-vector) offers an easy and efficient way to store vectors via [pgvector](https://github.com/pgvector/pgvector) extension for Postgres Database. [You can use Supabase CLI](https://github.com/supabase/cli) to set up a whole Supabase stack locally or in the cloud or you can also use docker-compose, k8s and other options available. For a hosted/managed solution, try [Supabase.com](https://supabase.com/) and unlock the full power of Postgres with built-in authentication, storage, auto APIs, and Realtime features. See more helpful examples of Supabase & pgvector as a vector database [here](https://github.com/supabase-community/nextjs-openai-doc-search).

- The database needs the `pgvector` extension, which is included in [Supabase distribution of Postgres](https://github.com/supabase/postgres).
- It is possible to provide a Postgres connection string and an app will add `documents` table, query Postgres function, and `pgvector` extension automatically.
- But it is recommended to separate the migration process from an app. And execute the migration script in a different pipeline by using SQL statements from `_init_db()` function in [Supabase datastore provider](/datastore/providers/supabase_datastore.py).

**Retrieval App Environment Variables**

| Name             | Required | Description                            |
| ---------------- | -------- | -------------------------------------- |
| `DATASTORE`      | Yes      | Datastore name. Set this to `supabase` |
| `BEARER_TOKEN`   | Yes      | Your secret token                      |
| `OPENAI_API_KEY` | Yes      | Your OpenAI API key                    |

**Supabase Datastore Environment Variables**

| Name                        | Required | Description                                                                    | Default |
| --------------------------- | -------- | ------------------------------------------------------------------------------ | ------- |
| `SUPABASE_URL`              | Yes      | Supabase Project URL                                                           |         |
| `SUPABASE_ANON_KEY`         | Optional | Supabase Project API anon key                                                  |         |
| `SUPABASE_SERVICE_ROLE_KEY` | Optional | Supabase Project API service key, will be used if provided instead of anon key |         |

## Supabase Datastore local development & testing

In order to test your changes to the Supabase Datastore, you can run the following commands:

1. Install [Supabase CLI](https://github.com/supabase/cli) and [Docker](https://docs.docker.com/get-docker/)

2. Run the Supabase `start` command from `examples/providers` directory. Config for Supabase local setup is available in `examples/providers/supabase` directory with required migrations.

```bash
# Run the Supabase stack using cli in docker
# go to examples/providers and run supabase start
cd examples/providers
supabase start
```

3. Supabase `start` will download docker images and launch Supabase stack locally. You will see similar output:

```bash
Applying migration 20230414142107_init_pg_vector.sql...
Seeding data supabase/seed.sql...
Started supabase local development setup.

         API URL: http://localhost:54321
          DB URL: postgresql://postgres:postgres@localhost:54322/postgres
      Studio URL: http://localhost:54323
    Inbucket URL: http://localhost:54324
      JWT secret: super-secret-jwt-token-with-at-least-32-characters-long
        anon key: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZS1kZW1vIiwicm9sZSI6ImFub24iLCJleHAiOjE5ODM4MTI5OTZ9.CRXP1A7WOeoJeXxjNni43kdQwgnWNReilDMblYTn_I0
service_role key: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZS1kZW1vIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImV4cCI6MTk4MzgxMjk5Nn0.EGIM96RAZx35lJzdJsyH-qQwv8Hdp7fsn3W0YpN81IU
```

4. Export environment variables required for the Supabase Datastore

```bash
export SUPABASE_URL=http://localhost:54321
export SUPABASE_SERVICE_ROLE_KEY='eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZS1kZW1vIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImV4cCI6MTk4MzgxMjk5Nn0.EGIM96RAZx35lJzdJsyH-qQwv8Hdp7fsn3W0YpN81IU'
```

5. Run the Supabase datastore tests from the project's root directory

```bash
# Run the Supabase datastore tests
# go to project's root directory and run
poetry run pytest -s ./tests/datastore/providers/supabase/test_supabase_datastore.py
```

6. When you go to prod (if cloud hosted) it is recommended to link your supabase project with the local setup from `examples/providers/supabase`. All migrations will be synced with the cloud project after you run `supabase db push`. Or you can manually apply migrations from `examples/providers/supabase/migrations` directory.

7. You might want to add RLS policies to the `documents` table. Or you can just continue using it on the server side only with the service role key. But you should not use service role key on the client side in any case.

## Indexes for Postgres

By default, pgvector performs exact nearest neighbor search. To speed up the vector comparison, you may want to create indexes for the `embedding` column in the `documents` table. You should do this **only** after a few thousand records are inserted.

As datasotre is using inner product for similarity search, you can add index as follows:

```sql
create index on documents using ivfflat (embedding vector_ip_ops) with (lists = 100);
```

To choose `lists` constant - a good place to start is records / 1000 for up to 1M records and sqrt(records) for over 1M records

For more information about indexes, see [pgvector docs](https://github.com/pgvector/pgvector#indexing).



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/docs/providers/weaviate/setup.md
================================================
# Weaviate

## Set up a Weaviate Instance

[Weaviate](https://weaviate.io/) is an open-source vector search engine designed to scale seamlessly into billions of data objects. This implementation supports hybrid search out-of-the-box (meaning it will perform better for keyword searches).

You can run Weaviate in 4 ways:

- **SaaS** – with [Weaviate Cloud Services (WCS)](https://weaviate.io/pricing).

  WCS is a fully managed service that takes care of hosting, scaling, and updating your Weaviate instance. You can try it out for free with a sandbox that lasts for 30 days.

  To set up a SaaS Weaviate instance with WCS:

  1.  Navigate to [Weaviate Cloud Console](https://console.weaviate.io/).
  2.  Register or sign in to your WCS account.
  3.  Create a new cluster with the following settings:
      - `Name` – a unique name for your cluster. The name will become part of the URL used to access this instance.
      - `Subscription Tier` – Sandbox for a free trial, or contact [hello@weaviate.io](mailto:hello@weaviate.io) for other options.
      - `Weaviate Version` - The latest version by default.
      - `OIDC Authentication` – Enabled by default. This requires a username and password to access your instance.
  4.  Wait for a few minutes until your cluster is ready. You will see a green tick ✔️ when it's done. Copy your cluster URL.

- **Hybrid SaaS**

  > If you need to keep your data on-premise for security or compliance reasons, Weaviate also offers a Hybrid SaaS option: Weaviate runs within your cloud instances, but the cluster is managed remotely by Weaviate. This gives you the benefits of a managed service without sending data to an external party.

  The Weaviate Hybrid SaaS is a custom solution. If you are interested in this option, please reach out to [hello@weaviate.io](mailto:hello@weaviate.io).

- **Self-hosted** – with a Docker container

  To set up a Weaviate instance with Docker:

  1. [Install Docker](https://docs.docker.com/engine/install/) on your local machine if it is not already installed.
  2. [Install the Docker Compose Plugin](https://docs.docker.com/compose/install/)
  3. Download a `docker-compose.yml` file with this `curl` command:

     ```
     curl -o docker-compose.yml "https://configuration.weaviate.io/v2/docker-compose/docker-compose.yml?modules=standalone&runtime=docker-compose&weaviate_version=v1.18.0"
     ```

     Alternatively, you can use Weaviate's docker compose [configuration tool](https://weaviate.io/developers/weaviate/installation/docker-compose) to generate your own `docker-compose.yml` file.

  4. Run `docker compose up -d` to spin up a Weaviate instance.

     > To shut it down, run `docker compose down`.

- **Self-hosted** – with a Kubernetes cluster

  To configure a self-hosted instance with Kubernetes, follow Weaviate's [documentation](https://weaviate.io/developers/weaviate/installation/kubernetes).

## Configure Weaviate Environment Variables

You need to set some environment variables to connect to your Weaviate instance.

**Retrieval App Environment Variables**

| Name             | Required | Description                                                                          |
| ---------------- | -------- |--------------------------------------------------------------------------------------|
| `DATASTORE`      | Yes      | Datastore name. Set this to `weaviate`                                               |
| `BEARER_TOKEN`   | Yes      | Your [secret token](/README.md#general-environment-variables) (not the Weaviate one) |
| `OPENAI_API_KEY` | Yes      | Your OpenAI API key                                                                  |

**Weaviate Datastore Environment Variables**

| Name             | Required | Description                                                        | Default            |
|------------------| -------- | ------------------------------------------------------------------ | ------------------ |
| `WEAVIATE_URL`  | Optional | Your weaviate instance's url/WCS endpoint              | `http://localhost:8080` |           |
| `WEAVIATE_CLASS` | Optional | Your chosen Weaviate class/collection name to store your documents | OpenAIDocument     |

**Weaviate Auth Environment Variables**

If using WCS instances, set the following environment variables:

| Name                | Required | Description                    |
| ------------------- | -------- | ------------------------------ |
| `WEAVIATE_API_KEY` | Yes      | Your API key WCS      |

Learn more about accessing your [WCS API key](https://weaviate.io/developers/wcs/guides/authentication#access-api-keys).


================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/docs/providers/zilliz/setup.md
================================================
# Zilliz

[Zilliz](https://zilliz.com) is a managed cloud-native vector database designed for the billion scale. Zilliz offers many key features, such as:

- Multiple indexing algorithms
- Multiple distance metrics
- Scalar filtering
- Time travel searches
- Rollback and with snapshots
- Full RBAC
- 99.9% uptime
- Separated storage and compute
- Multi-language SDK's

Find more information [here](https://zilliz.com).

**Self Hosted vs SaaS**

Zilliz is a SaaS database, but offers an open-source solution, Milvus. Both options offer fast searches at the billion scale, but Zilliz handles data management for you. It automatically scales compute and storage resources and creates optimal indexes for your data. See the comparison [here](https://zilliz.com/doc/about_zilliz_cloud).

## Deploying the Database

Zilliz Cloud is deployable in a few simple steps. First, create an account [here](https://cloud.zilliz.com/signup). Once you have an account set up, follow the guide [here](https://zilliz.com/doc/quick_start) to set up a database and get the parameters needed for this application.

Environment Variables:

| Name                       | Required | Description                                                      |
|----------------------------| -------- |------------------------------------------------------------------|
| `DATASTORE`                | Yes      | Datastore name, set to `zilliz`                                  |
| `BEARER_TOKEN`             | Yes      | Your secret token                                                |
| `OPENAI_API_KEY`           | Yes      | Your OpenAI API key                                              |
| `ZILLIZ_COLLECTION`        | Optional | Zilliz collection name. Defaults to a random UUID                |
| `ZILLIZ_URI`               | Yes      | URI for the Zilliz instance                                      |
| `ZILLIZ_USER`              | Yes      | Zilliz username                                                  |
| `ZILLIZ_PASSWORD`          | Yes      | Zilliz password                                                  |
| `ZILLIZ_CONSISTENCY_LEVEL` | Optional | Data consistency level for the collection, defaults to `Bounded` |

## Running Zilliz Integration Tests

A suite of integration tests is available to verify the Zilliz integration. To run the tests, create a Zilliz database and update the environment variables.

Then, launch the test suite with this command:

```bash
pytest ./tests/datastore/providers/zilliz/test_zilliz_datastore.py
```



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/examples/authentication-methods/no-auth/ai-plugin.json
================================================
{
  "schema_version": "v1",
  "name_for_model": "retrieval",
  "name_for_human": "Retrieval Plugin",
  "description_for_model": "Plugin for searching through the user's documents (such as files, emails, and more) to find answers to questions and retrieve relevant information. Use it whenever a user asks something that might be found in their personal information.",
  "description_for_human": "Search through your documents.",
  "auth": {
    "type": "none"
  },
  "api": {
    "type": "openapi",
    "url": "https://your-app-url.com/.well-known/openapi.yaml"
  },
  "logo_url": "https://your-app-url.com/.well-known/logo.png",
  "contact_email": "hello@contact.com", 
  "legal_info_url": "hello@legal.com"
}




================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/examples/authentication-methods/no-auth/main.py
================================================
# This is a version of the main.py file found in ../../../server/main.py without authentication.
# Copy and paste this into the main file at ../../../server/main.py if you choose to use no authentication for your retrieval plugin.
from typing import Optional
import uvicorn
from fastapi import FastAPI, File, Form, HTTPException, Body, UploadFile
from fastapi.staticfiles import StaticFiles
from loguru import logger

from models.api import (
    DeleteRequest,
    DeleteResponse,
    QueryRequest,
    QueryResponse,
    UpsertRequest,
    UpsertResponse,
)
from datastore.factory import get_datastore
from services.file import get_document_from_file

from models.models import DocumentMetadata, Source


app = FastAPI()
app.mount("/.well-known", StaticFiles(directory=".well-known"), name="static")

# Create a sub-application, in order to access just the query endpoints in the OpenAPI schema, found at http://0.0.0.0:8000/sub/openapi.json when the app is running locally
sub_app = FastAPI(
    title="Retrieval Plugin API",
    description="A retrieval API for querying and filtering documents based on natural language queries and metadata",
    version="1.0.0",
    servers=[{"url": "https://your-app-url.com"}],
)
app.mount("/sub", sub_app)


@app.post(
    "/upsert-file",
    response_model=UpsertResponse,
)
async def upsert_file(
    file: UploadFile = File(...),
    metadata: Optional[str] = Form(None),
):
    try:
        metadata_obj = (
            DocumentMetadata.parse_raw(metadata)
            if metadata
            else DocumentMetadata(source=Source.file)
        )
    except:
        metadata_obj = DocumentMetadata(source=Source.file)

    document = await get_document_from_file(file, metadata_obj)

    try:
        ids = await datastore.upsert([document])
        return UpsertResponse(ids=ids)
    except Exception as e:
        logger.error(e)
        raise HTTPException(status_code=500, detail=f"str({e})")


@app.post(
    "/upsert",
    response_model=UpsertResponse,
)
async def upsert(
    request: UpsertRequest = Body(...),
):
    try:
        ids = await datastore.upsert(request.documents)
        return UpsertResponse(ids=ids)
    except Exception as e:
        logger.error(e)
        raise HTTPException(status_code=500, detail="Internal Service Error")


@app.post(
    "/query",
    response_model=QueryResponse,
)
async def query_main(
    request: QueryRequest = Body(...),
):
    try:
        results = await datastore.query(
            request.queries,
        )
        return QueryResponse(results=results)
    except Exception as e:
        logger.error(e)
        raise HTTPException(status_code=500, detail="Internal Service Error")


@sub_app.post(
    "/query",
    response_model=QueryResponse,
    description="Accepts search query objects with query and optional filter. Break down complex questions into sub-questions. Refine results by criteria, e.g. time / source, don't do this often. Split queries if ResponseTooLargeError occurs.",
)
async def query(
    request: QueryRequest = Body(...),
):
    try:
        results = await datastore.query(
            request.queries,
        )
        return QueryResponse(results=results)
    except Exception as e:
        logger.error(e)
        raise HTTPException(status_code=500, detail="Internal Service Error")


@app.delete(
    "/delete",
    response_model=DeleteResponse,
)
async def delete(
    request: DeleteRequest = Body(...),
):
    if not (request.ids or request.filter or request.delete_all):
        raise HTTPException(
            status_code=400,
            detail="One of ids, filter, or delete_all is required",
        )
    try:
        success = await datastore.delete(
            ids=request.ids,
            filter=request.filter,
            delete_all=request.delete_all,
        )
        return DeleteResponse(success=success)
    except Exception as e:
        logger.error(e)
        raise HTTPException(status_code=500, detail="Internal Service Error")


@app.on_event("startup")
async def startup():
    global datastore
    datastore = await get_datastore()


def start():
    uvicorn.run("server.main:app", host="0.0.0.0", port=8000, reload=True)



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/examples/authentication-methods/oauth/ai-plugin.json
================================================
{
  "schema_version": "v1",
  "name_for_model": "retrieval",
  "name_for_human": "Retrieval Plugin",
  "description_for_model": "Plugin for searching through the user's documents (such as files, emails, and more) to find answers to questions and retrieve relevant information. Use it whenever a user asks something that might be found in their personal information.",
  "description_for_human": "Search through your documents.",
  "auth" : {
    "type":"oauth",
    "client_url":"e.g. https://<your domain>/oauth/v2/authorize",
    "authorization_url":"e.g. https://<your domain>/api/oauth.v2.access",
    "scope":"search:read",
    "authorization_content_type":"application/x-www-form-urlencoded",
    "verification_tokens":{
			"openai":"<token from add plugin flow from the ChatGPT UI>"
    }
  },
	"api":{
    "url": "https://your-app-url.com/.well-known/openapi.yaml",
		"has_user_authentication":true,
		"type":"openapi"
	},
  "logo_url": "https://your-app-url.com/.well-known/logo.png",
  "contact_email": "hello@contact.com", 
  "legal_info_url": "hello@legal.com"
}



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/examples/authentication-methods/service-http/ai-plugin.json
================================================
{
  "schema_version": "v1",
  "name_for_model": "retrieval",
  "name_for_human": "Retrieval Plugin",
  "description_for_model": "Plugin for searching through the user's documents (such as files, emails, and more) to find answers to questions and retrieve relevant information. Use it whenever a user asks something that might be found in their personal information.",
  "description_for_human": "Search through your documents.",
  "auth":{
		"type":"service_http",
		"authorization_type":"bearer",
		"verification_tokens":{
			"openai":"<token from add plugin flow from the ChatGPT UI>"
		}
	},
	"api":{
    "url": "https://your-app-url.com/.well-known/openapi.yaml",
		"has_user_authentication":false,
		"type":"openapi"
	},
  "logo_url": "https://your-app-url.com/.well-known/logo.png",
  "contact_email": "hello@contact.com", 
  "legal_info_url": "hello@legal.com"
}



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/examples/authentication-methods/user-http/ai-plugin.json
================================================
{
  "schema_version": "v1",
  "name_for_model": "retrieval",
  "name_for_human": "Retrieval Plugin",
  "description_for_model": "Plugin for searching through the user's documents (such as files, emails, and more) to find answers to questions and retrieve relevant information. Use it whenever a user asks something that might be found in their personal information.",
  "description_for_human": "Search through your documents.",
  "auth": {
    "type": "user_http",
    "authorization_type": "bearer"
  },
  "api": {
    "type": "openapi",
    "url": "https://your-app-url.com/.well-known/openapi.yaml",
    "has_user_authentication": false
  },
  "logo_url": "https://your-app-url.com/.well-known/logo.png",
  "contact_email": "hello@contact.com", 
  "legal_info_url": "hello@legal.com"
}


================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/examples/docker/elasticsearch/README.md
================================================
## Running Elasticsearch

```bash
docker-compose up -d
```

should now be running at http://localhost:9200



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/examples/docker/elasticsearch/docker-compose.yaml
================================================
version: "3.7"

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.8.2
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - node.name=elasticsearch
      - xpack.security.enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
    ports:
      - "9200:9200"
    networks:
      - esnet
    volumes:
      - esdata:/usr/share/elasticsearch/data

networks:
  esnet:

volumes:
  esdata:
    driver: local



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/examples/docker/milvus/docker-compose.yaml
================================================
version: '3.5'

services:
  etcd:
    container_name: milvus-etcd
    image: quay.io/coreos/etcd:v3.5.0
    environment:
      - ETCD_AUTO_COMPACTION_MODE=revision
      - ETCD_AUTO_COMPACTION_RETENTION=1000
      - ETCD_QUOTA_BACKEND_BYTES=4294967296
      - ETCD_SNAPSHOT_COUNT=50000
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/etcd:/etcd
    command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd

  minio:
    container_name: milvus-minio
    image: minio/minio:RELEASE.2023-03-20T20-16-18Z
    environment:
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/minio:/minio_data
    command: minio server /minio_data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  standalone:
    container_name: milvus-standalone
    image: milvusdb/milvus:v2.2.5
    command: ["milvus", "run", "standalone"]
    environment:
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9000
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/milvus:/var/lib/milvus
    ports:
      - "19530:19530"
      - "9091:9091"
    depends_on:
      - "etcd"
      - "minio"

networks:
  default:
    name: milvus


================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/examples/docker/qdrant/README.md
================================================
# Running the Retrieval Plugin with Qdrant in Docker Containers

To set up the ChatGPT retrieval plugin with a single instance of a Qdrant vector database, follow these steps:

## Set Environment Variables

Set the following environment variables:

```bash
# Provide your own OpenAI API key in order to start.
export OPENAI_API_KEY="<your_OpenAI_API_key>"
# This is an example of a minimal token generated by https://jwt.io/
export BEARER_TOKEN="<your_bearer_token>"
```

## Run Qdrant and the Retrieval Plugin in Docker Containers

Both Docker containers might be launched with docker-compose:

```bash
docker-compose up -d
```

## Store the Documents

Store an initial batch of documents by calling the `/upsert` endpoint:

```bash
curl -X POST \
  -H "Content-type: application/json" \
  -H "Authorization: Bearer $BEARER_TOKEN" \
  --data-binary '@documents.json' \
  "http://localhost:80/upsert"
```

## Send a Test Query

You can query Qdrant to find relevant document chunks by calling the `/query` endpoint:

```bash
curl -X POST \
  -H "Content-type: application/json" \
  -H "Authorization: Bearer $BEARER_TOKEN" \
  --data-binary '@queries.json' \
  "http://localhost:80/query"
```



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/examples/docker/qdrant/docker-compose.yaml
================================================
services:
  retrieval-app:
    build:
      context: ../../../
      dockerfile: Dockerfile
    image: openai/chatgpt-retrieval-plugin
    ports:
      - "80:80"
    depends_on:
      - qdrant
    environment:
      DATASTORE: "qdrant"
      QDRANT_URL: "http://qdrant"
      BEARER_TOKEN: "${BEARER_TOKEN}"
      OPENAI_API_KEY: "${OPENAI_API_KEY}"
  qdrant:
    image: qdrant/qdrant:v1.0.3


================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/examples/docker/qdrant/documents.json
================================================
{
  "documents": [
    {
      "id": "openai",
      "text": "OpenAI is an AI research and deployment company. Our mission is to ensure that artificial general intelligence benefits all of humanity.",
      "metadata": {
        "created_at": "2023-03-14"
      }
    },
    {
      "id": "chatgpt",
      "text": "ChatGPT is a sibling model to InstructGPT, which is trained to follow an instruction in a prompt and provide a detailed response. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests."
    },
    {
      "id": "qdrant",
      "text": "Qdrant is a vector similarity engine & vector database. It deploys as an API service providing search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!",
      "metadata": {
        "created_at": "2023-03-14",
        "author": "Kacper Łukawski"
      }
    }
  ]
}


================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/examples/docker/qdrant/queries.json
================================================
{
  "queries": [
    {
      "query": "What vector database should I use?"
    }
  ]
}


================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/examples/docker/redis/docker-compose.yml
================================================
version: "3.9"

services:
  redis:
    image: redis/redis-stack-server:latest
    ports:
      - "6379:6379"
    volumes:
        - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "-h", "localhost", "-p", "6379", "ping"]
      interval: 2s
      timeout: 1m30s
      retries: 5
      start_period: 5s

volumes:
  redis_data:


================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/examples/memory/README.md
================================================
# ChatGPT Retrieval Plugin with Memory

This example demonstrates how to give ChatGPT the ability to remember information from conversations and store it in the retrieval plugin for later use. By allowing the model to access the `/upsert` endpoint, it can save snippets from the conversation to the vector database and retrieve them when needed.

## Setup

To enable ChatGPT to save information from conversations, follow these steps:

- Copy the contents of [openapi.yaml](openapi.yaml) into the main [openapi.yaml](../../.well-known/openapi.yaml) file.

- Copy the contents of [ai-plugin.json](ai-plugin.json) into the main [ai-plugin.json](../../.well-known/ai-plugin.json) file.

**Optional:** If you make any changes to the plugin instructions or metadata models, you can also copy the contents of [main.py](main.py) into the main [main.py](../../server/main.py) file. This will allow you to access the openapi.json at `http://0.0.0.0:8000/sub/openapi.json` when you run the app locally. You can convert from JSON to YAML format with [Swagger Editor](https://editor.swagger.io/). Alternatively, you can replace the openapi.yaml file with an openapi.json file.

After completing these steps, ChatGPT will be able to access your plugin's `/upsert` endpoint and save snippets from the conversation to the vector database. This enables the model to remember information from previous conversations and retrieve it when needed.



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/examples/memory/ai-plugin.json
================================================
{
    "schema_version": "v1",
    "name_for_model": "retrieval",
    "name_for_human": "Retrieval Plugin",
    "description_for_model": "Plugin for searching through the user's documents (such as files, emails, and more) to find answers to questions and retrieve relevant information. Use it whenever a user asks something that might be found in their personal information, or asks you to save information for later.",
    "description_for_human": "Search through your documents.",
    "auth": {
      "type": "user_http",
      "authorization_type": "bearer"
    },
    "api": {
      "type": "openapi",
      "url": "https://your-app-url.com/.well-known/openapi.yaml",
      "has_user_authentication": false
    },
    "logo_url": "https://your-app-url.com/.well-known/logo.png",
    "contact_email": "hello@contact.com", 
    "legal_info_url": "hello@legal.com"
  }


================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/examples/memory/main.py
================================================
# This is a version of the main.py file found in ../../server/main.py that also gives ChatGPT access to the upsert endpoint
# (allowing it to save information from the chat back to the vector) database.
# Copy and paste this into the main file at ../../server/main.py if you choose to give the model access to the upsert endpoint
# and want to access the openapi.json when you run the app locally at http://0.0.0.0:8000/sub/openapi.json.
import os
from typing import Optional
import uvicorn
from fastapi import FastAPI, File, Form, HTTPException, Depends, Body, UploadFile
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from fastapi.staticfiles import StaticFiles
from loguru import logger

from models.api import (
    DeleteRequest,
    DeleteResponse,
    QueryRequest,
    QueryResponse,
    UpsertRequest,
    UpsertResponse,
)
from datastore.factory import get_datastore
from services.file import get_document_from_file

from models.models import DocumentMetadata, Source


bearer_scheme = HTTPBearer()
BEARER_TOKEN = os.environ.get("BEARER_TOKEN")
assert BEARER_TOKEN is not None


def validate_token(credentials: HTTPAuthorizationCredentials = Depends(bearer_scheme)):
    if credentials.scheme != "Bearer" or credentials.credentials != BEARER_TOKEN:
        raise HTTPException(status_code=401, detail="Invalid or missing token")
    return credentials


app = FastAPI()
app.mount("/.well-known", StaticFiles(directory=".well-known"), name="static")

# Create a sub-application, in order to access just the upsert and query endpoints in the OpenAPI schema, found at http://0.0.0.0:8000/sub/openapi.json when the app is running locally
sub_app = FastAPI(
    title="Retrieval Plugin API",
    description="A retrieval API for querying and filtering documents based on natural language queries and metadata",
    version="1.0.0",
    servers=[{"url": "https://your-app-url.com"}],
    dependencies=[Depends(validate_token)],
)
app.mount("/sub", sub_app)


@app.post(
    "/upsert-file",
    response_model=UpsertResponse,
)
async def upsert_file(
    file: UploadFile = File(...),
    metadata: Optional[str] = Form(None),
):
    try:
        metadata_obj = (
            DocumentMetadata.parse_raw(metadata)
            if metadata
            else DocumentMetadata(source=Source.file)
        )
    except:
        metadata_obj = DocumentMetadata(source=Source.file)

    document = await get_document_from_file(file, metadata_obj)

    try:
        ids = await datastore.upsert([document])
        return UpsertResponse(ids=ids)
    except Exception as e:
        logger.error(e)
        raise HTTPException(status_code=500, detail=f"str({e})")


@app.post(
    "/upsert",
    response_model=UpsertResponse,
)
async def upsert_main(
    request: UpsertRequest = Body(...),
    token: HTTPAuthorizationCredentials = Depends(validate_token),
):
    try:
        ids = await datastore.upsert(request.documents)
        return UpsertResponse(ids=ids)
    except Exception as e:
        logger.error(e)
        raise HTTPException(status_code=500, detail="Internal Service Error")


@sub_app.post(
    "/upsert",
    response_model=UpsertResponse,
    # NOTE: We are describing the shape of the API endpoint input due to a current limitation in parsing arrays of objects from OpenAPI schemas. This will not be necessary in the future.
    description="Save chat information. Accepts an array of documents with text (potential questions + conversation text), metadata (source 'chat' and timestamp, no ID as this will be generated). Confirm with the user before saving, ask for more details/context.",
)
async def upsert(
    request: UpsertRequest = Body(...),
    token: HTTPAuthorizationCredentials = Depends(validate_token),
):
    try:
        ids = await datastore.upsert(request.documents)
        return UpsertResponse(ids=ids)
    except Exception as e:
        logger.error(e)
        raise HTTPException(status_code=500, detail="Internal Service Error")


@app.post(
    "/query",
    response_model=QueryResponse,
)
async def query_main(
    request: QueryRequest = Body(...),
    token: HTTPAuthorizationCredentials = Depends(validate_token),
):
    try:
        results = await datastore.query(
            request.queries,
        )
        return QueryResponse(results=results)
    except Exception as e:
        logger.error(e)
        raise HTTPException(status_code=500, detail="Internal Service Error")


@sub_app.post(
    "/query",
    response_model=QueryResponse,
    # NOTE: We are describing the shape of the API endpoint input due to a current limitation in parsing arrays of objects from OpenAPI schemas. This will not be necessary in the future.
    description="Accepts search query objects array each with query and optional filter. Break down complex questions into sub-questions. Refine results by criteria, e.g. time / source, don't do this often. Split queries if ResponseTooLargeError occurs.",
)
async def query(
    request: QueryRequest = Body(...),
    token: HTTPAuthorizationCredentials = Depends(validate_token),
):
    try:
        results = await datastore.query(
            request.queries,
        )
        return QueryResponse(results=results)
    except Exception as e:
        logger.error(e)
        raise HTTPException(status_code=500, detail="Internal Service Error")


@app.delete(
    "/delete",
    response_model=DeleteResponse,
)
async def delete(
    request: DeleteRequest = Body(...),
    token: HTTPAuthorizationCredentials = Depends(validate_token),
):
    if not (request.ids or request.filter or request.delete_all):
        raise HTTPException(
            status_code=400,
            detail="One of ids, filter, or delete_all is required",
        )
    try:
        success = await datastore.delete(
            ids=request.ids,
            filter=request.filter,
            delete_all=request.delete_all,
        )
        return DeleteResponse(success=success)
    except Exception as e:
        logger.error(e)
        raise HTTPException(status_code=500, detail="Internal Service Error")


@app.on_event("startup")
async def startup():
    global datastore
    datastore = await get_datastore()


def start():
    uvicorn.run("server.main:app", host="0.0.0.0", port=8000, reload=True)



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/examples/memory/openapi.yaml
================================================
openapi: 3.0.2
info:
  title: Retrieval Plugin API
  description: A retrieval API for querying and filtering documents based on natural language queries and metadata
  version: 1.0.0
servers:
  - url: https://your-app-url.com
paths:
  /upsert:
    post:
      summary: Upsert
      description: Save chat information. Accepts an array of documents with text (potential questions + conversation text), metadata (source 'chat' and timestamp, no ID as this will be generated). Confirm with the user before saving, ask for more details/context.
      operationId: upsert_upsert_post
      requestBody:
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/UpsertRequest"
        required: true
      responses:
        "200":
          description: Successful Response
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/UpsertResponse"
        "422":
          description: Validation Error
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/HTTPValidationError"
      security:
        - HTTPBearer: []
  /query:
    post:
      summary: Query
      description: Accepts search query objects array each with query and optional filter. Break down complex questions into sub-questions. Refine results by criteria, e.g. time / source, don't do this often. Split queries if ResponseTooLargeError occurs.
      operationId: query_query_post
      requestBody:
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/QueryRequest"
        required: true
      responses:
        "200":
          description: Successful Response
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/QueryResponse"
        "422":
          description: Validation Error
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/HTTPValidationError"
      security:
        - HTTPBearer: []
components:
  schemas:
    Document:
      title: Document
      required:
        - text
      type: object
      properties:
        id:
          title: Id
          type: string
        text:
          title: Text
          type: string
        metadata:
          $ref: "#/components/schemas/DocumentMetadata"
    DocumentChunkMetadata:
      title: DocumentChunkMetadata
      type: object
      properties:
        source:
          $ref: "#/components/schemas/Source"
        source_id:
          title: Source Id
          type: string
        url:
          title: Url
          type: string
        created_at:
          title: Created At
          type: string
        author:
          title: Author
          type: string
        document_id:
          title: Document Id
          type: string
    DocumentChunkWithScore:
      title: DocumentChunkWithScore
      required:
        - text
        - metadata
        - score
      type: object
      properties:
        id:
          title: Id
          type: string
        text:
          title: Text
          type: string
        metadata:
          $ref: "#/components/schemas/DocumentChunkMetadata"
        embedding:
          title: Embedding
          type: array
          items:
            type: number
        score:
          title: Score
          type: number
    DocumentMetadata:
      title: DocumentMetadata
      type: object
      properties:
        source:
          $ref: "#/components/schemas/Source"
        source_id:
          title: Source Id
          type: string
        url:
          title: Url
          type: string
        created_at:
          title: Created At
          type: string
        author:
          title: Author
          type: string
    DocumentMetadataFilter:
      title: DocumentMetadataFilter
      type: object
      properties:
        document_id:
          title: Document Id
          type: string
        source:
          $ref: "#/components/schemas/Source"
        source_id:
          title: Source Id
          type: string
        author:
          title: Author
          type: string
        start_date:
          title: Start Date
          type: string
        end_date:
          title: End Date
          type: string
    HTTPValidationError:
      title: HTTPValidationError
      type: object
      properties:
        detail:
          title: Detail
          type: array
          items:
            $ref: "#/components/schemas/ValidationError"
    Query:
      title: Query
      required:
        - query
      type: object
      properties:
        query:
          title: Query
          type: string
        filter:
          $ref: "#/components/schemas/DocumentMetadataFilter"
        top_k:
          title: Top K
          type: integer
          default: 3
    QueryRequest:
      title: QueryRequest
      required:
        - queries
      type: object
      properties:
        queries:
          title: Queries
          type: array
          items:
            $ref: "#/components/schemas/Query"
    QueryResponse:
      title: QueryResponse
      required:
        - results
      type: object
      properties:
        results:
          title: Results
          type: array
          items:
            $ref: "#/components/schemas/QueryResult"
    QueryResult:
      title: QueryResult
      required:
        - query
        - results
      type: object
      properties:
        query:
          title: Query
          type: string
        results:
          title: Results
          type: array
          items:
            $ref: "#/components/schemas/DocumentChunkWithScore"
    Source:
      title: Source
      enum:
        - email
        - file
        - chat
      type: string
      description: An enumeration.
    UpsertRequest:
      title: UpsertRequest
      required:
        - documents
      type: object
      properties:
        documents:
          title: Documents
          type: array
          items:
            $ref: "#/components/schemas/Document"
    UpsertResponse:
      title: UpsertResponse
      required:
        - ids
      type: object
      properties:
        ids:
          title: Ids
          type: array
          items:
            type: string
    ValidationError:
      title: ValidationError
      required:
        - loc
        - msg
        - type
      type: object
      properties:
        loc:
          title: Location
          type: array
          items:
            anyOf:
              - type: string
              - type: integer
        msg:
          title: Message
          type: string
        type:
          title: Error Type
          type: string
  securitySchemes:
    HTTPBearer:
      type: http
      scheme: bearer



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/examples/providers/elasticsearch/search.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Using Elasticsearch as a datastore

In this walkthrough we will see how to use the retrieval API with a Elasticsearch datastore for *search / question-answering*.

Before running this notebook you should have already initialized the retrieval API and have it running locally or elsewhere. See readme for instructions on how to do this.
"""

"""
## App Quickstart
"""

"""
1. Install Python 3.10 if not already installed.

2. Clone the `retrieval-app` repository:

```
git clone git@github.com:openai/retrieval-app.git
```

3. Navigate to the app directory:

```
cd /path/to/retrieval-app
```

4. Install `poetry`:

```
pip install poetry
```

5. Create a new virtual environment:

```
poetry env use python3.10
```

6. Install the `retrieval-app` dependencies:

```
poetry install
```

7. Set app environment variables:

* `BEARER_TOKEN`: Secret token used by the app to authorize incoming requests. We will later include this in the request `headers`. The token can be generated however you prefer, such as using [jwt.io](https://jwt.io/).

* `OPENAI_API_KEY`: The OpenAI API key used for generating embeddings with the `text-embedding-ada-002` model. [Get an API key here](https://platform.openai.com/account/api-keys)!

8. Set Elasticsearch-specific environment variables:

* `DATASTORE`: set to `elasticsearch`.

9. Set the Elasticsearch connection specific environment variables. Either set `ELASTICSEARCH_CLOUD_ID` or `ELASTICSEARCH_URL`.
* `ELASTICSEARCH_CLOUD_ID`: Set to your deployment cloud id. You can find this in the [Elasticsearch console](https://cloud.elastic.co).

* `ELASTICSEARCH_URL`: Set to your Elasticsearch URL, looks like `https://<username>:<password>@<host>:<port>`. You can find this in the [Elasticsearch console](https://cloud.elastic.co).

10. Set the Elasticsearch authentication specific environment variables. Either set `ELASTICSEARCH_USERNAME` and `ELASTICSEARCH_PASSWORD` or `ELASTICSEARCH_API_KEY`.

* `ELASTICSEARCH_USERNAME`: Set to your Elasticsearch username. You can find this in the [Elasticsearch console](https://cloud.elastic.co). Typically this is set to `elastic`.

* `ELASTICSEARCH_PASSWORD`: Set to your Elasticsearch password. You can find this in the [Elasticsearch console](https://cloud.elastic.co) in security.

* `ELASTICSEARCH_API_KEY`: Set to your Elasticsearch API key. You can set one up in Kibana Stack management page.

11. Set the Elasticsearch index specific environment variables.

* `ELASTICSEARCH_INDEX`: Set to the name of the Elasticsearch index you want to use.

12. Run the app with:

```
poetry run start
```

If running the app locally you should see something like:

```
INFO:     Uvicorn running on http://0.0.0.0:8000
INFO:     Application startup complete.
```

In that case, the app is automatically connected to our index (specified by `ELASTICSEARCH_INDEX`), if no index with that name existed beforehand, the app creates one for us.

Now we're ready to move on to populating our index with some data.
"""

"""
## Required Libraries
"""

"""
There are a few Python libraries we must `pip install` for this notebook to run, those are:
"""

!pip install -qU datasets pandas tqdm
# Output:
#   

#   [1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m A new release of pip is available: [0m[31;49m23.2[0m[39;49m -> [0m[32;49m23.2.1[0m

#   [1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m To update, run: [0m[32;49mpip install --upgrade pip[0m


"""
## Preparing Data
"""

"""
In this example, we will use the **S**tanford **Qu**estion **A**nswering **D**ataset (SQuAD2), which we download from Hugging Face Datasets.
"""

from datasets import load_dataset

data = load_dataset("squad_v2", split="train")
data

"""
Transform the data into a Pandas dataframe for simpler preprocessing.
"""

data = data.to_pandas()
data.head()
# Output:
#                            id    title  \

#   0  56be85543aeaaa14008c9063  Beyoncé   

#   1  56be85543aeaaa14008c9065  Beyoncé   

#   2  56be85543aeaaa14008c9066  Beyoncé   

#   3  56bf6b0f3aeaaa14008c9601  Beyoncé   

#   4  56bf6b0f3aeaaa14008c9602  Beyoncé   

#   

#                                                context  \

#   0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   

#   1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   

#   2  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   

#   3  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   

#   4  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   

#   

#                                               question  \

#   0           When did Beyonce start becoming popular?   

#   1  What areas did Beyonce compete in when she was...   

#   2  When did Beyonce leave Destiny's Child and bec...   

#   3      In what city and state did Beyonce  grow up?    

#   4         In which decade did Beyonce become famous?   

#   

#                                                answers  

#   0  {'text': ['in the late 1990s'], 'answer_start'...  

#   1  {'text': ['singing and dancing'], 'answer_star...  

#   2          {'text': ['2003'], 'answer_start': [526]}  

#   3  {'text': ['Houston, Texas'], 'answer_start': [...  

#   4    {'text': ['late 1990s'], 'answer_start': [276]}  

"""
The dataset contains a lot of duplicate `context` paragraphs, this is because each `context` can have many relevant questions. We don't want these duplicates so we remove like so:
"""

data = data.drop_duplicates(subset=["context"])
print(len(data))
data.head()
# Output:
#   19029

#                             id    title  \

#   0   56be85543aeaaa14008c9063  Beyoncé   

#   15  56be86cf3aeaaa14008c9076  Beyoncé   

#   27  56be88473aeaaa14008c9080  Beyoncé   

#   39  56be892d3aeaaa14008c908b  Beyoncé   

#   52  56be8a583aeaaa14008c9094  Beyoncé   

#   

#                                                 context  \

#   0   Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   

#   15  Following the disbandment of Destiny's Child i...   

#   27  A self-described "modern-day feminist", Beyonc...   

#   39  Beyoncé Giselle Knowles was born in Houston, T...   

#   52  Beyoncé attended St. Mary's Elementary School ...   

#   

#                                                question  \

#   0            When did Beyonce start becoming popular?   

#   15  After her second solo album, what other entert...   

#   27  In her music, what are some recurring elements...   

#   39  Beyonce's younger sibling also sang with her i...   

#   52             What town did Beyonce go to school in?   

#   

#                                                 answers  

#   0   {'text': ['in the late 1990s'], 'answer_start'...  

#   15        {'text': ['acting'], 'answer_start': [207]}  

#   27  {'text': ['love, relationships, and monogamy']...  

#   39  {'text': ['Destiny's Child'], 'answer_start': ...  

#   52  {'text': ['Fredericksburg'], 'answer_start': [...  

"""
The format required by the apps `upsert` function is a list of documents like:

```json
[
    {
        "id": "abc",
        "text": "some important document text",
        "metadata": {
            "field1": "optional metadata goes here",
            "field2": 54
        }
    },
    {
        "id": "123",
        "text": "some other important text",
        "metadata": {
            "field1": "another metadata",
            "field2": 71,
            "field3": "not all metadatas need the same structure"
        }
    }
    ...
]
```

Every document *must* have a `"text"` field. The `"id"` and `"metadata"` fields are optional.

To create this format for our SQuAD data we do:
"""

documents = [
    {
        'id': r['id'],
        'text': r['context'],
        'metadata': {
            'title': r['title']
        }
    } for r in data.to_dict(orient='records')
]
documents[:3]

"""
### Indexing the Docs
"""

"""
Now, it's time to initiate the indexing process, also known as upserting, for our documents. To perform these requests to the retrieval app API, we must provide authorization using the BEARER_TOKEN we defined earlier. Below is how we accomplish this:
"""

import os

BEARER_TOKEN = os.environ.get("BEARER_TOKEN") or "BEARER_TOKEN_HERE"

headers = {
    "Authorization": f"Bearer {BEARER_TOKEN}"
}

"""
Now we will execute bulk inserts in batches set by the `batch_size`.

Now that all our SQuAD2 records have been successfully indexed, we can proceed with the querying phase.
"""

from tqdm.auto import tqdm
import requests
from requests.adapters import HTTPAdapter, Retry

batch_size = 100
endpoint_url = "http://localhost:8000"
s = requests.Session()

# we setup a retry strategy to retry on 5xx errors
retries = Retry(
    total=5,  # number of retries before raising error
    backoff_factor=0.1,
    status_forcelist=[500, 502, 503, 504]
)
s.mount('http://', HTTPAdapter(max_retries=retries))

for i in tqdm(range(0, 10, batch_size)):
    i_end = min(len(documents), i+batch_size)
    # make post request that allows up to 5 retries
    res = s.post(
        f"{endpoint_url}/upsert",
        headers=headers,
        json={
            "documents": documents[i:i_end]
        }
    )
# Output:
#   100%|██████████| 1/1 [00:16<00:00, 16.88s/it]


"""
### Making Queries
"""

"""
By passing one or more queries to the /query endpoint, we can easily conduct a query on the datastore. For this task, we can utilize a few questions from SQuAD2.
"""

queries = data['question'].tolist()
# format into the structure needed by the /query endpoint
queries = [{'query': queries[i]} for i in range(len(queries))]
len(queries)
# Output:
#   19029

res = requests.post(
    "http://0.0.0.0:8000/query",
    headers=headers,
    json={
        'queries': queries[:3]
    }
)
res
# Output:
#   <Response [200]>

"""
At this point, we have the ability to iterate through the responses and observe the outcomes obtained for each query:
"""

for query_result in res.json()['results']:
    query = query_result['query']
    answers = []
    scores = []
    for result in query_result['results']:
        answers.append(result['text'])
        scores.append(round(result['score'], 2))
    print("-"*70+"\n"+query+"\n\n"+"\n".join([f"{s}: {a}" for a, s in zip(answers, scores)])+"\n"+"-"*70+"\n\n")
# Output:
#   ----------------------------------------------------------------------

#   When did Beyonce start becoming popular?

#   

#   0.93: On December 13, 2013, Beyoncé unexpectedly released her eponymous fifth studio album on the iTunes Store without any prior announcement or promotion. The album debuted atop the Billboard 200 chart, giving Beyoncé her fifth consecutive number-one album in the US. This made her the first woman in the chart's history to have her first five studio albums debut at number one. Beyoncé received critical acclaim and commercial success, selling one million digital copies worldwide in six days; The New York Times noted the album's unconventional, unexpected release as significant. Musically an electro-R&B album, it concerns darker themes previously unexplored in her work, such as "bulimia, postnatal depression [and] the fears and insecurities of marriage and motherhood". The single "Drunk in Love", featuring Jay Z, peaked at number two on the Billboard Hot 100 chart.

#   0.93: Beyoncé's first solo recording was a feature on Jay Z's "'03 Bonnie & Clyde" that was released in October 2002, peaking at number four on the U.S. Billboard Hot 100 chart. Her first solo album Dangerously in Love was released on June 24, 2003, after Michelle Williams and Kelly Rowland had released their solo efforts. The album sold 317,000 copies in its first week, debuted atop the Billboard 200, and has since sold 11 million copies worldwide. The album's lead single, "Crazy in Love", featuring Jay Z, became Beyoncé's first number-one single as a solo artist in the US. The single "Baby Boy" also reached number one, and singles, "Me, Myself and I" and "Naughty Girl", both reached the top-five.

#   0.93: Beyoncé is believed to have first started a relationship with Jay Z after a collaboration on "'03 Bonnie & Clyde", which appeared on his seventh album The Blueprint 2: The Gift & The Curse (2002). Beyoncé appeared as Jay Z's girlfriend in the music video for the song, which would further fuel speculation of their relationship. On April 4, 2008, Beyoncé and Jay Z were married without publicity. As of April 2014, the couple have sold a combined 300 million records together. The couple are known for their private relationship, although they have appeared to become more relaxed in recent years. Beyoncé suffered a miscarriage in 2010 or 2011, describing it as "the saddest thing" she had ever endured. She returned to the studio and wrote music in order to cope with the loss.

#   ----------------------------------------------------------------------

#   

#   

#   ----------------------------------------------------------------------

#   After her second solo album, what other entertainment venture did Beyonce explore?

#   

#   0.93: Following the disbandment of Destiny's Child in June 2005, she released her second solo album, B'Day (2006), which contained hits "Déjà Vu", "Irreplaceable", and "Beautiful Liar". Beyoncé also ventured into acting, with a Golden Globe-nominated performance in Dreamgirls (2006), and starring roles in The Pink Panther (2006) and Obsessed (2009). Her marriage to rapper Jay Z and portrayal of Etta James in Cadillac Records (2008) influenced her third album, I Am... Sasha Fierce (2008), which saw the birth of her alter-ego Sasha Fierce and earned a record-setting six Grammy Awards in 2010, including Song of the Year for "Single Ladies (Put a Ring on It)".

#   0.92: Beyoncé announced a hiatus from her music career in January 2010, heeding her mother's advice, "to live life, to be inspired by things again". During the break she and her father parted ways as business partners. Beyoncé's musical break lasted nine months and saw her visit multiple European cities, the Great Wall of China, the Egyptian pyramids, Australia, English music festivals and various museums and ballet performances.

#   0.92: Beyoncé took a hiatus from music in 2010 and took over management of her career; her fourth album 4 (2011) was subsequently mellower in tone, exploring 1970s funk, 1980s pop, and 1990s soul. Her critically acclaimed fifth studio album, Beyoncé (2013), was distinguished from previous releases by its experimental production and exploration of darker themes.

#   ----------------------------------------------------------------------

#   

#   

#   ----------------------------------------------------------------------

#   In her music, what are some recurring elements in them?

#   

#   0.91: Beyoncé's music is generally R&B, but she also incorporates pop, soul and funk into her songs. 4 demonstrated Beyoncé's exploration of 90s-style R&B, as well as further use of soul and hip hop than compared to previous releases. While she almost exclusively releases English songs, Beyoncé recorded several Spanish songs for Irreemplazable (re-recordings of songs from B'Day for a Spanish-language audience), and the re-release of B'Day. To record these, Beyoncé was coached phonetically by American record producer Rudy Perez.

#   0.9: The feminism and female empowerment themes on Beyoncé's second solo album B'Day were inspired by her role in Dreamgirls and by singer Josephine Baker. Beyoncé paid homage to Baker by performing "Déjà Vu" at the 2006 Fashion Rocks concert wearing Baker's trademark mini-hula skirt embellished with fake bananas. Beyoncé's third solo album I Am... Sasha Fierce was inspired by Jay Z and especially by Etta James, whose "boldness" inspired Beyoncé to explore other musical genres and styles. Her fourth solo album, 4, was inspired by Fela Kuti, 1990s R&B, Earth, Wind & Fire, DeBarge, Lionel Richie, Teena Marie with additional influences by The Jackson 5, New Edition, Adele, Florence and the Machine, and Prince.

#   0.9: She has received co-writing credits for most of the songs recorded with Destiny's Child and her solo efforts. Her early songs were personally driven and female-empowerment themed compositions like "Independent Women" and "Survivor", but after the start of her relationship with Jay Z she transitioned to more man-tending anthems such as "Cater 2 U". Beyoncé has also received co-producing credits for most of the records in which she has been involved, especially during her solo efforts. However, she does not formulate beats herself, but typically comes up with melodies and ideas during production, sharing them with producers.

#   ----------------------------------------------------------------------

#   

#   


"""
The top results are all relevant as we would have hoped. We can see that the `score` is a measure of how relevant the document is to the query. The higher the score the more relevant the document is to the query.
"""



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/examples/providers/pinecone/semantic-search.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Using the Pinecone Retrieval App

In this walkthrough we will see how to use the retrieval API with a Pinecone datastore for *semantic search / question-answering*.

Before running this notebook you should have already initialized the retrieval API and have it running locally or elsewhere. The full instructions for doing this are found in the [project README]().

We will summarize the instructions (specific to the Pinecone datastore) before moving on to the walkthrough.
"""

"""
## App Quickstart
"""

"""
1. Install Python 3.10 if not already installed.

2. Clone the `retrieval-app` repository:

```
git clone git@github.com:openai/retrieval-app.git
```

3. Navigate to the app directory:

```
cd /path/to/retrieval-app
```

4. Install `poetry`:

```
pip install poetry
```

5. Create a new virtual environment:

```
poetry env use python3.10
```

6. Install the `retrieval-app` dependencies:

```
poetry install
```

7. Set app environment variables:

* `BEARER_TOKEN`: Secret token used by the app to authorize incoming requests. We will later include this in the request `headers`. The token can be generated however you prefer, such as using [jwt.io](https://jwt.io/).

* `OPENAI_API_KEY`: The OpenAI API key used for generating embeddings with the `text-embedding-ada-002` model. [Get an API key here](https://platform.openai.com/account/api-keys)!

8. Set Pinecone-specific environment variables:

* `DATASTORE`: set to `pinecone`.

* `PINECONE_API_KEY`: Set to your Pinecone API key. This requires a free Pinecone account and can be [found in the Pinecone console](https://app.pinecone.io/).

* `PINECONE_ENVIRONMENT`: Set to your Pinecone environment, looks like `us-east1-gcp`, `us-west1-aws`, and can be found next to your API key in the [Pinecone console](https://app.pinecone.io/).

* `PINECONE_INDEX`: Set this to your chosen index name. The name you choose is your choice, we just recommend setting it to something descriptive like `"openai-retrieval-app"`. *Note that index names are restricted to alphanumeric characters, `"-"`, and can contain a maximum of 45 characters.*

8. Run the app with:

```
poetry run start
```

If running the app locally you should see something like:

```
INFO:     Uvicorn running on http://0.0.0.0:8000
INFO:     Application startup complete.
```

In that case, the app is automatically connected to our index (specified by `PINECONE_INDEX`), if no index with that name existed beforehand, the app creates one for us.

Now we're ready to move on to populating our index with some data.
"""

"""
## Required Libraries
"""

"""
There are a few Python libraries we must `pip install` for this notebook to run, those are:
"""

!pip install -qU datasets pandas tqdm

"""
## Preparing Data
"""

"""
In this example, we will use the **S**tanford **Qu**estion **A**nswering **D**ataset (SQuAD), which we download from Hugging Face Datasets.
"""

from datasets import load_dataset

data = load_dataset("squad", split="train")
data
# Output:
#   Found cached dataset squad (/Users/jamesbriggs/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)

#   Dataset({

#       features: ['id', 'title', 'context', 'question', 'answers'],

#       num_rows: 87599

#   })

"""
Convert to Pandas dataframe for easier preprocessing steps.
"""

data = data.to_pandas()
data.head()
# Output:
#                            id                     title  \

#   0  5733be284776f41900661182  University_of_Notre_Dame   

#   1  5733be284776f4190066117f  University_of_Notre_Dame   

#   2  5733be284776f41900661180  University_of_Notre_Dame   

#   3  5733be284776f41900661181  University_of_Notre_Dame   

#   4  5733be284776f4190066117e  University_of_Notre_Dame   

#   

#                                                context  \

#   0  Architecturally, the school has a Catholic cha...   

#   1  Architecturally, the school has a Catholic cha...   

#   2  Architecturally, the school has a Catholic cha...   

#   3  Architecturally, the school has a Catholic cha...   

#   4  Architecturally, the school has a Catholic cha...   

#   

#                                               question  \

#   0  To whom did the Virgin Mary allegedly appear i...   

#   1  What is in front of the Notre Dame Main Building?   

#   2  The Basilica of the Sacred heart at Notre Dame...   

#   3                  What is the Grotto at Notre Dame?   

#   4  What sits on top of the Main Building at Notre...   

#   

#                                                answers  

#   0  {'text': ['Saint Bernadette Soubirous'], 'answ...  

#   1  {'text': ['a copper statue of Christ'], 'answe...  

#   2  {'text': ['the Main Building'], 'answer_start'...  

#   3  {'text': ['a Marian place of prayer and reflec...  

#   4  {'text': ['a golden statue of the Virgin Mary'...  

"""
The dataset contains a lot of duplicate `context` paragraphs, this is because each `context` can have many relevant questions. We don't want these duplicates so we remove like so:
"""

data = data.drop_duplicates(subset=["context"])
print(len(data))
data.head()
# Output:
#   18891

#                             id                     title  \

#   0   5733be284776f41900661182  University_of_Notre_Dame   

#   5   5733bf84d058e614000b61be  University_of_Notre_Dame   

#   10  5733bed24776f41900661188  University_of_Notre_Dame   

#   15  5733a6424776f41900660f51  University_of_Notre_Dame   

#   20  5733a70c4776f41900660f64  University_of_Notre_Dame   

#   

#                                                 context  \

#   0   Architecturally, the school has a Catholic cha...   

#   5   As at most other universities, Notre Dame's st...   

#   10  The university is the major seat of the Congre...   

#   15  The College of Engineering was established in ...   

#   20  All of Notre Dame's undergraduate students are...   

#   

#                                                question  \

#   0   To whom did the Virgin Mary allegedly appear i...   

#   5   When did the Scholastic Magazine of Notre dame...   

#   10  Where is the headquarters of the Congregation ...   

#   15  How many BS level degrees are offered in the C...   

#   20  What entity provides help with the management ...   

#   

#                                                 answers  

#   0   {'text': ['Saint Bernadette Soubirous'], 'answ...  

#   5   {'text': ['September 1876'], 'answer_start': [...  

#   10          {'text': ['Rome'], 'answer_start': [119]}  

#   15         {'text': ['eight'], 'answer_start': [487]}  

#   20  {'text': ['Learning Resource Center'], 'answer...  

"""
The format required by the apps `upsert` function is a list of documents like:

```json
[
    {
        "id": "abc",
        "text": "some important document text",
        "metadata": {
            "field1": "optional metadata goes here",
            "field2": 54
        }
    },
    {
        "id": "123",
        "text": "some other important text",
        "metadata": {
            "field1": "another metadata",
            "field2": 71,
            "field3": "not all metadatas need the same structure"
        }
    }
    ...
]
```

Every document *must* have a `"text"` field. The `"id"` and `"metadata"` fields are optional.

To create this format for our SQuAD data we do:
"""

documents = [
    {
        'id': r['id'],
        'text': r['context'],
        'metadata': {
            'title': r['title']
        }
    } for r in data.to_dict(orient='records')
]
documents[:3]
# Output:
#   [{'id': '5733be284776f41900661182',

#     'text': 'Architecturally, the school has a Catholic character. Atop the Main Building\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend "Venite Ad Me Omnes". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',

#     'metadata': {'title': 'University_of_Notre_Dame'}},

#    {'id': '5733bf84d058e614000b61be',

#     'text': "As at most other universities, Notre Dame's students run a number of news media outlets. The nine student-run outlets include three newspapers, both a radio and television station, and several magazines and journals. Begun as a one-page journal in September 1876, the Scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the United States. The other magazine, The Juggler, is released twice a year and focuses on student literature and artwork. The Dome yearbook is published annually. The newspapers have varying publication interests, with The Observer published daily and mainly reporting university and other news, and staffed by students from both Notre Dame and Saint Mary's College. Unlike Scholastic and The Dome, The Observer is an independent publication and does not have a faculty advisor or any editorial oversight from the University. In 1987, when some students believed that The Observer began to show a conservative bias, a liberal newspaper, Common Sense was published. Likewise, in 2003, when other students believed that the paper showed a liberal bias, the conservative paper Irish Rover went into production. Neither paper is published as often as The Observer; however, all three are distributed to all students. Finally, in Spring 2008 an undergraduate journal for political science research, Beyond Politics, made its debut.",

#     'metadata': {'title': 'University_of_Notre_Dame'}},

#    {'id': '5733bed24776f41900661188',

#     'text': 'The university is the major seat of the Congregation of Holy Cross (albeit not its official headquarters, which are in Rome). Its main seminary, Moreau Seminary, is located on the campus across St. Joseph lake from the Main Building. Old College, the oldest building on campus and located near the shore of St. Mary lake, houses undergraduate seminarians. Retired priests and brothers reside in Fatima House (a former retreat center), Holy Cross House, as well as Columba Hall near the Grotto. The university through the Moreau Seminary has ties to theologian Frederick Buechner. While not Catholic, Buechner has praised writers from Notre Dame and Moreau Seminary created a Buechner Prize for Preaching.',

#     'metadata': {'title': 'University_of_Notre_Dame'}}]

"""
### Indexing the Docs
"""

"""
We're now ready to begin indexing (or *upserting*) our `documents`. To make these requests to the retrieval app API, we will need to provide authorization in the form of the `BEARER_TOKEN` we set earlier. We do this below:
"""

import os

BEARER_TOKEN = os.environ.get("BEARER_TOKEN") or "BEARER_TOKEN_HERE"

"""
Use the `BEARER_TOKEN` to create our authorization `headers`:
"""

headers = {
    "Authorization": f"Bearer {BEARER_TOKEN}"
}

"""
We'll perform the upsert in batches of `batch_size`. Make sure that the `endpoint_url` variable is set to the correct location for your running *retrieval-app* API.
"""

from tqdm.auto import tqdm
import requests
from requests.adapters import HTTPAdapter, Retry

batch_size = 100
endpoint_url = "http://localhost:8000"
s = requests.Session()

# we setup a retry strategy to retry on 5xx errors
retries = Retry(
    total=5,  # number of retries before raising error
    backoff_factor=0.1,
    status_forcelist=[500, 502, 503, 504]
)
s.mount('http://', HTTPAdapter(max_retries=retries))

for i in tqdm(range(0, len(documents), batch_size)):
    i_end = min(len(documents), i+batch_size)
    # make post request that allows up to 5 retries
    res = s.post(
        f"{endpoint_url}/upsert",
        headers=headers,
        json={
            "documents": documents[i:i_end]
        }
    )
# Output:
#     0%|          | 0/10 [00:00<?, ?it/s]

"""
With that our SQuAD records have all been indexed and we can move on to querying.
"""

"""
### Making Queries
"""

"""
To query the datastore all we need to do is pass one or more queries to the `/query` endpoint. We can take a few questions from SQuAD:
"""

queries = data['question'].tolist()
# format into the structure needed by the /query endpoint
queries = [{'query': queries[i]} for i in range(len(queries))]
len(queries)
# Output:
#   18891

"""
We will use just the first *three* questions:
"""

queries[:3]
# Output:
#   [{'query': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?'},

#    {'query': 'When did the Scholastic Magazine of Notre dame begin publishing?'},

#    {'query': 'Where is the headquarters of the Congregation of the Holy Cross?'}]

res = requests.post(
    "http://0.0.0.0:8000/query",
    headers=headers,
    json={
        'queries': queries[:3]
    }
)
res
# Output:
#   <Response [200]>

"""
Now we can loop through the responses and see the results returned for each query:
"""

for query_result in res.json()['results']:
    query = query_result['query']
    answers = []
    scores = []
    for result in query_result['results']:
        answers.append(result['text'])
        scores.append(round(result['score'], 2))
    print("-"*70+"\n"+query+"\n\n"+"\n".join([f"{s}: {a}" for a, s in zip(answers, scores)])+"\n"+"-"*70+"\n\n")
# Output:
#   ----------------------------------------------------------------------

#   To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?

#   

#   0.83: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend "Venite Ad Me Omnes". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.

#   0.81: Within the white inescutcheon, the five quinas (small blue shields) with their five white bezants representing the five wounds of Christ (Portuguese: Cinco Chagas) when crucified and are popularly associated with the "Miracle of Ourique". The story associated with this miracle tells that before the Battle of Ourique (25 July 1139), an old hermit appeared before Count Afonso Henriques (future Afonso I) as a divine messenger. He foretold Afonso's victory and assured him that God was watching over him and his peers. The messenger advised him to walk away from his camp, alone, if he heard a nearby chapel bell tolling, in the following night. In doing so, he witnessed an apparition of Jesus on the cross. Ecstatic, Afonso heard Jesus promising victories for the coming battles, as well as God's wish to act through Afonso, and his descendants, in order to create an empire which would carry His name to unknown lands, thus choosing the Portuguese to perform great tasks.

#   0.79: In 1842, the Bishop of Vincennes, Célestine Guynemer de la Hailandière, offered land to Father Edward Sorin of the Congregation of the Holy Cross, on the condition that he build a college in two years. Fr. Sorin arrived on the site with eight Holy Cross brothers from France and Ireland on November 26, 1842, and began the school using Father Stephen Badin's old log chapel. He soon erected additional buildings, including Old College, the first church, and the first main building. They immediately acquired two students and set about building additions to the campus.

#   0.79: Because of its Catholic identity, a number of religious buildings stand on campus. The Old College building has become one of two seminaries on campus run by the Congregation of Holy Cross. The current Basilica of the Sacred Heart is located on the spot of Fr. Sorin's original church, which became too small for the growing college. It is built in French Revival style and it is decorated by stained glass windows imported directly from France. The interior was painted by Luigi Gregori, an Italian painter invited by Fr. Sorin to be artist in residence. The Basilica also features a bell tower with a carillon. Inside the church there are also sculptures by Ivan Mestrovic. The Grotto of Our Lady of Lourdes, which was built in 1896, is a replica of the original in Lourdes, France. It is very popular among students and alumni as a place of prayer and meditation, and it is considered one of the most beloved spots on campus.

#   0.78: The funeral, held at the Church of the Madeleine in Paris, was delayed almost two weeks, until 30 October. Entrance was restricted to ticket holders as many people were expected to attend. Over 3,000 people arrived without invitations, from as far as London, Berlin and Vienna, and were excluded.

#   ----------------------------------------------------------------------

#   

#   

#   ----------------------------------------------------------------------

#   When did the Scholastic Magazine of Notre dame begin publishing?

#   

#   0.88: As at most other universities, Notre Dame's students run a number of news media outlets. The nine student-run outlets include three newspapers, both a radio and television station, and several magazines and journals. Begun as a one-page journal in September 1876, the Scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the United States. The other magazine, The Juggler, is released twice a year and focuses on student literature and artwork. The Dome yearbook is published annually. The newspapers have varying publication interests, with The Observer published daily and mainly reporting university and other news, and staffed by students from both Notre Dame and Saint Mary's College. Unlike Scholastic and The Dome, The Observer is an independent publication and does not have a faculty advisor or any editorial oversight from the University. In 1987, when some students believed that The Observer began to show a conservative bias, a liberal newspaper, Common Sense was published. Likewise, in 2003, when other students believed that the paper showed a liberal bias, the conservative paper Irish Rover went into production. Neither paper is published as often as The Observer; however, all three are distributed to all students.

#   0.83: In 1919 Father James Burns became president of Notre Dame, and in three years he produced an academic revolution that brought the school up to national standards by adopting the elective system and moving away from the university's traditional scholastic and classical emphasis. By contrast, the Jesuit colleges, bastions of academic conservatism, were reluctant to move to a system of electives. Their graduates were shut out of Harvard Law School for that reason. Notre Dame continued to grow over the years, adding more colleges, programs, and sports teams. By 1921, with the addition of the College of Commerce, Notre Dame had grown from a small college to a university with five colleges and a professional law school. The university continued to expand and add new residence halls and buildings with each subsequent president.

#   0.83: The rise of Hitler and other dictators in the 1930s forced numerous Catholic intellectuals to flee Europe; president John O'Hara brought many to Notre Dame. From Germany came Anton-Hermann Chroust (1907–1982) in classics and law, and Waldemar Gurian a German Catholic intellectual of Jewish descent. Positivism dominated American intellectual life in the 1920s onward but in marked contrast, Gurian received a German Catholic education and wrote his doctoral dissertation under Max Scheler. Ivan Meštrović (1883–1962), a renowned sculptor, brought Croatian culture to campus, 1955–62. Yves Simon (1903–61), brought to ND in the 1940s the insights of French studies in the Aristotelian-Thomistic tradition of philosophy; his own teacher Jacques Maritain (1882–73) was a frequent visitor to campus.

#   0.82: In the 18 years under the presidency of Edward Malloy, C.S.C., (1987–2005), there was a rapid growth in the school's reputation, faculty, and resources. He increased the faculty by more than 500 professors; the academic quality of the student body has improved dramatically, with the average SAT score rising from 1240 to 1360; the number of minority students more than doubled; the endowment grew from $350 million to more than $3 billion; the annual operating budget rose from $177 million to more than $650 million; and annual research funding improved from $15 million to more than $70 million. Notre Dame's most recent[when?] capital campaign raised $1.1 billion, far exceeding its goal of $767 million, and is the largest in the history of Catholic higher education.

#   0.82: The Rev. John J. Cavanaugh, C.S.C. served as president from 1946 to 1952. Cavanaugh's legacy at Notre Dame in the post-war years was devoted to raising academic standards and reshaping the university administration to suit it to an enlarged educational mission and an expanded student body and stressing advanced studies and research at a time when Notre Dame quadrupled in student census, undergraduate enrollment increased by more than half, and graduate student enrollment grew fivefold. Cavanaugh also established the Lobund Institute for Animal Studies and Notre Dame's Medieval Institute. Cavanaugh also presided over the construction of the Nieuwland Science Hall, Fisher Hall, and the Morris Inn, as well as the Hall of Liberal Arts (now O'Shaughnessy Hall), made possible by a donation from I.A. O'Shaughnessy, at the time the largest ever made to an American Catholic university.

#   ----------------------------------------------------------------------

#   

#   

#   ----------------------------------------------------------------------

#   Where is the headquarters of the Congregation of the Holy Cross?

#   

#   0.88: The university is the major seat of the Congregation of Holy Cross (albeit not its official headquarters, which are in Rome). Its main seminary, Moreau Seminary, is located on the campus across St. Joseph lake from the Main Building. Old College, the oldest building on campus and located near the shore of St. Mary lake, houses undergraduate seminarians. Retired priests and brothers reside in Fatima House (a former retreat center), Holy Cross House, as well as Columba Hall near the Grotto. The university through the Moreau Seminary has ties to theologian Frederick Buechner. While not Catholic, Buechner has praised writers from Notre Dame and Moreau Seminary created a Buechner Prize for Preaching.

#   0.84: In 1842, the Bishop of Vincennes, Célestine Guynemer de la Hailandière, offered land to Father Edward Sorin of the Congregation of the Holy Cross, on the condition that he build a college in two years. Fr. Sorin arrived on the site with eight Holy Cross brothers from France and Ireland on November 26, 1842, and began the school using Father Stephen Badin's old log chapel. He soon erected additional buildings, including Old College, the first church, and the first main building. They immediately acquired two students and set about building additions to the campus.

#   0.84: Because of its Catholic identity, a number of religious buildings stand on campus. The Old College building has become one of two seminaries on campus run by the Congregation of Holy Cross. The current Basilica of the Sacred Heart is located on the spot of Fr. Sorin's original church, which became too small for the growing college. It is built in French Revival style and it is decorated by stained glass windows imported directly from France. The interior was painted by Luigi Gregori, an Italian painter invited by Fr. Sorin to be artist in residence. The Basilica also features a bell tower with a carillon. Inside the church there are also sculptures by Ivan Mestrovic. The Grotto of Our Lady of Lourdes, which was built in 1896, is a replica of the original in Lourdes, France. It is very popular among students and alumni as a place of prayer and meditation, and it is considered one of the most beloved spots on campus.

#   0.84: The university is affiliated with the Congregation of Holy Cross (Latin: Congregatio a Sancta Cruce, abbreviated postnominals: "CSC"). While religious affiliation is not a criterion for admission, more than 93% of students identify as Christian, with over 80% of the total being Catholic. Collectively, Catholic Mass is celebrated over 100 times per week on campus, and a large campus ministry program provides for the faith needs of the community. There are multitudes of religious statues and artwork around campus, most prominent of which are the statue of Mary on the Main Building, the Notre Dame Grotto, and the Word of Life mural on Hesburgh Library depicting Christ as a teacher. Additionally, every classroom displays a crucifix. There are many religious clubs (catholic and non-Catholic) at the school, including Council #1477 of the Knights of Columbus (KOC), Baptist Collegiate Ministry (BCM), Jewish Club, Muslim Student Association, Orthodox Christian Fellowship, The Mormon Club, and many more. The Notre Dame KofC are known for being the first collegiate council of KofC, operating a charitable concession stand during every home football game and owning their own building on campus which can be used as a cigar lounge.

#   0.83: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend "Venite Ad Me Omnes". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.

#   ----------------------------------------------------------------------

#   

#   


"""
The top results are all relevant as we would have hoped. With that we've finished. The retrieval app API can be shut down, and to save resources the Pinecone index can be deleted within the [Pinecone console](https://app.pinecone.io/).
"""



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/examples/providers/redis/semantic-search-and-filter.ipynb
================================================
# Jupyter notebook converted to Python script.

import os
import requests

"""
# Document retrieval: upsert and query basic usage

In this walkthrough we will see how to use the retrieval API with a Redis datastore for *semantic search / question-answering*. We will also provide a basic demo showing how to use the "filter" function.

Before running this notebook you should have already initialized the retrieval API and have it running locally or elsewhere. The full instructions for doing this are found in on the chatgpt-retrieval-plugin page [page](https://github.com/openai/chatgpt-retrieval-plugin#quickstart). Please follow the instructions to start the app with the redis datastore.

Additional examples using the search features can be found [here](https://github.com/openai/chatgpt-retrieval-plugin/blob/main/examples/providers/pinecone/semantic-search.ipynb).
"""

"""
## Document

First we will prepare a collection of documents. From the perspective of the retrieval plugin, a [document](https://github.com/openai/chatgpt-retrieval-plugin/blob/main/models/models.py) this consists
of an "id", "text" and a collection of "metadata".

The "metadata" has "source", "source_id", "created_at", "url" and "author" fields. Query metadata does not expose the "url" field.

The "source" field is an Enum and can only be one of ("file", "email" or "chat").

Text is taken from company SEC 10-K filings which are in the public domain.

For demonstration, we will insert some **fake** authors for the documents, see the respective links for the original sources. 
"""

document_1 = {
    "id": "twtr",
    "text": """Postponements, suspensions or cancellations of major events, such as sporting events
                and music festivals, may lead to people perceiving the content on Twitter as less
                relevant or useful or of lower quality, which could negatively affect mDAU growth,
                or may reduce monetization opportunities in connection with such events.""",
    "metadata" : {
        "source" : "file",
        "source_id" : "test:twtr10k",
        "created_at": "2020-12-31",
        "url": "https://www.sec.gov/Archives/edgar/data/1418091/000141809121000031/twtr-20201231.htm",
        "author": 'Elvis Tusk Sr.'        
    }
}

document_2 = {
    "id": "tsla",
    "text": """Because we do not have independent dealer networks, we are responsible for delivering
               all of our vehicles to our customers.""",
    "metadata" : {
        "source" : "file",
        "source_id" : "test:tesla10k",
        "created_at": "2021-12-31",
        "url": "https://www.sec.gov/Archives/edgar/data/1318605/000095017022000796/tsla-20211231.htm",
        "author": 'Elvis Tusk Jr.'        
    }     
}

document_3 = {
    "id": "xom",
    "text": """All practical and economically-viable energy sources will need to be pursued to continue
               meeting global energy demand, recognizing the scale and variety of worldwide energy needs
               as well as the importance of expanding access to modern energy to promote better standards
               of living for billions of people.""",
    "metadata" : {
        "source" : "file",
        "source_id" : "test:xom10k",
        "created_at": "2020-12-31",
        "url": "https://www.sec.gov/Archives/edgar/data/34088/000003408821000012/xom-20201231.htm",
        "author": 'Vape Jordan'        
    }     
}


"""
### Indexing the Docs
"""

"""
We're now ready to begin indexing (or *upserting*) our `documents`. To make these requests to the retrieval app API, we will need to provide authorization in the form of the `BEARER_TOKEN` we set earlier. We do this below:
"""

BEARER_TOKEN = os.environ.get("BEARER_TOKEN") or "BEARER_TOKEN_HERE"
endpoint_url = 'http://0.0.0.0:8000'
headers = {
    "Authorization": f"Bearer {BEARER_TOKEN}"
}

"""
Use the `BEARER_TOKEN` to create our authorization `headers`:
"""

response = requests.post(
    f"{endpoint_url}/upsert",
    headers=headers,
    json={
        "documents": [document_1, document_2, document_3]
    }
)
response.raise_for_status()

"""
### Example filter syntax
In our example data we have tagged each companies 10k documents as a source: test:twtr10k, test:tsla10k, and test:xom10k.
And we have created **fake** authors of the documents, Elvis Tusk Jr., Elvis Tusk Sr. and Vape Jordan. We will then filter based on these fields.
"""

"""
### TAG Fields

source and source_id are "TAG" fields, Redis supports a limited [query syntax](https://redis.io/docs/stack/search/reference/tags/) on TAGS, which includes and "or" syntax, i.e. "test:twtr10k|test:tesla10k" or a ```*``` wildcard to match a prefix.

In this example we have only two documents that match the filter so only two documents will show.

Gotcha: There cannot be a space between the bar "|", i.e. "test:twtr10k|test:tesla10k" is valid, "test:twtr10k | test:tesla10k" is not.
"""

query = {
    "query": "How does Tesla deliver cars?",
    "filter": {"source_id": "test:twtr10k|test:tesla10k"},
    "top_k": 3
}

response = requests.post(
    f"{endpoint_url}/query",
    headers=headers,
    json={
        "queries": [query]
    }
)
response.raise_for_status()

response.json()
# Output:
#   {'results': [{'query': 'How does Tesla deliver cars?',

#      'results': [{'id': 'tsla',

#        'text': 'Because we do not have independent dealer networks, we are responsible for delivering                all of our vehicles to our customers.',

#        'metadata': {'source': 'file',

#         'source_id': 'test:tesla10k',

#         'url': 'https://www.sec.gov/Archives/edgar/data/1318605/000095017022000796/tsla-20211231.htm',

#         'created_at': '1640908800',

#         'author': 'Elvis Tusk Jr.',

#         'document_id': 'tsla'},

#        'embedding': None,

#        'score': 0.185401830213},

#       {'id': 'twtr',

#        'text': 'Postponements, suspensions or cancellations of major events, such as sporting events                 and music festivals, may lead to people perceiving the content on Twitter as less                 relevant or useful or of lower quality, which could negatively affect mDAU growth,                 or may reduce monetization opportunities in connection with such events.',

#        'metadata': {'source': 'file',

#         'source_id': 'test:twtr10k',

#         'url': 'https://www.sec.gov/Archives/edgar/data/1418091/000141809121000031/twtr-20201231.htm',

#         'created_at': '1609372800',

#         'author': 'Elvis Tusk Sr.',

#         'document_id': 'twtr'},

#        'embedding': None,

#        'score': 0.300053447242}]}]}

"""
In this example we use a wild card to filter by prefix. There are three documents matching this filter so three results will be printed.

Gotcha, only prefix filtering is supported for redis TAGS, i.e. "test*" is valid, where as "te\*t\*" is not.
"""

query = {
    "query": "I want information related to car dealerships.",
    "filter": {"source_id": "test:*"},
    "top_k": 3
}

response = requests.post(
    f"{endpoint_url}/query",
    headers=headers,
    json={
        "queries": [query]
    }
)
response.raise_for_status()

response.json()
# Output:
#   {'results': [{'query': 'I want information related to car dealerships.',

#      'results': [{'id': 'tsla',

#        'text': 'Because we do not have independent dealer networks, we are responsible for delivering                all of our vehicles to our customers.',

#        'metadata': {'source': 'file',

#         'source_id': 'test:tesla10k',

#         'url': 'https://www.sec.gov/Archives/edgar/data/1318605/000095017022000796/tsla-20211231.htm',

#         'created_at': '1640908800',

#         'author': 'Elvis Tusk Jr.',

#         'document_id': 'tsla'},

#        'embedding': None,

#        'score': 0.204279193893},

#       {'id': 'twtr',

#        'text': 'Postponements, suspensions or cancellations of major events, such as sporting events                 and music festivals, may lead to people perceiving the content on Twitter as less                 relevant or useful or of lower quality, which could negatively affect mDAU growth,                 or may reduce monetization opportunities in connection with such events.',

#        'metadata': {'source': 'file',

#         'source_id': 'test:twtr10k',

#         'url': 'https://www.sec.gov/Archives/edgar/data/1418091/000141809121000031/twtr-20201231.htm',

#         'created_at': '1609372800',

#         'author': 'Elvis Tusk Sr.',

#         'document_id': 'twtr'},

#        'embedding': None,

#        'score': 0.292188997496},

#       {'id': 'xom',

#        'text': 'All practical and economically-viable energy sources will need to be pursued to continue                meeting global energy demand, recognizing the scale and variety of worldwide energy needs                as well as the importance of expanding access to modern energy to promote better standards                of living for billions of people.',

#        'metadata': {'source': 'file',

#         'source_id': 'test:xom10k',

#         'url': 'https://www.sec.gov/Archives/edgar/data/34088/000003408821000012/xom-20201231.htm',

#         'created_at': '1609372800',

#         'author': 'Vape Jordan',

#         'document_id': 'xom'},

#        'embedding': None,

#        'score': 0.305264299269}]}]}

"""
The last example we filter by the "author" field. The author field is a TextField, and so we have more options for filtering, 
see [here](https://redis.io/docs/stack/search/reference/query_syntax/) for a complete set of examples.

We can select by a specific author, here we only expect to return a single result.
"""

query = {
    "query": "I want information related to car dealerships.",
    "filter": {"source_id": "test:*", "author": "Vape Jordan"},
    "top_k": 3
}

response = requests.post(
    f"{endpoint_url}/query",
    headers=headers,
    json={
        "queries": [query]
    }
)
response.raise_for_status()

response.json()
# Output:
#   {'results': [{'query': 'I want information related to car dealerships.',

#      'results': [{'id': 'xom',

#        'text': 'All practical and economically-viable energy sources will need to be pursued to continue                meeting global energy demand, recognizing the scale and variety of worldwide energy needs                as well as the importance of expanding access to modern energy to promote better standards                of living for billions of people.',

#        'metadata': {'source': 'file',

#         'source_id': 'test:xom10k',

#         'url': 'https://www.sec.gov/Archives/edgar/data/34088/000003408821000012/xom-20201231.htm',

#         'created_at': '1609372800',

#         'author': 'Vape Jordan',

#         'document_id': 'xom'},

#        'embedding': None,

#        'score': 0.305264299269}]}]}

"""
Here we use the negation "-" to select all documents, except those published by an author called Elvis
"""

query = {
    "query": "I want information related to car dealerships.",
    "filter": {"source_id": "test:*", "author": "-Elvis"},
    "top_k": 3
}

response = requests.post(
    f"{endpoint_url}/query",
    headers=headers,
    json={
        "queries": [query]
    }
)
response.raise_for_status()

response.json()
# Output:
#   {'results': [{'query': 'I want information related to car dealerships.',

#      'results': [{'id': 'xom',

#        'text': 'All practical and economically-viable energy sources will need to be pursued to continue                meeting global energy demand, recognizing the scale and variety of worldwide energy needs                as well as the importance of expanding access to modern energy to promote better standards                of living for billions of people.',

#        'metadata': {'source': 'file',

#         'source_id': 'test:xom10k',

#         'url': 'https://www.sec.gov/Archives/edgar/data/34088/000003408821000012/xom-20201231.htm',

#         'created_at': '1609372800',

#         'author': 'Vape Jordan',

#         'document_id': 'xom'},

#        'embedding': None,

#        'score': 0.305264299269}]}]}

"""
Last example we filter two of the authors:
"""

query = {
    "query": "I want information related to car dealerships.",
    "filter": {"source_id": "test:*", "author": "Elvis*Jr.|Vape"},
    "top_k": 3
}

response = requests.post(
    f"{endpoint_url}/query",
    headers=headers,
    json={
        "queries": [query]
    }
)
response.raise_for_status()

response.json()
# Output:
#   {'results': [{'query': 'I want information related to car dealerships.',

#      'results': [{'id': 'tsla',

#        'text': 'Because we do not have independent dealer networks, we are responsible for delivering                all of our vehicles to our customers.',

#        'metadata': {'source': 'file',

#         'source_id': 'test:tesla10k',

#         'url': 'https://www.sec.gov/Archives/edgar/data/1318605/000095017022000796/tsla-20211231.htm',

#         'created_at': '1640908800',

#         'author': 'Elvis Tusk Jr.',

#         'document_id': 'tsla'},

#        'embedding': None,

#        'score': 0.204279193893},

#       {'id': 'xom',

#        'text': 'All practical and economically-viable energy sources will need to be pursued to continue                meeting global energy demand, recognizing the scale and variety of worldwide energy needs                as well as the importance of expanding access to modern energy to promote better standards                of living for billions of people.',

#        'metadata': {'source': 'file',

#         'source_id': 'test:xom10k',

#         'url': 'https://www.sec.gov/Archives/edgar/data/34088/000003408821000012/xom-20201231.htm',

#         'created_at': '1609372800',

#         'author': 'Vape Jordan',

#         'document_id': 'xom'},

#        'embedding': None,

#        'score': 0.305264299269}]}]}



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/examples/providers/supabase/config.toml
================================================
# A string used to distinguish different Supabase projects on the same host. Defaults to the working
# directory name when running `supabase init`.
project_id = "providers"

[api]
# Port to use for the API URL.
port = 54321
# Schemas to expose in your API. Tables, views and stored procedures in this schema will get API
# endpoints. public and storage are always included.
schemas = ["public", "storage", "graphql_public"]
# Extra schemas to add to the search_path of every request. public is always included.
extra_search_path = ["public", "extensions"]
# The maximum number of rows returns from a view, table, or stored procedure. Limits payload size
# for accidental or malicious requests.
max_rows = 1000

[db]
# Port to use for the local database URL.
port = 54322
# The database major version to use. This has to be the same as your remote database's. Run `SHOW
# server_version;` on the remote database to check.
major_version = 15

[studio]
# Port to use for Supabase Studio.
port = 54323

# Email testing server. Emails sent with the local dev setup are not actually sent - rather, they
# are monitored, and you can view the emails that would have been sent from the web interface.
[inbucket]
# Port to use for the email testing server web interface.
port = 54324
smtp_port = 54325
pop3_port = 54326

[storage]
# The maximum file size allowed (e.g. "5MB", "500KB").
file_size_limit = "50MiB"

[auth]
# The base URL of your website. Used as an allow-list for redirects and for constructing URLs used
# in emails.
site_url = "http://localhost:3000"
# A list of *exact* URLs that auth providers are permitted to redirect to post authentication.
additional_redirect_urls = ["https://localhost:3000"]
# How long tokens are valid for, in seconds. Defaults to 3600 (1 hour), maximum 604,800 seconds (one
# week).
jwt_expiry = 3600
# Allow/disallow new user signups to your project.
enable_signup = true

[auth.email]
# Allow/disallow new user signups via email to your project.
enable_signup = true
# If enabled, a user will be required to confirm any email change on both the old, and new email
# addresses. If disabled, only the new email is required to confirm.
double_confirm_changes = true
# If enabled, users need to confirm their email address before signing in.
enable_confirmations = false

# Use an external OAuth provider. The full list of providers are: `apple`, `azure`, `bitbucket`,
# `discord`, `facebook`, `github`, `gitlab`, `google`, `keycloak`, `linkedin`, `notion`, `twitch`,
# `twitter`, `slack`, `spotify`, `workos`, `zoom`.
[auth.external.apple]
enabled = false
client_id = ""
secret = ""
# Overrides the default auth redirectUrl.
redirect_uri = ""
# Overrides the default auth provider URL. Used to support self-hosted gitlab, single-tenant Azure,
# or any other third-party OIDC providers.
url = ""



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/examples/providers/supabase/seed.sql
================================================



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/examples/providers/supabase/.gitignore
================================================
# Supabase
.branches
.temp



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/examples/providers/supabase/migrations/20230414142107_init_pg_vector.sql
================================================
create extension vector;

create table if not exists documents (
    id text primary key default gen_random_uuid()::text,
    source text,
    source_id text,
    content text,
    document_id text,
    author text,
    url text,
    created_at timestamptz default now(),
    embedding vector(1536)
);

create index ix_documents_document_id on documents using btree ( document_id );
create index ix_documents_source on documents using btree ( source );
create index ix_documents_source_id on documents using btree ( source_id );
create index ix_documents_author on documents using btree ( author );
create index ix_documents_created_at on documents using brin ( created_at );

alter table documents enable row level security;

create or replace function match_page_sections(in_embedding vector(1536)
                                            , in_match_count int default 3
                                            , in_document_id text default '%%'
                                            , in_source_id text default '%%'
                                            , in_source text default '%%'
                                            , in_author text default '%%'
                                            , in_start_date timestamptz default '-infinity'
                                            , in_end_date timestamptz default 'infinity')
returns table (id text
            , source text
            , source_id text
            , document_id text
            , url text
            , created_at timestamptz
            , author text
            , content text
            , embedding vector(1536)
            , similarity float)
language plpgsql
as $$
#variable_conflict use_variable
begin
return query
select
    documents.id,
    documents.source,
    documents.source_id,
    documents.document_id,
    documents.url,
    documents.created_at,
    documents.author,
    documents.content,
    documents.embedding,
    (documents.embedding <#> in_embedding) * -1 as similarity
from documents

where in_start_date <= documents.created_at and 
    documents.created_at <= in_end_date and
    (documents.source_id like in_source_id or documents.source_id is null) and
    (documents.source like in_source or documents.source is null) and
    (documents.author like in_author or documents.author is null) and
    (documents.document_id like in_document_id or documents.document_id is null)

order by documents.embedding <#> in_embedding

limit in_match_count;
end;
$$;


================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/local_server/ai-plugin.json
================================================
{
  "schema_version": "v1",
  "name_for_model": "retrieval",
  "name_for_human": "Retrieval Plugin",
  "description_for_model": "Plugin for searching through the user's documents (such as files, emails, and more) to find answers to questions and retrieve relevant information. Use it whenever a user asks something that might be found in their personal information.",
  "description_for_human": "Search through your documents.",
  "auth": {
    "type": "none"
  },
  "api": {
    "type": "openapi",
    "url": "http://localhost:3333/.well-known/openapi.yaml"
  },
  "logo_url": "http://localhost:3333/.well-known/logo.png",
  "contact_email": "hello@contact.com", 
  "legal_info_url": "hello@legal.com"
}




================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/local_server/main.py
================================================
# This is a version of the main.py file found in ../../../server/main.py for testing the plugin locally.
# Use the command `poetry run dev` to run this.
from typing import Optional
import uvicorn
from fastapi import FastAPI, File, Form, HTTPException, Body, UploadFile
from loguru import logger

from models.api import (
    DeleteRequest,
    DeleteResponse,
    QueryRequest,
    QueryResponse,
    UpsertRequest,
    UpsertResponse,
)
from datastore.factory import get_datastore
from services.file import get_document_from_file

from starlette.responses import FileResponse

from models.models import DocumentMetadata, Source
from fastapi.middleware.cors import CORSMiddleware


app = FastAPI()

PORT = 3333

origins = [
    f"http://localhost:{PORT}",
    "https://chat.openai.com",
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.route("/.well-known/ai-plugin.json")
async def get_manifest(request):
    file_path = "./local_server/ai-plugin.json"
    simple_headers = {}
    simple_headers["Access-Control-Allow-Private-Network"] = "true"
    return FileResponse(file_path, media_type="text/json", headers=simple_headers)


@app.route("/.well-known/logo.png")
async def get_logo(request):
    file_path = "./local_server/logo.png"
    return FileResponse(file_path, media_type="text/json")


@app.route("/.well-known/openapi.yaml")
async def get_openapi(request):
    file_path = "./local_server/openapi.yaml"
    return FileResponse(file_path, media_type="text/json")


@app.post(
    "/upsert-file",
    response_model=UpsertResponse,
)
async def upsert_file(
    file: UploadFile = File(...),
    metadata: Optional[str] = Form(None),
):
    try:
        metadata_obj = (
            DocumentMetadata.parse_raw(metadata)
            if metadata
            else DocumentMetadata(source=Source.file)
        )
    except:
        metadata_obj = DocumentMetadata(source=Source.file)

    document = await get_document_from_file(file, metadata_obj)

    try:
        ids = await datastore.upsert([document])
        return UpsertResponse(ids=ids)
    except Exception as e:
        logger.error(e)
        raise HTTPException(status_code=500, detail=f"str({e})")


@app.post(
    "/upsert",
    response_model=UpsertResponse,
)
async def upsert(
    request: UpsertRequest = Body(...),
):
    try:
        ids = await datastore.upsert(request.documents)
        return UpsertResponse(ids=ids)
    except Exception as e:
        logger.error(e)
        raise HTTPException(status_code=500, detail="Internal Service Error")


@app.post("/query", response_model=QueryResponse)
async def query_main(request: QueryRequest = Body(...)):
    try:
        results = await datastore.query(
            request.queries,
        )
        return QueryResponse(results=results)
    except Exception as e:
        logger.error(e)
        raise HTTPException(status_code=500, detail="Internal Service Error")


@app.delete(
    "/delete",
    response_model=DeleteResponse,
)
async def delete(
    request: DeleteRequest = Body(...),
):
    if not (request.ids or request.filter or request.delete_all):
        raise HTTPException(
            status_code=400,
            detail="One of ids, filter, or delete_all is required",
        )
    try:
        success = await datastore.delete(
            ids=request.ids,
            filter=request.filter,
            delete_all=request.delete_all,
        )
        return DeleteResponse(success=success)
    except Exception as e:
        logger.error(e)
        raise HTTPException(status_code=500, detail="Internal Service Error")


@app.on_event("startup")
async def startup():
    global datastore
    datastore = await get_datastore()


def start():
    uvicorn.run("local_server.main:app", host="localhost", port=PORT, reload=True)



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/local_server/openapi.yaml
================================================
openapi: 3.0.2
info:
  title: Retrieval Plugin API
  description: A retrieval API for querying and filtering documents based on natural language queries and metadata
  version: 1.0.0
  servers:
    - url: http://localhost:3333
paths:
  /query:
    post:
      summary: Query
      description: Accepts search query objects array each with query and optional filter. Break down complex questions into sub-questions. Refine results by criteria, e.g. time / source, don't do this often. Split queries if ResponseTooLargeError occurs.
      operationId: query_query_post
      requestBody:
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/QueryRequest"
        required: true
      responses:
        "200":
          description: Successful Response
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/QueryResponse"
        "422":
          description: Validation Error
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/HTTPValidationError"
components:
  schemas:
    DocumentChunkMetadata:
      title: DocumentChunkMetadata
      type: object
      properties:
        source:
          $ref: "#/components/schemas/Source"
        source_id:
          title: Source Id
          type: string
        url:
          title: Url
          type: string
        created_at:
          title: Created At
          type: string
        author:
          title: Author
          type: string
        document_id:
          title: Document Id
          type: string
    DocumentChunkWithScore:
      title: DocumentChunkWithScore
      required:
        - text
        - metadata
        - score
      type: object
      properties:
        id:
          title: Id
          type: string
        text:
          title: Text
          type: string
        metadata:
          $ref: "#/components/schemas/DocumentChunkMetadata"
        embedding:
          title: Embedding
          type: array
          items:
            type: number
        score:
          title: Score
          type: number
    DocumentMetadataFilter:
      title: DocumentMetadataFilter
      type: object
      properties:
        document_id:
          title: Document Id
          type: string
        source:
          $ref: "#/components/schemas/Source"
        source_id:
          title: Source Id
          type: string
        author:
          title: Author
          type: string
        start_date:
          title: Start Date
          type: string
        end_date:
          title: End Date
          type: string
    HTTPValidationError:
      title: HTTPValidationError
      type: object
      properties:
        detail:
          title: Detail
          type: array
          items:
            $ref: "#/components/schemas/ValidationError"
    Query:
      title: Query
      required:
        - query
      type: object
      properties:
        query:
          title: Query
          type: string
        filter:
          $ref: "#/components/schemas/DocumentMetadataFilter"
        top_k:
          title: Top K
          type: integer
          default: 3
    QueryRequest:
      title: QueryRequest
      required:
        - queries
      type: object
      properties:
        queries:
          title: Queries
          type: array
          items:
            $ref: "#/components/schemas/Query"
    QueryResponse:
      title: QueryResponse
      required:
        - results
      type: object
      properties:
        results:
          title: Results
          type: array
          items:
            $ref: "#/components/schemas/QueryResult"
    QueryResult:
      title: QueryResult
      required:
        - query
        - results
      type: object
      properties:
        query:
          title: Query
          type: string
        results:
          title: Results
          type: array
          items:
            $ref: "#/components/schemas/DocumentChunkWithScore"
    Source:
      title: Source
      enum:
        - email
        - file
        - chat
      type: string
      description: An enumeration.
    ValidationError:
      title: ValidationError
      required:
        - loc
        - msg
        - type
      type: object
      properties:
        loc:
          title: Location
          type: array
          items:
            anyOf:
              - type: string
              - type: integer
        msg:
          title: Message
          type: string
        type:
          title: Error Type
          type: string



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/models/api.py
================================================
from models.models import (
    Document,
    DocumentMetadataFilter,
    Query,
    QueryResult,
)
from pydantic import BaseModel
from typing import List, Optional


class UpsertRequest(BaseModel):
    documents: List[Document]


class UpsertResponse(BaseModel):
    ids: List[str]


class QueryRequest(BaseModel):
    queries: List[Query]


class QueryResponse(BaseModel):
    results: List[QueryResult]


class DeleteRequest(BaseModel):
    ids: Optional[List[str]] = None
    filter: Optional[DocumentMetadataFilter] = None
    delete_all: Optional[bool] = False


class DeleteResponse(BaseModel):
    success: bool



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/models/models.py
================================================
from pydantic import BaseModel
from typing import List, Optional
from enum import Enum


class Source(str, Enum):
    email = "email"
    file = "file"
    chat = "chat"


class DocumentMetadata(BaseModel):
    source: Optional[Source] = None
    source_id: Optional[str] = None
    url: Optional[str] = None
    created_at: Optional[str] = None
    author: Optional[str] = None


class DocumentChunkMetadata(DocumentMetadata):
    document_id: Optional[str] = None


class DocumentChunk(BaseModel):
    id: Optional[str] = None
    text: str
    metadata: DocumentChunkMetadata
    embedding: Optional[List[float]] = None


class DocumentChunkWithScore(DocumentChunk):
    score: float


class Document(BaseModel):
    id: Optional[str] = None
    text: str
    metadata: Optional[DocumentMetadata] = None


class DocumentWithChunks(Document):
    chunks: List[DocumentChunk]


class DocumentMetadataFilter(BaseModel):
    document_id: Optional[str] = None
    source: Optional[Source] = None
    source_id: Optional[str] = None
    author: Optional[str] = None
    start_date: Optional[str] = None  # any date string format
    end_date: Optional[str] = None  # any date string format


class Query(BaseModel):
    query: str
    filter: Optional[DocumentMetadataFilter] = None
    top_k: Optional[int] = 3


class QueryWithEmbedding(Query):
    embedding: List[float]


class QueryResult(BaseModel):
    query: str
    results: List[DocumentChunkWithScore]



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/scripts/process_json/README.md
================================================
## Process a JSON File

This script is a utility to process a file dump of documents in a JSON format and store them in the vector database with some metadata. It can also optionally screen the documents for personally identifiable information (PII) using a language model, and skip them if detected. Additionally, the script can extract metadata from the document using a language model. You can customize the PII detection function in [`services/pii_detection`](../../services/pii_detection.py) and the metadata extraction function in [`services/extract_metadata`](../../services/extract_metadata.py) for your use case.

## Usage

To run this script from the terminal, navigate to this folder and use the following command:

```
python process_json.py --filepath path/to/file_dump.json --custom_metadata '{"source": "file"}' --screen_for_pii True --extract_metadata True
```

where:

- `path/to/file_dump.json` is the name or path to the file dump to be processed. The format of this JSON file should be a list of JSON objects, where each object represents a document. The JSON object should have a subset of the following fields: `id`, `text`, `source`, `source_id`, `url`, `created_at`, and `author`. The `text` field is required, while the rest are optional and will be used to populate the metadata of the document. If the `id` field is not specified, a random UUID will be generated for the document.
- `--custom_metadata` is an optional JSON string of key-value pairs to update the metadata of the documents. For example, `{"source": "file"}` will add a `source` field with the value `file` to the metadata of each document. The default value is an empty JSON object (`{}`).
- `--screen_for_pii` is an optional boolean flag to indicate whether to use the PII detection function or not. If set to `True`, the script will use the `screen_text_for_pii` function from the [`services/pii_detection`](../../services/pii_detection.py) module to check if the document text contains any PII using a language model. If PII is detected, the script will print a warning and skip the document. The default value is `False`.
- `--extract_metadata` is an optional boolean flag to indicate whether to try to extract metadata from the document using a language model. If set to `True`, the script will use the `extract_metadata_from_document` function from the [`services/extract_metadata`](../../services/extract_metadata.py) module to extract metadata from the document text and update the metadata object accordingly. The default value is`False`.

The script will load the JSON file as a list of dictionaries, iterate over the data, create document objects, and batch upsert them into the database. It will also print some progress messages and error messages if any, as well as the number and content of the skipped items due to errors or PII detection.

You can use `python process_json.py -h` to get a summary of the options and their descriptions.

Test the script with the example file, [example.json](example.json).



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/scripts/process_json/example.json
================================================
[
    {
      "id": "123",
      "text": "This is a document about something",
      "source": "file",
      "source_id": "https://example.com/doc1",
      "url": "https://example.com/doc1",
      "created_at": "2021-01-01T12:00:00Z",
      "author": "Alice"
    },
    {
      "text": "This is another document about something else",
      "source": "file",
      "source_id": "doc2.txt",
      "author": "Bob"
    },
    {
      "id": "456",
      "text": "This is Alice's phone number: 123-456-7890",
      "source": "email",
      "source_id": "567",
      "created_at": "2021-01-02T13:00:00Z",
      "author": "Alice"
    }
]


================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/scripts/process_json/process_json.py
================================================
import uuid
import json
import argparse
import asyncio

from loguru import logger
from models.models import Document, DocumentMetadata
from datastore.datastore import DataStore
from datastore.factory import get_datastore
from services.extract_metadata import extract_metadata_from_document
from services.pii_detection import screen_text_for_pii

DOCUMENT_UPSERT_BATCH_SIZE = 50


async def process_json_dump(
    filepath: str,
    datastore: DataStore,
    custom_metadata: dict,
    screen_for_pii: bool,
    extract_metadata: bool,
):
    # load the json file as a list of dictionaries
    with open(filepath) as json_file:
        data = json.load(json_file)

    documents = []
    skipped_items = []
    # iterate over the data and create document objects
    for item in data:
        if len(documents) % 20 == 0:
            logger.info(f"Processed {len(documents)} documents")

        try:
            # get the id, text, source, source_id, url, created_at and author from the item
            # use default values if not specified
            id = item.get("id", None)
            text = item.get("text", None)
            source = item.get("source", None)
            source_id = item.get("source_id", None)
            url = item.get("url", None)
            created_at = item.get("created_at", None)
            author = item.get("author", None)

            if not text:
                logger.info("No document text, skipping...")
                continue

            # create a metadata object with the source, source_id, url, created_at and author
            metadata = DocumentMetadata(
                source=source,
                source_id=source_id,
                url=url,
                created_at=created_at,
                author=author,
            )
            logger.info("metadata: ", str(metadata))

            # update metadata with custom values
            for key, value in custom_metadata.items():
                if hasattr(metadata, key):
                    setattr(metadata, key, value)

            # screen for pii if requested
            if screen_for_pii:
                pii_detected = screen_text_for_pii(text)
                # if pii detected, print a warning and skip the document
                if pii_detected:
                    logger.info("PII detected in document, skipping")
                    skipped_items.append(item)  # add the skipped item to the list
                    continue

            # extract metadata if requested
            if extract_metadata:
                # extract metadata from the document text
                extracted_metadata = extract_metadata_from_document(
                    f"Text: {text}; Metadata: {str(metadata)}"
                )
                # get a Metadata object from the extracted metadata
                metadata = DocumentMetadata(**extracted_metadata)

            # create a document object with the id or a random id, text and metadata
            document = Document(
                id=id or str(uuid.uuid4()),
                text=text,
                metadata=metadata,
            )
            documents.append(document)
        except Exception as e:
            # log the error and continue with the next item
            logger.error(f"Error processing {item}: {e}")
            skipped_items.append(item)  # add the skipped item to the list

    # do this in batches, the upsert method already batches documents but this allows
    # us to add more descriptive logging
    for i in range(0, len(documents), DOCUMENT_UPSERT_BATCH_SIZE):
        # Get the text of the chunks in the current batch
        batch_documents = documents[i : i + DOCUMENT_UPSERT_BATCH_SIZE]
        logger.info(f"Upserting batch of {len(batch_documents)} documents, batch {i}")
        logger.info("documents: ", documents)
        await datastore.upsert(batch_documents)

    # print the skipped items
    logger.info(f"Skipped {len(skipped_items)} items due to errors or PII detection")
    for item in skipped_items:
        logger.info(item)


async def main():
    # parse the command-line arguments
    parser = argparse.ArgumentParser()
    parser.add_argument("--filepath", required=True, help="The path to the json dump")
    parser.add_argument(
        "--custom_metadata",
        default="{}",
        help="A JSON string of key-value pairs to update the metadata of the documents",
    )
    parser.add_argument(
        "--screen_for_pii",
        default=False,
        type=bool,
        help="A boolean flag to indicate whether to try the PII detection function (using a language model)",
    )
    parser.add_argument(
        "--extract_metadata",
        default=False,
        type=bool,
        help="A boolean flag to indicate whether to try to extract metadata from the document (using a language model)",
    )
    args = parser.parse_args()

    # get the arguments
    filepath = args.filepath
    custom_metadata = json.loads(args.custom_metadata)
    screen_for_pii = args.screen_for_pii
    extract_metadata = args.extract_metadata

    # initialize the db instance once as a global variable
    datastore = await get_datastore()
    # process the json dump
    await process_json_dump(
        filepath, datastore, custom_metadata, screen_for_pii, extract_metadata
    )


if __name__ == "__main__":
    asyncio.run(main())



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/scripts/process_jsonl/README.md
================================================
## Process a JSONL File

This script is a utility to process a file dump of documents in a JSONL format and store them in the vector database with some metadata. It can also optionally screen the documents for personally identifiable information (PII) using a language model, and skip them if detected. Additionally, the script can extract metadata from the document using a language model. You can customize the PII detection function in [`services/pii_detection`](../../services/pii_detection.py) and the metadata extraction function in [`services/extract_metadata`](../../services/extract_metadata.py) for your use case.

## Usage

To run this script from the terminal, navigate to this folder and use the following command:

```
python process_jsonl.py --filepath path/to/file_dump.jsonl --custom_metadata '{"source": "email"}' --screen_for_pii True --extract_metadata True
```

where:

- `path/to/file_dump.jsonl` is the name or path to the file dump to be processed. The format of this JSONL file should be a newline-delimited JSON file, where each line is a valid JSON object representing a document. The JSON object should have a subset of the following fields: `id`, `text`, `source`, `source_id`, `url`, `created_at`, and `author`. The `text` field is required, while the rest are optional and will be used to populate the metadata of the document. If the `id` field is not specified, a random UUID will be generated for the document.
- `--custom_metadata` is an optional JSON string of key-value pairs to update the metadata of the documents. For example, `{"source": "file"}` will add a `source` field with the value `file` to the metadata of each document. The default value is an empty JSON object (`{}`).
- `--screen_for_pii` is an optional boolean flag to indicate whether to use the PII detection function or not. If set to `True`, the script will use the `screen_text_for_pii` function from the [`services/pii_detection`](../../services/pii_detection.py) module to check if the document text contains any PII using a language model. If PII is detected, the script will print a warning and skip the document. The default value is `False`.
- `--extract_metadata` is an optional boolean flag to indicate whether to try to extract metadata from the document using a language model. If set to `True`, the script will use the `extract_metadata_from_document` function from the [`services/extract_metadata`](../../services/extract_metadata.py) module to extract metadata from the document text and update the metadata object accordingly. The default value is`False`.

The script will open the JSONL file as a generator of dictionaries, iterate over the data, create document objects, and batch upsert them into the database. It will also print some progress messages and error messages if any, as well as the number and content of the skipped items due to errors, PII detection, or metadata extraction issues.

You can use `python process_jsonl.py -h` to get a summary of the options and their descriptions.

Test the script with the example file, [example.jsonl](example.jsonl).



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/scripts/process_jsonl/example.jsonl
================================================
{"id": "4", "text": "This document only has an ID and text. The other fields are missing."}
{"text": "This document has no ID, but it has text and a source.", "source": "email"}
{"id": "6", "text": "This document has an ID, text, and author, but no source information.", "author": "John Doe"}
{"text": "This document has text, a source, and a URL, but no ID or author.", "source": "file", "url": "https://example.com/file/2"}
{"id": "8", "text": "This document has an ID, text, source, and created_at timestamp, but no author or URL.", "source": "chat", "created_at": "2022-01-04T00:00:00"}
{"id": "9", "text": "This document contains PII. John Smith's email address is john.smith@example.com and his phone number is +1 (555) 123-4567.", "source": "email", "source_id": "email_2", "url": "https://example.com/email/2", "created_at": "2022-01-05T00:00:00", "author": "John Smith"}


================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/scripts/process_jsonl/process_jsonl.py
================================================
import uuid
import json
import argparse
import asyncio

from loguru import logger
from models.models import Document, DocumentMetadata
from datastore.datastore import DataStore
from datastore.factory import get_datastore
from services.extract_metadata import extract_metadata_from_document
from services.pii_detection import screen_text_for_pii

DOCUMENT_UPSERT_BATCH_SIZE = 50


async def process_jsonl_dump(
    filepath: str,
    datastore: DataStore,
    custom_metadata: dict,
    screen_for_pii: bool,
    extract_metadata: bool,
):
    # open the jsonl file as a generator of dictionaries
    with open(filepath) as jsonl_file:
        data = [json.loads(line) for line in jsonl_file]

    documents = []
    skipped_items = []
    # iterate over the data and create document objects
    for item in data:
        if len(documents) % 20 == 0:
            logger.info(f"Processed {len(documents)} documents")

        try:
            # get the id, text, source, source_id, url, created_at and author from the item
            # use default values if not specified
            id = item.get("id", None)
            text = item.get("text", None)
            source = item.get("source", None)
            source_id = item.get("source_id", None)
            url = item.get("url", None)
            created_at = item.get("created_at", None)
            author = item.get("author", None)

            if not text:
                logger.info("No document text, skipping...")
                continue

            # create a metadata object with the source, source_id, url, created_at and author
            metadata = DocumentMetadata(
                source=source,
                source_id=source_id,
                url=url,
                created_at=created_at,
                author=author,
            )

            # update metadata with custom values
            for key, value in custom_metadata.items():
                if hasattr(metadata, key):
                    setattr(metadata, key, value)

            # screen for pii if requested
            if screen_for_pii:
                pii_detected = screen_text_for_pii(text)
                # if pii detected, print a warning and skip the document
                if pii_detected:
                    logger.info("PII detected in document, skipping")
                    skipped_items.append(item)  # add the skipped item to the list
                    continue

            # extract metadata if requested
            if extract_metadata:
                # extract metadata from the document text
                extracted_metadata = extract_metadata_from_document(
                    f"Text: {text}; Metadata: {str(metadata)}"
                )
                # get a Metadata object from the extracted metadata
                metadata = DocumentMetadata(**extracted_metadata)

            # create a document object with the id, text and metadata
            document = Document(
                id=id,
                text=text,
                metadata=metadata,
            )
            documents.append(document)
        except Exception as e:
            # log the error and continue with the next item
            logger.error(f"Error processing {item}: {e}")
            skipped_items.append(item)  # add the skipped item to the list

    # do this in batches, the upsert method already batches documents but this allows
    # us to add more descriptive logging
    for i in range(0, len(documents), DOCUMENT_UPSERT_BATCH_SIZE):
        # Get the text of the chunks in the current batch
        batch_documents = documents[i : i + DOCUMENT_UPSERT_BATCH_SIZE]
        logger.info(f"Upserting batch of {len(batch_documents)} documents, batch {i}")
        await datastore.upsert(batch_documents)

    # print the skipped items
    logger.info(f"Skipped {len(skipped_items)} items due to errors or PII detection")
    for item in skipped_items:
        logger.info(item)


async def main():
    # parse the command-line arguments
    parser = argparse.ArgumentParser()
    parser.add_argument("--filepath", required=True, help="The path to the jsonl dump")
    parser.add_argument(
        "--custom_metadata",
        default="{}",
        help="A JSON string of key-value pairs to update the metadata of the documents",
    )
    parser.add_argument(
        "--screen_for_pii",
        default=False,
        type=bool,
        help="A boolean flag to indicate whether to try the PII detection function (using a language model)",
    )
    parser.add_argument(
        "--extract_metadata",
        default=False,
        type=bool,
        help="A boolean flag to indicate whether to try to extract metadata from the document (using a language model)",
    )
    args = parser.parse_args()

    # get the arguments
    filepath = args.filepath
    custom_metadata = json.loads(args.custom_metadata)
    screen_for_pii = args.screen_for_pii
    extract_metadata = args.extract_metadata

    # initialize the db instance once as a global variable
    datastore = await get_datastore()
    # process the jsonl dump
    await process_jsonl_dump(
        filepath, datastore, custom_metadata, screen_for_pii, extract_metadata
    )


if __name__ == "__main__":
    asyncio.run(main())



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/scripts/process_zip/README.md
================================================
## Process a ZIP File

This script is a utility to process a file dump of documents in a zip file and store them in the vector database with some metadata. It can also optionally screen the documents for personally identifiable information (PII) using a language model, and skip them if detected. Additionally, the script can extract metadata from the document using a language model. You can customize the PII detection function in [`services/pii_detection`](../../services/pii_detection.py) and the metadata extraction function in [`services/extract_metadata`](../../services/extract_metadata.py) for your use case.

## Usage

To run this script from the terminal, navigate to this folder and use the following command:

```
python process_zip.py --filepath path/to/file_dump.zip --custom_metadata '{"source": "email"}' --screen_for_pii True --extract_metadata True
```

where:

- `path/to/file_dump.zip` is the name or path to the file dump to be processed. The format of this zip file should be a zip file containing of docx, pdf, txt, md and pptx files (any internal folder structure is acceptable).
- `--custom_metadata` is an optional JSON string of key-value pairs to update the metadata of the documents. For example, `{"source": "file"}` will add a `source` field with the value `file` to the metadata of each document. The default value is an empty JSON object (`{}`).
- `--screen_for_pii` is an optional boolean flag to indicate whether to use the PII detection function or not. If set to `True`, the script will use the `screen_text_for_pii` function from the [`services/pii_detection`](../../services/pii_detection.py) module to check if the document text contains any PII using a language model. If PII is detected, the script will print a warning and skip the document. The default value is `False`.
- `--extract_metadata` is an optional boolean flag to indicate whether to try to extract metadata from the document using a language model. If set to `True`, the script will use the `extract_metadata_from_document` function from the [`services/extract_metadata`](../../services/extract_metadata.py) module to extract metadata from the document text and update the metadata object accordingly. The default value is`False`.

The script will extract the files from the zip file into a temporary directory named `dump`, process each file and store the document text and metadata in the database, and then delete the temporary directory and its contents. It will also print some progress messages and error messages if any.

You can use `python process_zip.py -h` to get a summary of the options and their descriptions.

Test the script with the example file, [example.zip](example.zip).



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/scripts/process_zip/example.zip
================================================
[Non-text file]


================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/scripts/process_zip/process_zip.py
================================================
import uuid
import zipfile
import os
import json
import argparse
import asyncio

from loguru import logger
from models.models import Document, DocumentMetadata, Source
from datastore.datastore import DataStore
from datastore.factory import get_datastore
from services.extract_metadata import extract_metadata_from_document
from services.file import extract_text_from_filepath
from services.pii_detection import screen_text_for_pii

DOCUMENT_UPSERT_BATCH_SIZE = 50


async def process_file_dump(
    filepath: str,
    datastore: DataStore,
    custom_metadata: dict,
    screen_for_pii: bool,
    extract_metadata: bool,
):
    # create a ZipFile object and extract all the files into a directory named 'dump'
    with zipfile.ZipFile(filepath) as zip_file:
        zip_file.extractall("dump")

    documents = []
    skipped_files = []
    # use os.walk to traverse the dump directory and its subdirectories
    for root, dirs, files in os.walk("dump"):
        for filename in files:
            if len(documents) % 20 == 0:
                logger.info(f"Processed {len(documents)} documents")

            filepath = os.path.join(root, filename)

            try:
                extracted_text = extract_text_from_filepath(filepath)
                logger.info(f"extracted_text from {filepath}")

                # create a metadata object with the source and source_id fields
                metadata = DocumentMetadata(
                    source=Source.file,
                    source_id=filename,
                )

                # update metadata with custom values
                for key, value in custom_metadata.items():
                    if hasattr(metadata, key):
                        setattr(metadata, key, value)

                # screen for pii if requested
                if screen_for_pii:
                    pii_detected = screen_text_for_pii(extracted_text)
                    # if pii detected, print a warning and skip the document
                    if pii_detected:
                        logger.info("PII detected in document, skipping")
                        skipped_files.append(
                            filepath
                        )  # add the skipped file to the list
                        continue

                # extract metadata if requested
                if extract_metadata:
                    # extract metadata from the document text
                    extracted_metadata = extract_metadata_from_document(
                        f"Text: {extracted_text}; Metadata: {str(metadata)}"
                    )
                    # get a Metadata object from the extracted metadata
                    metadata = DocumentMetadata(**extracted_metadata)

                # create a document object with a random id, text and metadata
                document = Document(
                    id=str(uuid.uuid4()),
                    text=extracted_text,
                    metadata=metadata,
                )
                documents.append(document)
            except Exception as e:
                # log the error and continue with the next file
                logger.error(f"Error processing {filepath}: {e}")
                skipped_files.append(filepath)  # add the skipped file to the list

    # do this in batches, the upsert method already batches documents but this allows
    # us to add more descriptive logging
    for i in range(0, len(documents), DOCUMENT_UPSERT_BATCH_SIZE):
        # Get the text of the chunks in the current batch
        batch_documents = [doc for doc in documents[i : i + DOCUMENT_UPSERT_BATCH_SIZE]]
        logger.info(f"Upserting batch of {len(batch_documents)} documents, batch {i}")
        logger.info("documents: ", documents)
        await datastore.upsert(batch_documents)

    # delete all files in the dump directory
    for root, dirs, files in os.walk("dump", topdown=False):
        for filename in files:
            filepath = os.path.join(root, filename)
            os.remove(filepath)
        for dirname in dirs:
            dirpath = os.path.join(root, dirname)
            os.rmdir(dirpath)

    # delete the dump directory
    os.rmdir("dump")

    # print the skipped files
    logger.info(f"Skipped {len(skipped_files)} files due to errors or PII detection")
    for file in skipped_files:
        logger.info(file)


async def main():
    # parse the command-line arguments
    parser = argparse.ArgumentParser()
    parser.add_argument("--filepath", required=True, help="The path to the file dump")
    parser.add_argument(
        "--custom_metadata",
        default="{}",
        help="A JSON string of key-value pairs to update the metadata of the documents",
    )
    parser.add_argument(
        "--screen_for_pii",
        default=False,
        type=bool,
        help="A boolean flag to indicate whether to try the PII detection function (using a language model)",
    )
    parser.add_argument(
        "--extract_metadata",
        default=False,
        type=bool,
        help="A boolean flag to indicate whether to try to extract metadata from the document (using a language model)",
    )
    args = parser.parse_args()

    # get the arguments
    filepath = args.filepath
    custom_metadata = json.loads(args.custom_metadata)
    screen_for_pii = args.screen_for_pii
    extract_metadata = args.extract_metadata

    # initialize the db instance once as a global variable
    datastore = await get_datastore()
    # process the file dump
    await process_file_dump(
        filepath, datastore, custom_metadata, screen_for_pii, extract_metadata
    )


if __name__ == "__main__":
    asyncio.run(main())



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/server/main.py
================================================
import os
from typing import Optional
import uvicorn
from fastapi import FastAPI, File, Form, HTTPException, Depends, Body, UploadFile
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from fastapi.staticfiles import StaticFiles
from loguru import logger

from models.api import (
    DeleteRequest,
    DeleteResponse,
    QueryRequest,
    QueryResponse,
    UpsertRequest,
    UpsertResponse,
)
from datastore.factory import get_datastore
from services.file import get_document_from_file

from models.models import DocumentMetadata, Source

bearer_scheme = HTTPBearer()
BEARER_TOKEN = os.environ.get("BEARER_TOKEN")
assert BEARER_TOKEN is not None


def validate_token(credentials: HTTPAuthorizationCredentials = Depends(bearer_scheme)):
    if credentials.scheme != "Bearer" or credentials.credentials != BEARER_TOKEN:
        raise HTTPException(status_code=401, detail="Invalid or missing token")
    return credentials

app = FastAPI(dependencies=[Depends(validate_token)])
app.mount("/.well-known", StaticFiles(directory=".well-known"), name="static")

# Create a sub-application, in order to access just the query endpoint in an OpenAPI schema, found at http://0.0.0.0:8000/sub/openapi.json when the app is running locally
sub_app = FastAPI(
    title="Retrieval Plugin API",
    description="A retrieval API for querying and filtering documents based on natural language queries and metadata",
    version="1.0.0",
    servers=[{"url": "https://your-app-url.com"}],
    dependencies=[Depends(validate_token)],
)
app.mount("/sub", sub_app)


@app.post(
    "/upsert-file",
    response_model=UpsertResponse,
)
async def upsert_file(
    file: UploadFile = File(...),
    metadata: Optional[str] = Form(None),
):
    try:
        metadata_obj = (
            DocumentMetadata.parse_raw(metadata)
            if metadata
            else DocumentMetadata(source=Source.file)
        )
    except:
        metadata_obj = DocumentMetadata(source=Source.file)

    document = await get_document_from_file(file, metadata_obj)

    try:
        ids = await datastore.upsert([document])
        return UpsertResponse(ids=ids)
    except Exception as e:
        logger.error(e)
        raise HTTPException(status_code=500, detail=f"str({e})")


@app.post(
    "/upsert",
    response_model=UpsertResponse,
)
async def upsert(
    request: UpsertRequest = Body(...),
):
    try:
        ids = await datastore.upsert(request.documents)
        return UpsertResponse(ids=ids)
    except Exception as e:
        logger.error(e)
        raise HTTPException(status_code=500, detail="Internal Service Error")


@app.post(
    "/query",
    response_model=QueryResponse,
)
async def query_main(
    request: QueryRequest = Body(...),
):
    try:
        results = await datastore.query(
            request.queries,
        )
        return QueryResponse(results=results)
    except Exception as e:
        logger.error(e)
        raise HTTPException(status_code=500, detail="Internal Service Error")


@sub_app.post(
    "/query",
    response_model=QueryResponse,
    # NOTE: We are describing the shape of the API endpoint input due to a current limitation in parsing arrays of objects from OpenAPI schemas. This will not be necessary in the future.
    description="Accepts search query objects array each with query and optional filter. Break down complex questions into sub-questions. Refine results by criteria, e.g. time / source, don't do this often. Split queries if ResponseTooLargeError occurs.",
)
async def query(
    request: QueryRequest = Body(...),
):
    try:
        results = await datastore.query(
            request.queries,
        )
        return QueryResponse(results=results)
    except Exception as e:
        logger.error(e)
        raise HTTPException(status_code=500, detail="Internal Service Error")


@app.delete(
    "/delete",
    response_model=DeleteResponse,
)
async def delete(
    request: DeleteRequest = Body(...),
):
    if not (request.ids or request.filter or request.delete_all):
        raise HTTPException(
            status_code=400,
            detail="One of ids, filter, or delete_all is required",
        )
    try:
        success = await datastore.delete(
            ids=request.ids,
            filter=request.filter,
            delete_all=request.delete_all,
        )
        return DeleteResponse(success=success)
    except Exception as e:
        logger.error(e)
        raise HTTPException(status_code=500, detail="Internal Service Error")


@app.on_event("startup")
async def startup():
    global datastore
    datastore = await get_datastore()


def start():
    uvicorn.run("server.main:app", host="0.0.0.0", port=8000, reload=True)



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/services/chunks.py
================================================
from typing import Dict, List, Optional, Tuple
import uuid
import os
from models.models import Document, DocumentChunk, DocumentChunkMetadata

import tiktoken

from services.openai import get_embeddings

# Global variables
tokenizer = tiktoken.get_encoding(
    "cl100k_base"
)  # The encoding scheme to use for tokenization

# Constants
CHUNK_SIZE = 1024  # The target size of each text chunk in tokens
MIN_CHUNK_SIZE_CHARS = 350  # The minimum size of each text chunk in characters
MIN_CHUNK_LENGTH_TO_EMBED = 5  # Discard chunks shorter than this
EMBEDDINGS_BATCH_SIZE = int(os.environ.get("OPENAI_EMBEDDING_BATCH_SIZE", 128))  # The number of embeddings to request at a time
MAX_NUM_CHUNKS = 10000000  # The maximum number of chunks to generate from a text


def get_text_chunks(text: str, chunk_token_size: Optional[int]) -> List[str]:
    """
    Split a text into chunks of ~CHUNK_SIZE tokens, based on punctuation and newline boundaries.

    Args:
        text: The text to split into chunks.
        chunk_token_size: The target size of each chunk in tokens, or None to use the default CHUNK_SIZE.

    Returns:
        A list of text chunks, each of which is a string of ~CHUNK_SIZE tokens.
    """
    # Return an empty list if the text is empty or whitespace
    if not text or text.isspace():
        return []

    # Tokenize the text
    tokens = tokenizer.encode(text, disallowed_special=())

    # Initialize an empty list of chunks
    chunks = []

    # Use the provided chunk token size or the default one
    chunk_size = chunk_token_size or CHUNK_SIZE

    # Initialize a counter for the number of chunks
    num_chunks = 0

    # Loop until all tokens are consumed
    while tokens and num_chunks < MAX_NUM_CHUNKS:
        # Take the first chunk_size tokens as a chunk
        chunk = tokens[:chunk_size]

        # Decode the chunk into text
        chunk_text = tokenizer.decode(chunk)

        # Skip the chunk if it is empty or whitespace
        if not chunk_text or chunk_text.isspace():
            # Remove the tokens corresponding to the chunk text from the remaining tokens
            tokens = tokens[len(chunk) :]
            # Continue to the next iteration of the loop
            continue

        # Find the last period or punctuation mark in the chunk
        last_punctuation = max(
            chunk_text.rfind("."),
            chunk_text.rfind("?"),
            chunk_text.rfind("!"),
            chunk_text.rfind("\n"),
        )

        # If there is a punctuation mark, and the last punctuation index is before MIN_CHUNK_SIZE_CHARS
        if last_punctuation != -1 and last_punctuation > MIN_CHUNK_SIZE_CHARS:
            # Truncate the chunk text at the punctuation mark
            chunk_text = chunk_text[: last_punctuation + 1]

        # Remove any newline characters and strip any leading or trailing whitespace
        chunk_text_to_append = chunk_text.replace("\n", " ").strip()

        if len(chunk_text_to_append) > MIN_CHUNK_LENGTH_TO_EMBED:
            # Append the chunk text to the list of chunks
            chunks.append(chunk_text_to_append)

        # Remove the tokens corresponding to the chunk text from the remaining tokens
        tokens = tokens[len(tokenizer.encode(chunk_text, disallowed_special=())) :]

        # Increment the number of chunks
        num_chunks += 1

    # Handle the remaining tokens
    if tokens:
        remaining_text = tokenizer.decode(tokens).replace("\n", " ").strip()
        if len(remaining_text) > MIN_CHUNK_LENGTH_TO_EMBED:
            chunks.append(remaining_text)

    return chunks


def create_document_chunks(
    doc: Document, chunk_token_size: Optional[int]
) -> Tuple[List[DocumentChunk], str]:
    """
    Create a list of document chunks from a document object and return the document id.

    Args:
        doc: The document object to create chunks from. It should have a text attribute and optionally an id and a metadata attribute.
        chunk_token_size: The target size of each chunk in tokens, or None to use the default CHUNK_SIZE.

    Returns:
        A tuple of (doc_chunks, doc_id), where doc_chunks is a list of document chunks, each of which is a DocumentChunk object with an id, a document_id, a text, and a metadata attribute,
        and doc_id is the id of the document object, generated if not provided. The id of each chunk is generated from the document id and a sequential number, and the metadata is copied from the document object.
    """
    # Check if the document text is empty or whitespace
    if not doc.text or doc.text.isspace():
        return [], doc.id or str(uuid.uuid4())

    # Generate a document id if not provided
    doc_id = doc.id or str(uuid.uuid4())

    # Split the document text into chunks
    text_chunks = get_text_chunks(doc.text, chunk_token_size)

    metadata = (
        DocumentChunkMetadata(**doc.metadata.__dict__)
        if doc.metadata is not None
        else DocumentChunkMetadata()
    )

    metadata.document_id = doc_id

    # Initialize an empty list of chunks for this document
    doc_chunks = []

    # Assign each chunk a sequential number and create a DocumentChunk object
    for i, text_chunk in enumerate(text_chunks):
        chunk_id = f"{doc_id}_{i}"
        doc_chunk = DocumentChunk(
            id=chunk_id,
            text=text_chunk,
            metadata=metadata,
        )
        # Append the chunk object to the list of chunks for this document
        doc_chunks.append(doc_chunk)

    # Return the list of chunks and the document id
    return doc_chunks, doc_id


def get_document_chunks(
    documents: List[Document], chunk_token_size: Optional[int]
) -> Dict[str, List[DocumentChunk]]:
    """
    Convert a list of documents into a dictionary from document id to list of document chunks.

    Args:
        documents: The list of documents to convert.
        chunk_token_size: The target size of each chunk in tokens, or None to use the default CHUNK_SIZE.

    Returns:
        A dictionary mapping each document id to a list of document chunks, each of which is a DocumentChunk object
        with text, metadata, and embedding attributes.
    """
    # Initialize an empty dictionary of lists of chunks
    chunks: Dict[str, List[DocumentChunk]] = {}

    # Initialize an empty list of all chunks
    all_chunks: List[DocumentChunk] = []

    # Loop over each document and create chunks
    for doc in documents:
        doc_chunks, doc_id = create_document_chunks(doc, chunk_token_size)

        # Append the chunks for this document to the list of all chunks
        all_chunks.extend(doc_chunks)

        # Add the list of chunks for this document to the dictionary with the document id as the key
        chunks[doc_id] = doc_chunks

    # Check if there are no chunks
    if not all_chunks:
        return {}

    # Get all the embeddings for the document chunks in batches, using get_embeddings
    embeddings: List[List[float]] = []
    for i in range(0, len(all_chunks), EMBEDDINGS_BATCH_SIZE):
        # Get the text of the chunks in the current batch
        batch_texts = [
            chunk.text for chunk in all_chunks[i : i + EMBEDDINGS_BATCH_SIZE]
        ]

        # Get the embeddings for the batch texts
        batch_embeddings = get_embeddings(batch_texts)

        # Append the batch embeddings to the embeddings list
        embeddings.extend(batch_embeddings)

    # Update the document chunk objects with the embeddings
    for i, chunk in enumerate(all_chunks):
        # Assign the embedding from the embeddings list to the chunk object
        chunk.embedding = embeddings[i]

    return chunks



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/services/date.py
================================================
import arrow
from loguru import logger


def to_unix_timestamp(date_str: str) -> int:
    """
    Convert a date string to a unix timestamp (seconds since epoch).

    Args:
        date_str: The date string to convert.

    Returns:
        The unix timestamp corresponding to the date string.

    If the date string cannot be parsed as a valid date format, returns the current unix timestamp and prints a warning.
    """
    # Try to parse the date string using arrow, which supports many common date formats
    try:
        date_obj = arrow.get(date_str)
        return int(date_obj.timestamp())
    except arrow.parser.ParserError:
        # If the parsing fails, return the current unix timestamp and print a warning
        logger.info(f"Invalid date format: {date_str}")
        return int(arrow.now().timestamp())



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/services/extract_metadata.py
================================================
from models.models import Source
from services.openai import get_chat_completion
import json
from typing import Dict
import os
from loguru import logger

def extract_metadata_from_document(text: str) -> Dict[str, str]:
    sources = Source.__members__.keys()
    sources_string = ", ".join(sources)
    # This prompt is just an example, change it to fit your use case
    messages = [
        {
            "role": "system",
            "content": f"""
            Given a document from a user, try to extract the following metadata:
            - source: string, one of {sources_string}
            - url: string or don't specify
            - created_at: string or don't specify
            - author: string or don't specify

            Respond with a JSON containing the extracted metadata in key value pairs. If you don't find a metadata field, don't specify it.
            """,
        },
        {"role": "user", "content": text},
    ]

    # NOTE: Azure Open AI requires deployment id
    # Read environment variable - if not set - not used
    completion = get_chat_completion(
        messages,
        "gpt-4",
        os.environ.get("OPENAI_METADATA_EXTRACTIONMODEL_DEPLOYMENTID")
    )  # TODO: change to your preferred model name

    logger.info(f"completion: {completion}")

    try:
        metadata = json.loads(completion)
    except:
        metadata = {}

    return metadata



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/services/file.py
================================================
import os
from io import BufferedReader
from typing import Optional
from fastapi import UploadFile
import mimetypes
from PyPDF2 import PdfReader
import docx2txt
import csv
import pptx
from loguru import logger

from models.models import Document, DocumentMetadata


async def get_document_from_file(
    file: UploadFile, metadata: DocumentMetadata
) -> Document:
    extracted_text = await extract_text_from_form_file(file)

    doc = Document(text=extracted_text, metadata=metadata)

    return doc


def extract_text_from_filepath(filepath: str, mimetype: Optional[str] = None) -> str:
    """Return the text content of a file given its filepath."""

    if mimetype is None:
        # Get the mimetype of the file based on its extension
        mimetype, _ = mimetypes.guess_type(filepath)

    if not mimetype:
        if filepath.endswith(".md"):
            mimetype = "text/markdown"
        else:
            raise Exception("Unsupported file type")

    try:
        with open(filepath, "rb") as file:
            extracted_text = extract_text_from_file(file, mimetype)
    except Exception as e:
        logger.error(e)
        raise e

    return extracted_text


def extract_text_from_file(file: BufferedReader, mimetype: str) -> str:
    if mimetype == "application/pdf":
        # Extract text from pdf using PyPDF2
        reader = PdfReader(file)
        extracted_text = " ".join([page.extract_text() for page in reader.pages])
    elif mimetype == "text/plain" or mimetype == "text/markdown":
        # Read text from plain text file
        extracted_text = file.read().decode("utf-8")
    elif (
        mimetype
        == "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    ):
        # Extract text from docx using docx2txt
        extracted_text = docx2txt.process(file)
    elif mimetype == "text/csv":
        # Extract text from csv using csv module
        extracted_text = ""
        decoded_buffer = (line.decode("utf-8") for line in file)
        reader = csv.reader(decoded_buffer)
        for row in reader:
            extracted_text += " ".join(row) + "\n"
    elif (
        mimetype
        == "application/vnd.openxmlformats-officedocument.presentationml.presentation"
    ):
        # Extract text from pptx using python-pptx
        extracted_text = ""
        presentation = pptx.Presentation(file)
        for slide in presentation.slides:
            for shape in slide.shapes:
                if shape.has_text_frame:
                    for paragraph in shape.text_frame.paragraphs:
                        for run in paragraph.runs:
                            extracted_text += run.text + " "
                    extracted_text += "\n"
    else:
        # Unsupported file type
        raise ValueError("Unsupported file type: {}".format(mimetype))

    return extracted_text


# Extract text from a file based on its mimetype
async def extract_text_from_form_file(file: UploadFile):
    """Return the text content of a file."""
    # get the file body from the upload file object
    mimetype = file.content_type
    logger.info(f"mimetype: {mimetype}")
    logger.info(f"file.file: {file.file}")
    logger.info("file: ", file)

    file_stream = await file.read()

    temp_file_path = "/tmp/temp_file"

    # write the file to a temporary location
    with open(temp_file_path, "wb") as f:
        f.write(file_stream)

    try:
        extracted_text = extract_text_from_filepath(temp_file_path, mimetype)
    except Exception as e:
        logger.error(e)
        os.remove(temp_file_path)
        raise e

    # remove file from temp location
    os.remove(temp_file_path)

    return extracted_text



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/services/openai.py
================================================
from typing import List
import openai
import os
from loguru import logger

from tenacity import retry, wait_random_exponential, stop_after_attempt


@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(3))
def get_embeddings(texts: List[str]) -> List[List[float]]:
    """
    Embed texts using OpenAI's ada model.

    Args:
        texts: The list of texts to embed.

    Returns:
        A list of embeddings, each of which is a list of floats.

    Raises:
        Exception: If the OpenAI API call fails.
    """
    # Call the OpenAI API to get the embeddings
    # NOTE: Azure Open AI requires deployment id
    #deployment = os.environ.get("OPENAI_EMBEDDINGMODEL_DEPLOYMENTID")
    deployment =None

    response = {}
    if deployment == None:
        response = openai.Embedding.create(input=texts, model="text-embedding-ada-002")
    else:
        response = openai.Embedding.create(input=texts, deployment_id=deployment)

    # Extract the embedding data from the response
    data = response["data"]  # type: ignore

    # Return the embeddings as a list of lists of floats
    return [result["embedding"] for result in data]


@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(3))
def get_chat_completion(
    messages,
    model="gpt-3.5-turbo",  # use "gpt-4" for better results
    deployment_id = None
):
    """
    Generate a chat completion using OpenAI's chat completion API.

    Args:
        messages: The list of messages in the chat history.
        model: The name of the model to use for the completion. Default is gpt-3.5-turbo, which is a fast, cheap and versatile model. Use gpt-4 for higher quality but slower results.

    Returns:
        A string containing the chat completion.

    Raises:
        Exception: If the OpenAI API call fails.
    """
    # call the OpenAI chat completion API with the given messages
    # Note: Azure Open AI requires deployment id
    response = {}
    if deployment_id == None:
        response = openai.ChatCompletion.create(
            model=model,
            messages=messages,
        )
    else:
        response = openai.ChatCompletion.create(
            deployment_id = deployment_id,
            messages=messages,
        )


    choices = response["choices"]  # type: ignore
    completion = choices[0].message.content.strip()
    logger.info(f"Completion: {completion}")
    return completion



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/services/pii_detection.py
================================================
import os
from services.openai import get_chat_completion


def screen_text_for_pii(text: str) -> bool:
    # This prompt is just an example, change it to fit your use case
    messages = [
        {
            "role": "system",
            "content": f"""
            You can only respond with the word "True" or "False", where your answer indicates whether the text in the user's message contains PII.
            Do not explain your answer, and do not use punctuation.
            Your task is to identify whether the text extracted from your company files
            contains sensitive PII information that should not be shared with the broader company. Here are some things to look out for:
            - An email address that identifies a specific person in either the local-part or the domain
            - The postal address of a private residence (must include at least a street name)
            - The postal address of a public place (must include either a street name or business name)
            - Notes about hiring decisions with mentioned names of candidates. The user will send a document for you to analyze.
            """,
        },
        {"role": "user", "content": text},
    ]

    completion = get_chat_completion(
        messages,
        deployment_id=os.environ.get("OPENAI_COMPLETIONMODEL_DEPLOYMENTID")
    )

    if completion.startswith("True"):
        return True

    return False



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/tests/__init__.py
================================================



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/tests/datastore/providers/analyticdb/test_analyticdb_datastore.py
================================================
import pytest
from models.models import (
    DocumentChunkMetadata,
    DocumentMetadataFilter,
    DocumentChunk,
    QueryWithEmbedding,
    Source,
)
from datastore.providers.analyticdb_datastore import (
    OUTPUT_DIM,
    AnalyticDBDataStore,
)


@pytest.fixture
def analyticdb_datastore():
    return AnalyticDBDataStore()


@pytest.fixture
def document_chunk_one():
    doc_id = "zerp"
    doc_chunks = []

    ids = ["abc_123", "def_456", "ghi_789"]
    texts = [
        "lorem ipsum dolor sit amet",
        "consectetur adipiscing elit",
        "sed do eiusmod tempor incididunt",
    ]
    sources = [Source.email, Source.file, Source.chat]
    source_ids = ["foo", "bar", "baz"]
    urls = ["foo.com", "bar.net", "baz.org"]
    created_ats = [
        "1929-10-28T09:30:00-05:00",
        "2009-01-03T16:39:57-08:00",
        "2021-01-21T10:00:00-02:00",
    ]
    authors = ["Max Mustermann", "John Doe", "Jane Doe"]
    embeddings = [[x] * OUTPUT_DIM for x in range(3)]

    for i in range(3):
        chunk = DocumentChunk(
            id=ids[i],
            text=texts[i],
            metadata=DocumentChunkMetadata(
                document_id=doc_id,
                source=sources[i],
                source_id=source_ids[i],
                url=urls[i],
                created_at=created_ats[i],
                author=authors[i],
            ),
            embedding=embeddings[i],  # type: ignore
        )

        doc_chunks.append(chunk)

    return {doc_id: doc_chunks}


@pytest.fixture
def document_chunk_two():
    doc_id_1 = "zerp"
    doc_chunks_1 = []

    ids = ["abc_123", "def_456", "ghi_789"]
    texts = [
        "1lorem ipsum dolor sit amet",
        "2consectetur adipiscing elit",
        "3sed do eiusmod tempor incididunt",
    ]
    sources = [Source.email, Source.file, Source.chat]
    source_ids = ["foo", "bar", "baz"]
    urls = ["foo.com", "bar.net", "baz.org"]
    created_ats = [
        "1929-10-28T09:30:00-05:00",
        "2009-01-03T16:39:57-08:00",
        "3021-01-21T10:00:00-02:00",
    ]
    authors = ["Max Mustermann", "John Doe", "Jane Doe"]
    embeddings = [[x] * OUTPUT_DIM for x in range(3)]

    for i in range(3):
        chunk = DocumentChunk(
            id=ids[i],
            text=texts[i],
            metadata=DocumentChunkMetadata(
                document_id=doc_id_1,
                source=sources[i],
                source_id=source_ids[i],
                url=urls[i],
                created_at=created_ats[i],
                author=authors[i],
            ),
            embedding=embeddings[i],  # type: ignore
        )

        doc_chunks_1.append(chunk)

    doc_id_2 = "merp"
    doc_chunks_2 = []

    ids = ["jkl_123", "lmn_456", "opq_789"]
    texts = [
        "3sdsc efac feas sit qweas",
        "4wert sdfas fdsc",
        "52dsc fdsf eiusmod asdasd incididunt",
    ]
    sources = [Source.email, Source.file, Source.chat]
    source_ids = ["foo", "bar", "baz"]
    urls = ["foo.com", "bar.net", "baz.org"]
    created_ats = [
        "4929-10-28T09:30:00-05:00",
        "5009-01-03T16:39:57-08:00",
        "6021-01-21T10:00:00-02:00",
    ]
    authors = ["Max Mustermann", "John Doe", "Jane Doe"]
    embeddings = [[x] * OUTPUT_DIM for x in range(3, 6)]

    for i in range(3):
        chunk = DocumentChunk(
            id=ids[i],
            text=texts[i],
            metadata=DocumentChunkMetadata(
                document_id=doc_id_2,
                source=sources[i],
                source_id=source_ids[i],
                url=urls[i],
                created_at=created_ats[i],
                author=authors[i],
            ),
            embedding=embeddings[i],  # type: ignore
        )

        doc_chunks_2.append(chunk)

    return {doc_id_1: doc_chunks_1, doc_id_2: doc_chunks_2}


@pytest.mark.asyncio
async def test_upsert(analyticdb_datastore, document_chunk_one):
    await analyticdb_datastore.delete(delete_all=True)
    res = await analyticdb_datastore._upsert(document_chunk_one)
    assert res == list(document_chunk_one.keys())
    query = QueryWithEmbedding(
        query="lorem",
        top_k=10,
        embedding=[0.5] * OUTPUT_DIM,
    )
    query_results = await analyticdb_datastore._query(queries=[query])
    assert 3 == len(query_results[0].results)


@pytest.mark.asyncio
async def test_reload(analyticdb_datastore, document_chunk_one, document_chunk_two):
    await analyticdb_datastore.delete(delete_all=True)

    res = await analyticdb_datastore._upsert(document_chunk_one)
    assert res == list(document_chunk_one.keys())

    query = QueryWithEmbedding(
        query="lorem",
        top_k=10,
        embedding=[0.5] * OUTPUT_DIM,
    )

    query_results = await analyticdb_datastore._query(queries=[query])
    assert 3 == len(query_results[0].results)
    new_store = AnalyticDBDataStore()
    another_in = {i: document_chunk_two[i] for i in document_chunk_two if i != res[0]}
    res = await new_store._upsert(another_in)

    query_results = await analyticdb_datastore._query(queries=[query])
    assert 1 == len(query_results)
    assert 6 == len(query_results[0].results)


@pytest.mark.asyncio
async def test_upsert_query_all(analyticdb_datastore, document_chunk_two):
    await analyticdb_datastore.delete(delete_all=True)
    res = await analyticdb_datastore._upsert(document_chunk_two)
    assert res == list(document_chunk_two.keys())
    # Num entities currently doesn't track deletes
    query = QueryWithEmbedding(
        query="lorem",
        top_k=10,
        embedding=[0.5] * OUTPUT_DIM,
    )
    query_results = await analyticdb_datastore._query(queries=[query])

    assert 1 == len(query_results)
    assert 6 == len(query_results[0].results)


@pytest.mark.asyncio
async def test_query_accuracy(analyticdb_datastore, document_chunk_one):
    await analyticdb_datastore.delete(delete_all=True)
    res = await analyticdb_datastore._upsert(document_chunk_one)
    assert res == list(document_chunk_one.keys())
    query = QueryWithEmbedding(
        query="lorem",
        top_k=1,
        embedding=[0] * OUTPUT_DIM,
    )
    query_results = await analyticdb_datastore._query(queries=[query])

    assert 1 == len(query_results)
    assert 1 == len(query_results[0].results)
    assert 0 == query_results[0].results[0].score
    assert "abc_123" == query_results[0].results[0].id


@pytest.mark.asyncio
async def test_query_filter(analyticdb_datastore, document_chunk_one):
    await analyticdb_datastore.delete(delete_all=True)
    res = await analyticdb_datastore._upsert(document_chunk_one)
    assert res == list(document_chunk_one.keys())
    query = QueryWithEmbedding(
        query="lorem",
        top_k=1,
        embedding=[0] * OUTPUT_DIM,
        filter=DocumentMetadataFilter(
            start_date="2000-01-03T16:39:57-08:00", end_date="2010-01-03T16:39:57-08:00"
        ),
    )
    query_results = await analyticdb_datastore._query(queries=[query])

    assert 1 == len(query_results)
    assert 1 == len(query_results[0].results)
    assert 0 != query_results[0].results[0].score
    assert "def_456" == query_results[0].results[0].id


@pytest.mark.asyncio
async def test_delete_with_date_filter(analyticdb_datastore, document_chunk_one):
    await analyticdb_datastore.delete(delete_all=True)
    res = await analyticdb_datastore._upsert(document_chunk_one)
    assert res == list(document_chunk_one.keys())
    await analyticdb_datastore.delete(
        filter=DocumentMetadataFilter(
            end_date="2009-01-03T16:39:57-08:00",
        )
    )

    query = QueryWithEmbedding(
        query="lorem",
        top_k=9,
        embedding=[0] * OUTPUT_DIM,
    )
    query_results = await analyticdb_datastore._query(queries=[query])

    assert 1 == len(query_results)
    assert 1 == len(query_results[0].results)
    assert "ghi_789" == query_results[0].results[0].id


@pytest.mark.asyncio
async def test_delete_with_source_filter(analyticdb_datastore, document_chunk_one):
    await analyticdb_datastore.delete(delete_all=True)
    res = await analyticdb_datastore._upsert(document_chunk_one)
    assert res == list(document_chunk_one.keys())
    await analyticdb_datastore.delete(
        filter=DocumentMetadataFilter(
            source=Source.email,
        )
    )

    query = QueryWithEmbedding(
        query="lorem",
        top_k=9,
        embedding=[0] * OUTPUT_DIM,
    )
    query_results = await analyticdb_datastore._query(queries=[query])

    assert 1 == len(query_results)
    assert 2 == len(query_results[0].results)
    assert "def_456" == query_results[0].results[0].id


@pytest.mark.asyncio
async def test_delete_with_document_id_filter(analyticdb_datastore, document_chunk_one):
    await analyticdb_datastore.delete(delete_all=True)
    res = await analyticdb_datastore._upsert(document_chunk_one)
    assert res == list(document_chunk_one.keys())
    await analyticdb_datastore.delete(
        filter=DocumentMetadataFilter(
            document_id=res[0],
        )
    )
    query = QueryWithEmbedding(
        query="lorem",
        top_k=9,
        embedding=[0] * OUTPUT_DIM,
    )
    query_results = await analyticdb_datastore._query(queries=[query])

    assert 1 == len(query_results)
    assert 0 == len(query_results[0].results)


@pytest.mark.asyncio
async def test_delete_with_document_id(analyticdb_datastore, document_chunk_one):
    await analyticdb_datastore.delete(delete_all=True)
    res = await analyticdb_datastore._upsert(document_chunk_one)
    assert res == list(document_chunk_one.keys())
    await analyticdb_datastore.delete([res[0]])

    query = QueryWithEmbedding(
        query="lorem",
        top_k=9,
        embedding=[0] * OUTPUT_DIM,
    )
    query_results = await analyticdb_datastore._query(queries=[query])

    assert 1 == len(query_results)
    assert 0 == len(query_results[0].results)


# if __name__ == '__main__':
#     import sys
#     import pytest
#     pytest.main(sys.argv)



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/tests/datastore/providers/azuresearch/test_azuresearch_datastore.py
================================================
import pytest
import os
import time
from typing import Union
from azure.search.documents.indexes import SearchIndexClient
from models.models import DocumentMetadataFilter, Query, Source, Document, DocumentMetadata

AZURESEARCH_TEST_INDEX = "testindex"
os.environ["AZURESEARCH_INDEX"] = AZURESEARCH_TEST_INDEX
if os.environ.get("AZURESEARCH_SERVICE") == None:
    os.environ["AZURESEARCH_SERVICE"] = "invalid service name" # Will fail anyway if not set to a real service, but allows tests to be discovered

import datastore.providers.azuresearch_datastore
from datastore.providers.azuresearch_datastore import AzureSearchDataStore

@pytest.fixture(scope="module")
def azuresearch_mgmt_client():
    service = os.environ["AZURESEARCH_SERVICE"]
    return SearchIndexClient(
        endpoint=f"https://{service}.search.windows.net",
        credential=AzureSearchDataStore._create_credentials(False)
    )    

def test_translate_filter():
    assert AzureSearchDataStore._translate_filter(
        DocumentMetadataFilter()
    ) == None

    for field in ["document_id", "source", "source_id", "author"]:
        value = Source.file if field == "source" else f"test_{field}"
        needs_escaping_value = None if field == "source" else f"test'_{field}"
        assert AzureSearchDataStore._translate_filter(
            DocumentMetadataFilter(**{field: value})
        ) == f"{field} eq '{value}'"
        if needs_escaping_value != None:
            assert AzureSearchDataStore._translate_filter(
                DocumentMetadataFilter(**{field: needs_escaping_value})
            ) == f"{field} eq 'test''_{field}'"

    assert AzureSearchDataStore._translate_filter(
        DocumentMetadataFilter(
            document_id = "test_document_id",
            source = Source.file,
            source_id = "test_source_id",
            author = "test_author"
        )
    ) == "document_id eq 'test_document_id' and source eq 'file' and source_id eq 'test_source_id' and author eq 'test_author'"

    with pytest.raises(ValueError):
        assert AzureSearchDataStore._translate_filter(
            DocumentMetadataFilter(start_date="2023-01-01")
        )
    with pytest.raises(ValueError):
        assert AzureSearchDataStore._translate_filter(
            DocumentMetadataFilter(end_date="2023-01-01")
        )
    
    assert AzureSearchDataStore._translate_filter(
        DocumentMetadataFilter(start_date="2023-01-01T00:00:00Z", end_date="2023-01-02T00:00:00Z", document_id = "test_document_id")
    ) == "document_id eq 'test_document_id' and created_at ge 2023-01-01T00:00:00Z and created_at le 2023-01-02T00:00:00Z"

@pytest.mark.asyncio
async def test_lifecycle_hybrid(azuresearch_mgmt_client: SearchIndexClient):
    datastore.providers.azuresearch_datastore.AZURESEARCH_DISABLE_HYBRID = None
    datastore.providers.azuresearch_datastore.AZURESEARCH_SEMANTIC_CONFIG = None
    await lifecycle(azuresearch_mgmt_client)

@pytest.mark.asyncio
async def test_lifecycle_vectors_only(azuresearch_mgmt_client: SearchIndexClient):
    datastore.providers.azuresearch_datastore.AZURESEARCH_DISABLE_HYBRID = "1"
    datastore.providers.azuresearch_datastore.AZURESEARCH_SEMANTIC_CONFIG = None
    await lifecycle(azuresearch_mgmt_client)

@pytest.mark.asyncio
async def test_lifecycle_semantic(azuresearch_mgmt_client: SearchIndexClient):
    datastore.providers.azuresearch_datastore.AZURESEARCH_DISABLE_HYBRID = None
    datastore.providers.azuresearch_datastore.AZURESEARCH_SEMANTIC_CONFIG = "testsemconfig"
    await lifecycle(azuresearch_mgmt_client)

async def lifecycle(azuresearch_mgmt_client: SearchIndexClient):
    if AZURESEARCH_TEST_INDEX in azuresearch_mgmt_client.list_index_names():
        azuresearch_mgmt_client.delete_index(AZURESEARCH_TEST_INDEX)
    assert AZURESEARCH_TEST_INDEX not in azuresearch_mgmt_client.list_index_names()
    try:
        store = AzureSearchDataStore()
        index = azuresearch_mgmt_client.get_index(AZURESEARCH_TEST_INDEX)
        assert index is not None

        result = await store.upsert([
            Document(
                id="test_id_1", 
                text="test text", 
                metadata=DocumentMetadata(source=Source.file, source_id="test_source_id", author="test_author", created_at="2023-01-01T00:00:00Z", url="http://some-test-url/path")),
            Document(
                id="test_id_2+", 
                text="different", 
                metadata=DocumentMetadata(source=Source.file, source_id="test_source_id", author="test_author", created_at="2023-01-01T00:00:00Z", url="http://some-test-url/path"))])
        assert len(result) == 2 and result[0] == "test_id_1" and result[1] == "test_id_2+"

        # query in a loop in case we need to retry since documents aren't searchable synchronosuly after updates
        for _ in range(4):
            time.sleep(0.25)
            result = await store.query([Query(query="text")])
            if len(result) > 0 and len(result[0].results) > 0:
                break
        assert len(result) == 1 and len(result[0].results) == 2
        assert result[0].results[0].metadata.document_id == "test_id_1" and result[0].results[1].metadata.document_id == "test_id_2+"

        result = await store.query([Query(query="text", filter=DocumentMetadataFilter(source_id="test_source_id"))])
        assert len(result) == 1 and len(result[0].results) == 2
        assert result[0].results[0].metadata.document_id == "test_id_1" and result[0].results[1].metadata.document_id == "test_id_2+"

        result = await store.query([Query(query="text", filter=DocumentMetadataFilter(source_id="nonexisting_id"))])
        assert len(result) == 1 and len(result[0].results) == 0

        result = await store.query([Query(query="text", filter=DocumentMetadataFilter(start_date="2023-01-02T00:00:00Z"))])
        assert len(result) == 1 and len(result[0].results) == 0

        result = await store.query([Query(query="text", filter=DocumentMetadataFilter(start_date="2023-01-01T00:00:00Z"))])
        assert len(result) == 1 and len(result[0].results) == 2
        assert result[0].results[0].metadata.document_id == "test_id_1" and result[0].results[1].metadata.document_id == "test_id_2+"

        result = await store.query([Query(query="text", filter=DocumentMetadataFilter(end_date="2022-12-31T00:00:00Z"))])
        assert len(result) == 1 and len(result[0].results) == 0

        result = await store.query([Query(query="text", filter=DocumentMetadataFilter(end_date="2023-01-02T00:00:00Z"))])
        assert len(result) == 1 and len(result[0].results) == 2
        assert result[0].results[0].metadata.document_id == "test_id_1" and result[0].results[1].metadata.document_id == "test_id_2+"

        # query in a loop in case we need to retry since documents aren't searchable synchronosuly after updates
        assert await store.delete(["test_id_1", "test_id_2+"])
        for _ in range(4):
            time.sleep(0.25)
            result = await store.query([Query(query="text")])
            if len(result) > 0 and len(result[0].results) == 0:
                break
        assert len(result) == 1 and len(result[0].results) == 0
    finally:
        azuresearch_mgmt_client.delete_index(AZURESEARCH_TEST_INDEX)



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/tests/datastore/providers/chroma/test_chroma_datastore.py
================================================
import shutil
from typing import Dict, List
import pytest
import random

from datastore.providers.chroma_datastore import ChromaDataStore
from models.models import (
    DocumentChunk,
    DocumentChunkMetadata,
    DocumentMetadataFilter,
    QueryWithEmbedding,
    Source,
)

TEST_PERSISTENCE_DIR = "chroma_test_datastore"
COLLECTION_NAME = "documents"


def ephemeral_chroma_datastore() -> ChromaDataStore:
    # Initialize an ephemeral in-memory ChromaDB instance
    return ChromaDataStore(
        collection_name=COLLECTION_NAME, in_memory=True, persistence_dir=None
    )


def persisted_chroma_datastore() -> ChromaDataStore:
    # Initialize an in-memory ChromaDB instance with persistence
    return ChromaDataStore(
        collection_name=COLLECTION_NAME,
        in_memory=True,
        persistence_dir=TEST_PERSISTENCE_DIR,
    )


def get_chroma_datastore() -> ChromaDataStore:
    yield ephemeral_chroma_datastore()
    yield persisted_chroma_datastore()
    # Delete the persistence directory after the test


@pytest.fixture(autouse=True)
def cleanup():
    yield
    shutil.rmtree(TEST_PERSISTENCE_DIR, ignore_errors=True)


# Seed for deterministic testing
random.seed(0)


def create_embedding(dim: int) -> List[float]:
    return [random.random() for _ in range(dim)]


# Data fixtures
TEST_EMBEDDING_DIM = 5
N_TEST_CHUNKS = 5


@pytest.fixture
def initial_document_chunks() -> Dict[str, List[DocumentChunk]]:
    first_doc_chunks = [
        DocumentChunk(
            id=f"first-doc-{i}",
            text=f"Lorem ipsum {i}",
            metadata=DocumentChunkMetadata(),
            embedding=create_embedding(TEST_EMBEDDING_DIM),
        )
        for i in range(N_TEST_CHUNKS)
    ]
    return {
        "first-doc": first_doc_chunks,
    }


@pytest.fixture
def document_chunks(initial_document_chunks) -> Dict[str, List[DocumentChunk]]:
    doc_chunks = initial_document_chunks

    for k, v in doc_chunks.items():
        for chunk in v:
            chunk.metadata = DocumentChunkMetadata(
                source=Source.email, created_at="2023-04-03", document_id="first-doc"
            )
            chunk.embedding = create_embedding(TEST_EMBEDDING_DIM)

    doc_chunks["second-doc"] = [
        DocumentChunk(
            id=f"second-doc-{i}",
            text=f"Dolor sit amet {i}",
            metadata=DocumentChunkMetadata(
                created_at="2023-04-04", document_id="second-doc"
            ),
            embedding=create_embedding(TEST_EMBEDDING_DIM),
        )
        for i in range(N_TEST_CHUNKS)
    ]

    return doc_chunks


@pytest.mark.asyncio
async def test_add_chunks(document_chunks: Dict[str, List[DocumentChunk]]):
    for datastore in get_chroma_datastore():
        await datastore.delete(delete_all=True)
        assert datastore._collection.count() == 0

        print(document_chunks)

        assert await datastore._upsert(document_chunks) == list(document_chunks.keys())
        assert datastore._collection.count() == sum(
            len(v) for v in document_chunks.values()
        )


@pytest.mark.asyncio
async def test_upsert(
    initial_document_chunks: Dict[str, List[DocumentChunk]],
    document_chunks: Dict[str, List[DocumentChunk]],
):
    for datastore in get_chroma_datastore():
        await datastore.delete(delete_all=True)

        assert await datastore._upsert(initial_document_chunks) == list(
            initial_document_chunks.keys()
        )
        assert datastore._collection.count() == sum(
            len(v) for v in initial_document_chunks.values()
        )

        assert await datastore._upsert(document_chunks) == list(document_chunks.keys())
        assert datastore._collection.count() == sum(
            len(v) for v in document_chunks.values()
        )


@pytest.mark.asyncio
async def test_add_and_query_all(document_chunks):
    for datastore in get_chroma_datastore():
        await datastore.delete(delete_all=True)

        await datastore._upsert(document_chunks) == list(document_chunks.keys())

        query = QueryWithEmbedding(
            query="",
            embedding=create_embedding(TEST_EMBEDDING_DIM),
            top_k=10,
        )
        query_results = await datastore._query(queries=[query])
        assert 1 == len(query_results)
        assert 10 == len(query_results[0].results)


@pytest.mark.asyncio
async def test_query_accuracy(document_chunks):
    for _, v in document_chunks.items():
        for chunk in v:
            print(f"id: {chunk.id} emb: {chunk.embedding}")

    def add_noise_to_embedding(embedding: List[float], eps: float = 0) -> List[float]:
        return [x + eps * (1.0 - 2 * random.random()) for x in embedding]

    for datastore in get_chroma_datastore():
        await datastore.delete(delete_all=True)

        print(datastore._collection.get(include=["embeddings"]))

        res = await datastore._upsert(document_chunks)

        res = datastore._collection.get(include=["embeddings"])
        for id, emb in zip(res["ids"], res["embeddings"]):
            print(f"id: {id} emb: {emb}")

        for _, v in document_chunks.items():
            for chunk in v:
                print(f"chunk: {chunk}")
                query = QueryWithEmbedding(
                    query="",
                    embedding=add_noise_to_embedding(chunk.embedding),
                    top_k=1,
                )
                query_results = await datastore._query(queries=[query])
                print(query_results)
                assert query_results[0].results[0].id == chunk.id


@pytest.mark.asyncio
async def test_query_filter_by_id(document_chunks):
    for datastore in get_chroma_datastore():
        await datastore.delete(delete_all=True)

        await datastore._upsert(document_chunks)

        for doc_id, chunks in document_chunks.items():
            query = QueryWithEmbedding(
                query="",
                embedding=chunks[0].embedding,
                top_k=N_TEST_CHUNKS,
                filter=DocumentMetadataFilter(document_id=doc_id),
            )
            query_results = await datastore._query(queries=[query])
            # Assert that all document chunks are returned
            assert len(query_results[0].results) == len(chunks)
            assert all(
                [
                    result.id in [chunk.id for chunk in chunks]
                    for result in query_results[0].results
                ]
            )


@pytest.mark.asyncio
async def test_query_filter_by_date(document_chunks):
    for datastore in get_chroma_datastore():
        await datastore.delete(delete_all=True)

        await datastore._upsert(document_chunks)

        # Filter by dates for only the first document
        query = QueryWithEmbedding(
            query="",
            embedding=document_chunks["first-doc"][0].embedding,
            top_k=N_TEST_CHUNKS,
            filter=DocumentMetadataFilter(
                start_date="2023-04-03", end_date="2023-04-03"
            ),
        )

        query_results = await datastore._query(queries=[query])

        # Assert that only the first document is returned
        assert len(query_results[0].results) == len(document_chunks["first-doc"])
        assert all(
            [
                result.id in [chunk.id for chunk in document_chunks["first-doc"]]
                for result in query_results[0].results
            ]
        )

        # Filter for the entire date span
        query = QueryWithEmbedding(
            query="",
            embedding=document_chunks["first-doc"][0].embedding,
            top_k=N_TEST_CHUNKS * len(document_chunks),
            filter=DocumentMetadataFilter(
                start_date="2023-04-03", end_date="2023-04-04"
            ),
        )

        query_results = await datastore._query(queries=[query])

        # Assert that both documents are returned
        assert len(query_results[0].results) == len(document_chunks["first-doc"]) + len(
            document_chunks["second-doc"]
        )
        assert all(
            [
                result.id
                in [chunk.id for chunk in document_chunks["first-doc"]]
                + [chunk.id for chunk in document_chunks["second-doc"]]
                for result in query_results[0].results
            ]
        )


@pytest.mark.asyncio
async def test_delete_by_id(document_chunks):
    for datastore in get_chroma_datastore():
        await datastore.delete(delete_all=True)

        await datastore._upsert(document_chunks)

        # Delete the first document
        await datastore.delete(ids=["first-doc"])

        # Assert that the first document is deleted
        query = QueryWithEmbedding(
            query="",
            embedding=document_chunks["first-doc"][0].embedding,
            top_k=N_TEST_CHUNKS,
        )
        query_results = await datastore._query(queries=[query])

        # Assert that only the second document is still there
        query_results = await datastore._query(queries=[query])
        assert len(query_results[0].results) == len(document_chunks["second-doc"])

        assert all(
            [
                result.id in [chunk.id for chunk in document_chunks["second-doc"]]
                for result in query_results[0].results
            ]
        )



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/tests/datastore/providers/elasticsearch/test_elasticsearch_datastore.py
================================================
import pytest
from models.models import (
    DocumentChunkMetadata,
    DocumentMetadataFilter,
    DocumentChunk,
    QueryWithEmbedding,
    Source,
)
from datastore.providers.elasticsearch_datastore import (
    ElasticsearchDataStore,
)
import time

DIM_SIZE = 1536


@pytest.fixture
def elasticsearch_datastore():
    return ElasticsearchDataStore()


def sample_embedding(one_element_poz: int):
    embedding = [0] * DIM_SIZE
    embedding[one_element_poz % DIM_SIZE] = 1
    return embedding


def sample_embeddings(num: int, one_element_start: int = 0):
    embeddings = []
    for x in range(num):
        embedding = [0] * DIM_SIZE
        embedding[(x + one_element_start) % DIM_SIZE] = 1
        embeddings.append(embedding)
    return embeddings


@pytest.fixture
def document_chunk_one():
    doc_id = "abc"
    doc_chunks = []

    ids = ["123", "456", "789"]
    texts = [
        "Aenean euismod bibendum laoreet",
        "Vivamus non enim vitae tortor",
        "Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia curae",
    ]
    sources = [Source.email, Source.file, Source.chat]
    created_ats = [
        "1929-10-28T09:30:00-05:00",
        "2009-01-03T16:39:57-08:00",
        "2021-01-21T10:00:00-02:00",
    ]
    authors = ["Fred Smith", "Bob Doe", "Appleton Doe"]

    embeddings = sample_embeddings(len(texts))

    for i in range(3):
        chunk = DocumentChunk(
            id=ids[i],
            text=texts[i],
            metadata=DocumentChunkMetadata(
                document_id=doc_id,
                source=sources[i],
                created_at=created_ats[i],
                author=authors[i],
            ),
            embedding=embeddings[i],  # type: ignore
        )

        doc_chunks.append(chunk)

    return {doc_id: doc_chunks}


async def test_upsert(elasticsearch_datastore, document_chunk_one):
    await elasticsearch_datastore.delete(delete_all=True)
    res = await elasticsearch_datastore._upsert(document_chunk_one)
    assert res == list(document_chunk_one.keys())
    time.sleep(1)

    results = elasticsearch_datastore.client.search(
        index=elasticsearch_datastore.index_name, query={"match_all": {}}
    )
    assert results["hits"]["total"]["value"] == 3
    elasticsearch_datastore.client.indices.delete(
        index=elasticsearch_datastore.index_name
    )


async def test_upsert_query_all(elasticsearch_datastore, document_chunk_one):
    await elasticsearch_datastore.delete(delete_all=True)
    res = await elasticsearch_datastore._upsert(document_chunk_one)
    assert res == list(document_chunk_one.keys())
    time.sleep(1)

    query = QueryWithEmbedding(
        query="Aenean",
        top_k=10,
        embedding=sample_embedding(0),  # type: ignore
    )
    query_results = await elasticsearch_datastore._query(queries=[query])

    assert 1 == len(query_results)
    assert 3 == len(query_results[0].results)


async def test_delete_with_document_id(elasticsearch_datastore, document_chunk_one):
    await elasticsearch_datastore.delete(delete_all=True)
    res = await elasticsearch_datastore._upsert(document_chunk_one)
    time.sleep(1)
    assert res == list(document_chunk_one.keys())
    await elasticsearch_datastore.delete([res[0]])
    time.sleep(1)

    query = QueryWithEmbedding(
        query="Aenean",
        top_k=9,
        embedding=sample_embedding(0),  # type: ignore
    )
    query_results = await elasticsearch_datastore._query(queries=[query])

    assert 1 == len(query_results)
    assert 0 == len(query_results[0].results)

    elasticsearch_datastore.client.indices.delete(
        index=elasticsearch_datastore.index_name
    )


async def test_delete_with_source_filter(elasticsearch_datastore, document_chunk_one):
    await elasticsearch_datastore.delete(delete_all=True)
    res = await elasticsearch_datastore._upsert(document_chunk_one)
    assert res == list(document_chunk_one.keys())
    time.sleep(1)

    await elasticsearch_datastore.delete(
        filter=DocumentMetadataFilter(
            source=Source.email,
        )
    )

    time.sleep(1)

    query = QueryWithEmbedding(
        query="Aenean",
        top_k=9,
        embedding=sample_embedding(0),  # type: ignore
    )
    query_results = await elasticsearch_datastore._query(queries=[query])

    assert 1 == len(query_results)
    assert 2 == len(query_results[0].results)
    assert "456" == query_results[0].results[0].id

    elasticsearch_datastore.client.indices.delete(
        index=elasticsearch_datastore.index_name
    )



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/tests/datastore/providers/llama/test_llama_datastore.py
================================================
from typing import Dict, List
import pytest
from datastore.providers.llama_datastore import LlamaDataStore
from models.models import DocumentChunk, DocumentChunkMetadata, QueryWithEmbedding


def create_embedding(non_zero_pos: int, size: int) -> List[float]:
    vector = [0.0] * size
    vector[non_zero_pos % size] = 1.0
    return vector


@pytest.fixture
def initial_document_chunks() -> Dict[str, List[DocumentChunk]]:
    first_doc_chunks = [
        DocumentChunk(
            id=f"first-doc-{i}",
            text=f"Lorem ipsum {i}",
            metadata=DocumentChunkMetadata(),
            embedding=create_embedding(i, 5),
        )
        for i in range(4, 7)
    ]
    return {
        "first-doc": first_doc_chunks,
    }


@pytest.fixture
def queries() -> List[QueryWithEmbedding]:
    queries = [
        QueryWithEmbedding(
            query='Query 1',
            top_k=1,
            embedding=create_embedding(4, 5),
        ),
        QueryWithEmbedding(
            query='Query 2',
            top_k=2,
            embedding=create_embedding(5, 5),
        ),
    ]
    return queries


@pytest.fixture
def llama_datastore() -> LlamaDataStore:
    return LlamaDataStore()

@pytest.mark.asyncio
async def test_upsert(
    llama_datastore: LlamaDataStore, 
    initial_document_chunks: Dict[str, List[DocumentChunk]]
) -> None:
    """Test basic upsert."""
    doc_ids = await llama_datastore._upsert(initial_document_chunks)
    assert doc_ids == [doc_id for doc_id in initial_document_chunks]


@pytest.mark.asyncio
async def test_query(
    llama_datastore: LlamaDataStore, 
    initial_document_chunks: Dict[str, List[DocumentChunk]],
    queries: List[QueryWithEmbedding],
) -> None:
    """Test basic query."""
    # insert to prepare for test
    await llama_datastore._upsert(initial_document_chunks)

    query_results = await llama_datastore._query(queries)
    assert len(query_results) == len(queries)

    query_0_results = query_results[0].results 
    query_1_results = query_results[1].results

    assert len(query_0_results) == 1
    assert len(query_1_results) == 2
    
    # NOTE: this is the correct behavior
    assert query_0_results[0].id == 'first-doc-4'
    assert query_1_results[0].id == 'first-doc-5'
    assert query_1_results[1].id == 'first-doc-4'


@pytest.mark.asyncio
async def test_delete(
    llama_datastore: LlamaDataStore, 
    initial_document_chunks: Dict[str, List[DocumentChunk]],
) -> None:
    # insert to prepare for test
    await llama_datastore._upsert(initial_document_chunks)

    is_success = llama_datastore.delete(['first-doc'])
    assert is_success




================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/tests/datastore/providers/milvus/test_milvus_datastore.py
================================================
# from pathlib import Path
# from dotenv import find_dotenv, load_dotenv
# env_path = Path(".") / "milvus.env"
# load_dotenv(dotenv_path=env_path, verbose=True)

import pytest
from models.models import (
    DocumentChunkMetadata,
    DocumentMetadataFilter,
    DocumentChunk,
    QueryWithEmbedding,
    Source,
)
from datastore.providers.milvus_datastore import (
    OUTPUT_DIM,
    MilvusDataStore,
)


@pytest.fixture
def milvus_datastore():
    return MilvusDataStore(consistency_level = "Strong")


def sample_embedding(one_element_poz: int):
    embedding = [0] * OUTPUT_DIM
    embedding[one_element_poz % OUTPUT_DIM] = 1
    return embedding

def sample_embeddings(num: int, one_element_start: int = 0):
    # since metric type is consine, we create vector contains only one element 1, others 0
    embeddings = []
    for x in range(num):
        embedding = [0] * OUTPUT_DIM
        embedding[(x + one_element_start) % OUTPUT_DIM] = 1
        embeddings.append(embedding)
    return embeddings

@pytest.fixture
def document_chunk_one():
    doc_id = "zerp"
    doc_chunks = []

    ids = ["abc_123", "def_456", "ghi_789"]
    texts = [
        "lorem ipsum dolor sit amet",
        "consectetur adipiscing elit",
        "sed do eiusmod tempor incididunt",
    ]
    sources = [Source.email, Source.file, Source.chat]
    source_ids = ["foo", "bar", "baz"]
    urls = ["foo.com", "bar.net", "baz.org"]
    created_ats = [
        "1929-10-28T09:30:00-05:00",
        "2009-01-03T16:39:57-08:00",
        "2021-01-21T10:00:00-02:00",
    ]
    authors = ["Max Mustermann", "John Doe", "Jane Doe"]

    embeddings = sample_embeddings(len(texts))

    for i in range(3):
        chunk = DocumentChunk(
            id=ids[i],
            text=texts[i],
            metadata=DocumentChunkMetadata(
                document_id=doc_id,
                source=sources[i],
                source_id=source_ids[i],
                url=urls[i],
                created_at=created_ats[i],
                author=authors[i],
            ),
            embedding=embeddings[i],  # type: ignore
        )

        doc_chunks.append(chunk)

    return {doc_id: doc_chunks}


@pytest.fixture
def document_chunk_two():
    doc_id_1 = "zerp"
    doc_chunks_1 = []

    ids = ["abc_123", "def_456", "ghi_789"]
    texts = [
        "1lorem ipsum dolor sit amet",
        "2consectetur adipiscing elit",
        "3sed do eiusmod tempor incididunt",
    ]
    sources = [Source.email, Source.file, Source.chat]
    source_ids = ["foo", "bar", "baz"]
    urls = ["foo.com", "bar.net", "baz.org"]
    created_ats = [
        "1929-10-28T09:30:00-05:00",
        "2009-01-03T16:39:57-08:00",
        "3021-01-21T10:00:00-02:00",
    ]
    authors = ["Max Mustermann", "John Doe", "Jane Doe"]
    embeddings = sample_embeddings(len(texts))

    for i in range(3):
        chunk = DocumentChunk(
            id=ids[i],
            text=texts[i],
            metadata=DocumentChunkMetadata(
                document_id=doc_id_1,
                source=sources[i],
                source_id=source_ids[i],
                url=urls[i],
                created_at=created_ats[i],
                author=authors[i],
            ),
            embedding=embeddings[i],  # type: ignore
        )

        doc_chunks_1.append(chunk)

    doc_id_2 = "merp"
    doc_chunks_2 = []

    ids = ["jkl_123", "lmn_456", "opq_789"]
    texts = [
        "3sdsc efac feas sit qweas",
        "4wert sdfas fdsc",
        "52dsc fdsf eiusmod asdasd incididunt",
    ]
    sources = [Source.email, Source.file, Source.chat]
    source_ids = ["foo", "bar", "baz"]
    urls = ["foo.com", "bar.net", "baz.org"]
    created_ats = [
        "4929-10-28T09:30:00-05:00",
        "5009-01-03T16:39:57-08:00",
        "6021-01-21T10:00:00-02:00",
    ]
    authors = ["Max Mustermann", "John Doe", "Jane Doe"]
    embeddings = sample_embeddings(len(texts), 3)

    for i in range(3):
        chunk = DocumentChunk(
            id=ids[i],
            text=texts[i],
            metadata=DocumentChunkMetadata(
                document_id=doc_id_2,
                source=sources[i],
                source_id=source_ids[i],
                url=urls[i],
                created_at=created_ats[i],
                author=authors[i],
            ),
            embedding=embeddings[i],  # type: ignore
        )

        doc_chunks_2.append(chunk)

    return {doc_id_1: doc_chunks_1, doc_id_2: doc_chunks_2}


@pytest.mark.asyncio
async def test_upsert(milvus_datastore, document_chunk_one):
    await milvus_datastore.delete(delete_all=True)
    res = await milvus_datastore._upsert(document_chunk_one)
    assert res == list(document_chunk_one.keys())
    milvus_datastore.col.flush()
    assert 3 == milvus_datastore.col.num_entities
    milvus_datastore.col.drop()


@pytest.mark.asyncio
async def test_reload(milvus_datastore, document_chunk_one, document_chunk_two):
    await milvus_datastore.delete(delete_all=True)

    res = await milvus_datastore._upsert(document_chunk_one)
    assert res == list(document_chunk_one.keys())
    milvus_datastore.col.flush()
    assert 3 == milvus_datastore.col.num_entities

    new_store = MilvusDataStore()
    another_in = {i: document_chunk_two[i] for i in document_chunk_two if i != res[0]}
    res = await new_store._upsert(another_in)
    new_store.col.flush()
    assert 6 == new_store.col.num_entities
    query = QueryWithEmbedding(
        query="lorem",
        top_k=10,
        embedding=sample_embedding(0),
    )
    query_results = await milvus_datastore._query(queries=[query])
    assert 1 == len(query_results)
    new_store.col.drop()


@pytest.mark.asyncio
async def test_upsert_query_all(milvus_datastore, document_chunk_two):
    await milvus_datastore.delete(delete_all=True)
    res = await milvus_datastore._upsert(document_chunk_two)
    assert res == list(document_chunk_two.keys())
    milvus_datastore.col.flush()

    # Num entities currently doesn't track deletes
    query = QueryWithEmbedding(
        query="lorem",
        top_k=10,
        embedding=sample_embedding(0),
    )
    query_results = await milvus_datastore._query(queries=[query])

    assert 1 == len(query_results)
    assert 6 == len(query_results[0].results)
    milvus_datastore.col.drop()


@pytest.mark.asyncio
async def test_query_accuracy(milvus_datastore, document_chunk_one):
    await milvus_datastore.delete(delete_all=True)
    res = await milvus_datastore._upsert(document_chunk_one)
    assert res == list(document_chunk_one.keys())
    milvus_datastore.col.flush()
    query = QueryWithEmbedding(
        query="lorem",
        top_k=1,
        embedding=sample_embedding(0),
    )
    query_results = await milvus_datastore._query(queries=[query])

    assert 1 == len(query_results)
    assert 1 == len(query_results[0].results)
    assert 1.0 == query_results[0].results[0].score
    assert "abc_123" == query_results[0].results[0].id
    milvus_datastore.col.drop()


@pytest.mark.asyncio
async def test_query_filter(milvus_datastore, document_chunk_one):
    await milvus_datastore.delete(delete_all=True)
    res = await milvus_datastore._upsert(document_chunk_one)
    assert res == list(document_chunk_one.keys())
    milvus_datastore.col.flush()
    query = QueryWithEmbedding(
        query="lorem",
        top_k=1,
        embedding=sample_embedding(0),
        filter=DocumentMetadataFilter(
            start_date="2000-01-03T16:39:57-08:00", end_date="2010-01-03T16:39:57-08:00"
        ),
    )
    query_results = await milvus_datastore._query(queries=[query])

    assert 1 == len(query_results)
    assert 1 == len(query_results[0].results)
    assert 1.0 != query_results[0].results[0].score
    assert "def_456" == query_results[0].results[0].id
    milvus_datastore.col.drop()


@pytest.mark.asyncio
async def test_delete_with_date_filter(milvus_datastore, document_chunk_one):
    await milvus_datastore.delete(delete_all=True)
    res = await milvus_datastore._upsert(document_chunk_one)
    assert res == list(document_chunk_one.keys())
    milvus_datastore.col.flush()
    await milvus_datastore.delete(
        filter=DocumentMetadataFilter(
            end_date="2009-01-03T16:39:57-08:00",
        )
    )

    query = QueryWithEmbedding(
        query="lorem",
        top_k=9,
        embedding=sample_embedding(0),
    )
    query_results = await milvus_datastore._query(queries=[query])

    assert 1 == len(query_results)
    assert 1 == len(query_results[0].results)
    assert "ghi_789" == query_results[0].results[0].id
    milvus_datastore.col.drop()


@pytest.mark.asyncio
async def test_delete_with_source_filter(milvus_datastore, document_chunk_one):
    await milvus_datastore.delete(delete_all=True)
    res = await milvus_datastore._upsert(document_chunk_one)
    assert res == list(document_chunk_one.keys())
    milvus_datastore.col.flush()
    await milvus_datastore.delete(
        filter=DocumentMetadataFilter(
            source=Source.email,
        )
    )

    query = QueryWithEmbedding(
        query="lorem",
        top_k=9,
        embedding=sample_embedding(0),
    )
    query_results = await milvus_datastore._query(queries=[query])

    assert 1 == len(query_results)
    assert 2 == len(query_results[0].results)
    assert "def_456" == query_results[0].results[0].id
    milvus_datastore.col.drop()


@pytest.mark.asyncio
async def test_delete_with_document_id_filter(milvus_datastore, document_chunk_one):
    await milvus_datastore.delete(delete_all=True)
    res = await milvus_datastore._upsert(document_chunk_one)
    assert res == list(document_chunk_one.keys())
    milvus_datastore.col.flush()
    await milvus_datastore.delete(
        filter=DocumentMetadataFilter(
            document_id=res[0],
        )
    )
    query = QueryWithEmbedding(
        query="lorem",
        top_k=9,
        embedding=sample_embedding(0),
    )
    query_results = await milvus_datastore._query(queries=[query])

    assert 1 == len(query_results)
    assert 0 == len(query_results[0].results)
    milvus_datastore.col.drop()


@pytest.mark.asyncio
async def test_delete_with_document_id(milvus_datastore, document_chunk_one):
    await milvus_datastore.delete(delete_all=True)
    res = await milvus_datastore._upsert(document_chunk_one)
    assert res == list(document_chunk_one.keys())
    milvus_datastore.col.flush()
    await milvus_datastore.delete([res[0]])

    query = QueryWithEmbedding(
        query="lorem",
        top_k=9,
        embedding=sample_embedding(0),
    )
    query_results = await milvus_datastore._query(queries=[query])

    assert 1 == len(query_results)
    assert 0 == len(query_results[0].results)
    milvus_datastore.col.drop()


# if __name__ == '__main__':
#     import sys
#     import pytest
#     pytest.main(sys.argv)



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/tests/datastore/providers/postgres/test_postgres_datastore.py
================================================
from typing import Dict, List
import pytest
from datastore.providers.postgres_datastore import PostgresDataStore
from models.models import (
    DocumentChunk,
    DocumentChunkMetadata,
    DocumentMetadataFilter,
    QueryWithEmbedding,
)


def create_embedding(non_zero_pos: int) -> List[float]:
    # create a vector with a single non-zero value of dimension 1535
    vector = [0.0] * 1536
    vector[non_zero_pos - 1] = 1.0
    return vector


@pytest.fixture
def initial_document_chunks() -> Dict[str, List[DocumentChunk]]:
    first_doc_chunks = [
        DocumentChunk(
            id=f"first-doc-{i}",
            text=f"Lorem ipsum {i}",
            metadata=DocumentChunkMetadata(),
            embedding=create_embedding(i),
        )
        for i in range(4, 7)
    ]
    return {
        "first-doc": first_doc_chunks,
    }


@pytest.fixture
def queries() -> List[QueryWithEmbedding]:
    queries = [
        QueryWithEmbedding(
            query="Query 1",
            top_k=1,
            embedding=create_embedding(4),
        ),
        QueryWithEmbedding(
            query="Query 2",
            top_k=2,
            embedding=create_embedding(5),
        ),
    ]
    return queries


@pytest.fixture
def postgres_datastore() -> PostgresDataStore:
    return PostgresDataStore()


@pytest.mark.asyncio
async def test_upsert(
    postgres_datastore: PostgresDataStore,
    initial_document_chunks: Dict[str, List[DocumentChunk]],
) -> None:
    """Test basic upsert."""
    doc_ids = await postgres_datastore._upsert(initial_document_chunks)
    assert doc_ids == [doc_id for doc_id in initial_document_chunks]


@pytest.mark.asyncio
async def test_query(
    postgres_datastore: PostgresDataStore,
    initial_document_chunks: Dict[str, List[DocumentChunk]],
    queries: List[QueryWithEmbedding],
) -> None:
    """Test basic query."""
    # insert to prepare for test
    await postgres_datastore._upsert(initial_document_chunks)

    query_results = await postgres_datastore._query(queries)
    assert len(query_results) == len(queries)

    query_0_results = query_results[0].results
    query_1_results = query_results[1].results

    assert len(query_0_results) == 1
    assert len(query_1_results) == 2

    # NOTE: this is the correct behavior
    assert query_0_results[0].id == "first-doc-4"
    assert query_1_results[0].id == "first-doc-5"
    assert query_1_results[1].id == "first-doc-4"


@pytest.mark.asyncio
async def test_delete(
    postgres_datastore: PostgresDataStore,
    initial_document_chunks: Dict[str, List[DocumentChunk]],
) -> None:
    # insert to prepare for test
    await postgres_datastore._upsert(initial_document_chunks)

    is_success = await postgres_datastore.delete(["first-doc"])
    assert is_success


@pytest.mark.asyncio
async def test_upsert_new_chunk(postgres_datastore):
    await postgres_datastore.delete(delete_all=True)
    chunk = DocumentChunk(
        id="chunk1",
        text="Sample text",
        embedding=[1] * 1536,
        metadata=DocumentChunkMetadata(),
    )
    ids = await postgres_datastore._upsert({"doc1": [chunk]})
    assert len(ids) == 1


@pytest.mark.asyncio
async def test_upsert_existing_chunk(postgres_datastore):
    await postgres_datastore.delete(delete_all=True)
    chunk = DocumentChunk(
        id="chunk1",
        text="Sample text",
        embedding=[1] * 1536,
        metadata=DocumentChunkMetadata(),
    )
    ids = await postgres_datastore._upsert({"doc1": [chunk]})

    chunk = DocumentChunk(
        id="chunk1",
        text="New text",
        embedding=[1] * 1536,
        metadata=DocumentChunkMetadata(),
    )
    ids = await postgres_datastore._upsert({"doc1": [chunk]})

    query_embedding = [1] * 1536
    query = QueryWithEmbedding(
        query="Query",
        embedding=query_embedding,
        top_k=1,
    )
    results = await postgres_datastore._query([query])

    assert len(ids) == 1
    assert len(results[0].results) == 1
    assert results[0].results[0].id == "chunk1"
    assert results[0].results[0].text == "New text"


@pytest.mark.asyncio
async def test_query_score(postgres_datastore):
    await postgres_datastore.delete(delete_all=True)
    chunk1 = DocumentChunk(
        id="chunk1",
        text="Sample text",
        embedding=[1] * 1536,
        metadata=DocumentChunkMetadata(),
    )
    chunk2 = DocumentChunk(
        id="chunk2",
        text="Another text",
        embedding=[-1 if i % 2 == 0 else 1 for i in range(1536)],
        metadata=DocumentChunkMetadata(),
    )
    await postgres_datastore._upsert({"doc1": [chunk1], "doc2": [chunk2]})

    query_embedding = [1] * 1536
    query = QueryWithEmbedding(
        query="Query",
        embedding=query_embedding,
    )
    results = await postgres_datastore._query([query])

    assert results[0].results[0].id == "chunk1"
    assert int(results[0].results[0].score) == 1536


@pytest.mark.asyncio
async def test_query_filter(postgres_datastore):
    await postgres_datastore.delete(delete_all=True)
    chunk1 = DocumentChunk(
        id="chunk1",
        text="Sample text",
        embedding=[1] * 1536,
        metadata=DocumentChunkMetadata(
            source="email", created_at="2021-01-01", author="John"
        ),
    )
    chunk2 = DocumentChunk(
        id="chunk2",
        text="Another text",
        embedding=[1] * 1536,
        metadata=DocumentChunkMetadata(
            source="chat", created_at="2022-02-02", author="Mike"
        ),
    )
    await postgres_datastore._upsert({"doc1": [chunk1], "doc2": [chunk2]})

    # Test author filter -- string
    query_embedding = [1] * 1536
    query = QueryWithEmbedding(
        query="Query",
        embedding=query_embedding,
        filter=DocumentMetadataFilter(author="John"),
    )
    results = await postgres_datastore._query([query])
    assert results[0].results[0].id == "chunk1"

    # Test source filter -- enum
    query_embedding = [1] * 1536
    query = QueryWithEmbedding(
        query="Query",
        embedding=query_embedding,
        filter=DocumentMetadataFilter(source="chat"),
    )
    results = await postgres_datastore._query([query])
    assert results[0].results[0].id == "chunk2"

    # Test created_at filter -- date
    query_embedding = [1] * 1536
    query = QueryWithEmbedding(
        query="Query",
        embedding=query_embedding,
        filter=DocumentMetadataFilter(start_date="2022-01-01"),
    )
    results = await postgres_datastore._query([query])
    assert results[0].results[0].id == "chunk2"


@pytest.mark.asyncio
async def test_delete(postgres_datastore):
    await postgres_datastore.delete(delete_all=True)
    chunk1 = DocumentChunk(
        id="chunk1",
        text="Sample text",
        embedding=[1] * 1536,
        metadata=DocumentChunkMetadata(),
    )
    chunk2 = DocumentChunk(
        id="chunk2",
        text="Another text",
        embedding=[1] * 1536,
        metadata=DocumentChunkMetadata(),
    )
    await postgres_datastore._upsert({"doc1": [chunk1], "doc2": [chunk2]})

    query_embedding = [1] * 1536
    query = QueryWithEmbedding(
        query="Another query",
        embedding=query_embedding,
    )
    results = await postgres_datastore._query([query])

    assert len(results[0].results) == 2
    assert results[0].results[0].id == "chunk1"
    assert results[0].results[1].id == "chunk2"

    await postgres_datastore.delete(ids=["doc1"])
    results_after_delete = await postgres_datastore._query([query])

    assert len(results_after_delete[0].results) == 1
    assert results_after_delete[0].results[0].id == "chunk2"


@pytest.mark.asyncio
async def test_delete_all(postgres_datastore):
    await postgres_datastore.delete(delete_all=True)
    chunk = DocumentChunk(
        id="chunk",
        text="Another text",
        embedding=[1] * 1536,
        metadata=DocumentChunkMetadata(),
    )
    await postgres_datastore._upsert({"doc": [chunk]})

    query_embedding = [1] * 1536
    query = QueryWithEmbedding(
        query="Another query",
        embedding=query_embedding,
        top_k=1,
    )
    results = await postgres_datastore._query([query])

    assert len(results) == 1
    assert len(results[0].results) == 1
    assert results[0].results[0].id == "chunk"

    await postgres_datastore.delete(delete_all=True)
    results_after_delete = await postgres_datastore._query([query])

    assert len(results_after_delete[0].results) == 0



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/tests/datastore/providers/qdrant/test_qdrant_datastore.py
================================================
from typing import Dict, List

import pytest
import qdrant_client
from qdrant_client.http.models import PayloadSchemaType

from datastore.providers.qdrant_datastore import QdrantDataStore
from models.models import (
    DocumentChunk,
    DocumentChunkMetadata,
    QueryWithEmbedding,
    DocumentMetadataFilter,
    Source,
)


def create_embedding(non_zero_pos: int, size: int) -> List[float]:
    vector = [0.0] * size
    vector[non_zero_pos % size] = 1.0
    return vector


@pytest.fixture
def qdrant_datastore() -> QdrantDataStore:
    return QdrantDataStore(
        collection_name="documents", vector_size=5, recreate_collection=True
    )


@pytest.fixture
def client() -> qdrant_client.QdrantClient:
    return qdrant_client.QdrantClient()


@pytest.fixture
def initial_document_chunks() -> Dict[str, List[DocumentChunk]]:
    first_doc_chunks = [
        DocumentChunk(
            id=f"first-doc-{i}",
            text=f"Lorem ipsum {i}",
            metadata=DocumentChunkMetadata(),
            embedding=create_embedding(i, 5),
        )
        for i in range(4, 7)
    ]
    return {
        "first-doc": first_doc_chunks,
    }


@pytest.fixture
def document_chunks() -> Dict[str, List[DocumentChunk]]:
    first_doc_chunks = [
        DocumentChunk(
            id=f"first-doc_{i}",
            text=f"Lorem ipsum {i}",
            metadata=DocumentChunkMetadata(
                source=Source.email, created_at="2023-03-05", document_id="first-doc"
            ),
            embedding=create_embedding(i, 5),
        )
        for i in range(3)
    ]
    second_doc_chunks = [
        DocumentChunk(
            id=f"second-doc_{i}",
            text=f"Dolor sit amet {i}",
            metadata=DocumentChunkMetadata(
                created_at="2023-03-04", document_id="second-doc"
            ),
            embedding=create_embedding(i + len(first_doc_chunks), 5),
        )
        for i in range(2)
    ]
    return {
        "first-doc": first_doc_chunks,
        "second-doc": second_doc_chunks,
    }


@pytest.mark.asyncio
async def test_datastore_creates_payload_indexes(
    qdrant_datastore,
    client,
):
    collection_info = client.get_collection(collection_name="documents")

    assert 2 == len(collection_info.payload_schema)
    assert "created_at" in collection_info.payload_schema
    created_at = collection_info.payload_schema["created_at"]
    assert PayloadSchemaType.INTEGER == created_at.data_type
    assert "metadata.document_id" in collection_info.payload_schema
    document_id = collection_info.payload_schema["metadata.document_id"]
    assert PayloadSchemaType.KEYWORD == document_id.data_type


@pytest.mark.asyncio
async def test_upsert_creates_all_points(
    qdrant_datastore,
    client,
    document_chunks,
):
    document_ids = await qdrant_datastore._upsert(document_chunks)

    assert 2 == len(document_ids)
    assert 5 == client.count(collection_name="documents").count


@pytest.mark.asyncio
async def test_upsert_does_not_remove_existing_documents_but_store_new(
    qdrant_datastore,
    client,
    initial_document_chunks,
    document_chunks,
):
    """
    This test ensures calling ._upsert no longer removes the existing document chunks,
    as they are currently removed in the .upsert method directly.
    """
    # Fill the database with document chunks before running the actual test
    await qdrant_datastore._upsert(initial_document_chunks)

    await qdrant_datastore._upsert(document_chunks)

    assert 8 == client.count(collection_name="documents").count


@pytest.mark.asyncio
async def test_query_returns_all_on_single_query(qdrant_datastore, document_chunks):
    # Fill the database with document chunks before running the actual test
    await qdrant_datastore._upsert(document_chunks)

    query = QueryWithEmbedding(
        query="lorem",
        top_k=5,
        embedding=[0.5, 0.5, 0.5, 0.5, 0.5],
    )
    query_results = await qdrant_datastore._query(queries=[query])

    assert 1 == len(query_results)
    assert "lorem" == query_results[0].query
    assert 5 == len(query_results[0].results)


@pytest.mark.asyncio
async def test_query_returns_closest_entry(qdrant_datastore, document_chunks):
    # Fill the database with document chunks before running the actual test
    await qdrant_datastore._upsert(document_chunks)

    query = QueryWithEmbedding(
        query="ipsum",
        top_k=1,
        embedding=[0.0, 0.0, 0.5, 0.0, 0.0],
    )
    query_results = await qdrant_datastore._query(queries=[query])

    assert 1 == len(query_results)
    assert "ipsum" == query_results[0].query
    assert 1 == len(query_results[0].results)
    first_document_chunk = query_results[0].results[0]
    assert 0.0 <= first_document_chunk.score <= 1.0
    assert Source.email == first_document_chunk.metadata.source
    assert "2023-03-05" == first_document_chunk.metadata.created_at
    assert "first-doc" == first_document_chunk.metadata.document_id


@pytest.mark.asyncio
async def test_query_filter_by_document_id_returns_this_document_chunks(
    qdrant_datastore, document_chunks
):
    # Fill the database with document chunks before running the actual test
    await qdrant_datastore._upsert(document_chunks)

    first_query = QueryWithEmbedding(
        query="dolor",
        filter=DocumentMetadataFilter(document_id="first-doc"),
        top_k=5,
        embedding=[0.0, 0.0, 0.5, 0.0, 0.0],
    )
    second_query = QueryWithEmbedding(
        query="dolor",
        filter=DocumentMetadataFilter(document_id="second-doc"),
        top_k=5,
        embedding=[0.0, 0.0, 0.5, 0.0, 0.0],
    )
    query_results = await qdrant_datastore._query(queries=[first_query, second_query])

    assert 2 == len(query_results)
    assert "dolor" == query_results[0].query
    assert "dolor" == query_results[1].query
    assert 3 == len(query_results[0].results)
    assert 2 == len(query_results[1].results)


@pytest.mark.asyncio
@pytest.mark.parametrize("start_date", ["2023-03-05T00:00:00", "2023-03-05"])
async def test_query_start_date_converts_datestring(
    qdrant_datastore,
    document_chunks,
    start_date,
):
    # Fill the database with document chunks before running the actual test
    await qdrant_datastore._upsert(document_chunks)

    query = QueryWithEmbedding(
        query="sit amet",
        filter=DocumentMetadataFilter(start_date=start_date),
        top_k=5,
        embedding=[0.0, 0.0, 0.5, 0.0, 0.0],
    )
    query_results = await qdrant_datastore._query(queries=[query])

    assert 1 == len(query_results)
    assert 3 == len(query_results[0].results)


@pytest.mark.asyncio
@pytest.mark.parametrize("end_date", ["2023-03-04T00:00:00", "2023-03-04"])
async def test_query_end_date_converts_datestring(
    qdrant_datastore,
    document_chunks,
    end_date,
):
    # Fill the database with document chunks before running the actual test
    await qdrant_datastore._upsert(document_chunks)

    query = QueryWithEmbedding(
        query="sit amet",
        filter=DocumentMetadataFilter(end_date=end_date),
        top_k=5,
        embedding=[0.0, 0.0, 0.5, 0.0, 0.0],
    )
    query_results = await qdrant_datastore._query(queries=[query])

    assert 1 == len(query_results)
    assert 2 == len(query_results[0].results)


@pytest.mark.asyncio
async def test_delete_removes_by_ids(
    qdrant_datastore,
    client,
    document_chunks,
):
    # Fill the database with document chunks before running the actual test
    await qdrant_datastore._upsert(document_chunks)

    await qdrant_datastore.delete(ids=["first-doc"])

    assert 2 == client.count(collection_name="documents").count


@pytest.mark.asyncio
async def test_delete_removes_by_document_id_filter(
    qdrant_datastore,
    client,
    document_chunks,
):
    # Fill the database with document chunks before running the actual test
    await qdrant_datastore._upsert(document_chunks)

    await qdrant_datastore.delete(
        filter=DocumentMetadataFilter(document_id="first-doc")
    )

    assert 2 == client.count(collection_name="documents").count


@pytest.mark.asyncio
async def test_delete_removes_all(
    qdrant_datastore,
    client,
    document_chunks,
):
    # Fill the database with document chunks before running the actual test
    await qdrant_datastore._upsert(document_chunks)

    await qdrant_datastore.delete(delete_all=True)

    assert 0 == client.count(collection_name="documents").count



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/tests/datastore/providers/redis/test_redis_datastore.py
================================================
from datastore.providers.redis_datastore import RedisDataStore
from models.models import DocumentChunk, DocumentChunkMetadata, QueryWithEmbedding, Source, DocumentMetadataFilter
import pytest
import redis.asyncio as redis
import numpy as np

NUM_TEST_DOCS = 10

@pytest.fixture
async def redis_datastore():
    return await RedisDataStore.init(dim=5)

def create_embedding(i, dim):
    vec = np.array([0.1] * dim).astype(np.float64).tolist()
    vec[dim-1] = i+1/10
    return vec

def create_document_chunk(i, dim):
    return DocumentChunk(
        id=f"first-doc_{i}",
        text=f"Lorem ipsum {i}",
        embedding=create_embedding(i, dim),
        metadata=DocumentChunkMetadata(
            source=Source.file, created_at="1970-01-01", document_id="docs"
        ),
    )

def create_document_chunks(n, dim):
    docs =  [create_document_chunk(i, dim) for i in range(n)]
    return {"docs": docs}

@pytest.mark.asyncio
async def test_redis_upsert_query(redis_datastore):
    docs = create_document_chunks(NUM_TEST_DOCS, 5)
    await redis_datastore._upsert(docs)
    query = QueryWithEmbedding(
        query="Lorem ipsum 0",
        top_k=5,
        embedding= create_embedding(0, 5),
    )
    query_results = await redis_datastore._query(queries=[query])
    assert 1 == len(query_results)
    for i in range(5):
        assert f"Lorem ipsum {i}" == query_results[0].results[i].text
        assert "docs" == query_results[0].results[i].id

@pytest.mark.asyncio
async def test_redis_filter_query(redis_datastore):
    query = QueryWithEmbedding(
        query="Lorem ipsum 0",
        filter=DocumentMetadataFilter(document_id="docs"),
        top_k=5,
        embedding= create_embedding(0, 5),
    )
    query_results = await redis_datastore._query(queries=[query])
    print(query_results)
    assert 1 == len(query_results)
    assert "docs" == query_results[0].results[0].id


@pytest.mark.asyncio
async def test_redis_delete_docs(redis_datastore):
    res = await redis_datastore.delete(ids=["docs"])
    assert res



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/tests/datastore/providers/supabase/test_supabase_datastore.py
================================================
from typing import Dict, List
import pytest
from datastore.providers.supabase_datastore import SupabaseDataStore
from models.models import (
    DocumentChunk,
    DocumentChunkMetadata,
    DocumentMetadataFilter,
    QueryWithEmbedding,
)


def create_embedding(non_zero_pos: int) -> List[float]:
    # create a vector with a single non-zero value of dimension 1535
    vector = [0.0] * 1536
    vector[non_zero_pos - 1] = 1.0
    return vector


@pytest.fixture
def initial_document_chunks() -> Dict[str, List[DocumentChunk]]:
    first_doc_chunks = [
        DocumentChunk(
            id=f"first-doc-{i}",
            text=f"Lorem ipsum {i}",
            metadata=DocumentChunkMetadata(),
            embedding=create_embedding(i),
        )
        for i in range(4, 7)
    ]
    return {
        "first-doc": first_doc_chunks,
    }


@pytest.fixture
def queries() -> List[QueryWithEmbedding]:
    queries = [
        QueryWithEmbedding(
            query="Query 1",
            top_k=1,
            embedding=create_embedding(4),
        ),
        QueryWithEmbedding(
            query="Query 2",
            top_k=2,
            embedding=create_embedding(5),
        ),
    ]
    return queries


@pytest.fixture
def supabase_datastore() -> SupabaseDataStore:
    return SupabaseDataStore()


@pytest.mark.asyncio
async def test_upsert(
    supabase_datastore: SupabaseDataStore,
    initial_document_chunks: Dict[str, List[DocumentChunk]],
) -> None:
    """Test basic upsert."""
    doc_ids = await supabase_datastore._upsert(initial_document_chunks)
    assert doc_ids == [doc_id for doc_id in initial_document_chunks]


@pytest.mark.asyncio
async def test_query(
    supabase_datastore: SupabaseDataStore,
    initial_document_chunks: Dict[str, List[DocumentChunk]],
    queries: List[QueryWithEmbedding],
) -> None:
    """Test basic query."""
    # insert to prepare for test
    await supabase_datastore._upsert(initial_document_chunks)

    query_results = await supabase_datastore._query(queries)
    assert len(query_results) == len(queries)

    query_0_results = query_results[0].results
    query_1_results = query_results[1].results

    assert len(query_0_results) == 1
    assert len(query_1_results) == 2

    # NOTE: this is the correct behavior
    assert query_0_results[0].id == "first-doc-4"
    assert query_1_results[0].id == "first-doc-5"
    assert query_1_results[1].id == "first-doc-4"


@pytest.mark.asyncio
async def test_delete(
    supabase_datastore: SupabaseDataStore,
    initial_document_chunks: Dict[str, List[DocumentChunk]],
) -> None:
    # insert to prepare for test
    await supabase_datastore._upsert(initial_document_chunks)

    is_success = await supabase_datastore.delete(["first-doc"])
    assert is_success


@pytest.mark.asyncio
async def test_upsert_new_chunk(supabase_datastore):
    await supabase_datastore.delete(delete_all=True)
    chunk = DocumentChunk(
        id="chunk1",
        text="Sample text",
        embedding=[1] * 1536,
        metadata=DocumentChunkMetadata(),
    )
    ids = await supabase_datastore._upsert({"doc1": [chunk]})
    assert len(ids) == 1


@pytest.mark.asyncio
async def test_upsert_existing_chunk(supabase_datastore):
    await supabase_datastore.delete(delete_all=True)
    chunk = DocumentChunk(
        id="chunk1",
        text="Sample text",
        embedding=[1] * 1536,
        metadata=DocumentChunkMetadata(),
    )
    ids = await supabase_datastore._upsert({"doc1": [chunk]})

    chunk = DocumentChunk(
        id="chunk1",
        text="New text",
        embedding=[1] * 1536,
        metadata=DocumentChunkMetadata(),
    )
    ids = await supabase_datastore._upsert({"doc1": [chunk]})

    query_embedding = [1] * 1536
    query = QueryWithEmbedding(
        query="Query",
        embedding=query_embedding,
        top_k=1,
    )
    results = await supabase_datastore._query([query])

    assert len(ids) == 1
    assert len(results[0].results) == 1
    assert results[0].results[0].id == "chunk1"
    assert results[0].results[0].text == "New text"


@pytest.mark.asyncio
async def test_query_score(supabase_datastore):
    await supabase_datastore.delete(delete_all=True)
    chunk1 = DocumentChunk(
        id="chunk1",
        text="Sample text",
        embedding=[1] * 1536,
        metadata=DocumentChunkMetadata(),
    )
    chunk2 = DocumentChunk(
        id="chunk2",
        text="Another text",
        embedding=[-1 if i % 2 == 0 else 1 for i in range(1536)],
        metadata=DocumentChunkMetadata(),
    )
    await supabase_datastore._upsert({"doc1": [chunk1], "doc2": [chunk2]})

    query_embedding = [1] * 1536
    query = QueryWithEmbedding(
        query="Query",
        embedding=query_embedding,
    )
    results = await supabase_datastore._query([query])

    assert results[0].results[0].id == "chunk1"
    assert int(results[0].results[0].score) == 1536


@pytest.mark.asyncio
async def test_query_filter(supabase_datastore):
    await supabase_datastore.delete(delete_all=True)
    chunk1 = DocumentChunk(
        id="chunk1",
        text="Sample text",
        embedding=[1] * 1536,
        metadata=DocumentChunkMetadata(
            source="email", created_at="2021-01-01", author="John"
        ),
    )
    chunk2 = DocumentChunk(
        id="chunk2",
        text="Another text",
        embedding=[1] * 1536,
        metadata=DocumentChunkMetadata(
            source="chat", created_at="2022-02-02", author="Mike"
        ),
    )
    await supabase_datastore._upsert({"doc1": [chunk1], "doc2": [chunk2]})

    # Test author filter -- string
    query_embedding = [1] * 1536
    query = QueryWithEmbedding(
        query="Query",
        embedding=query_embedding,
        filter=DocumentMetadataFilter(author="John"),
    )
    results = await supabase_datastore._query([query])
    assert results[0].results[0].id == "chunk1"

    # Test source filter -- enum
    query_embedding = [1] * 1536
    query = QueryWithEmbedding(
        query="Query",
        embedding=query_embedding,
        filter=DocumentMetadataFilter(source="chat"),
    )
    results = await supabase_datastore._query([query])
    assert results[0].results[0].id == "chunk2"

    # Test created_at filter -- date
    query_embedding = [1] * 1536
    query = QueryWithEmbedding(
        query="Query",
        embedding=query_embedding,
        filter=DocumentMetadataFilter(start_date="2022-01-01"),
    )
    results = await supabase_datastore._query([query])
    assert results[0].results[0].id == "chunk2"


@pytest.mark.asyncio
async def test_delete(supabase_datastore):
    await supabase_datastore.delete(delete_all=True)
    chunk1 = DocumentChunk(
        id="chunk1",
        text="Sample text",
        embedding=[1] * 1536,
        metadata=DocumentChunkMetadata(),
    )
    chunk2 = DocumentChunk(
        id="chunk2",
        text="Another text",
        embedding=[1] * 1536,
        metadata=DocumentChunkMetadata(),
    )
    await supabase_datastore._upsert({"doc1": [chunk1], "doc2": [chunk2]})

    query_embedding = [1] * 1536
    query = QueryWithEmbedding(
        query="Another query",
        embedding=query_embedding,
    )
    results = await supabase_datastore._query([query])

    assert len(results[0].results) == 2
    assert results[0].results[0].id == "chunk1"
    assert results[0].results[1].id == "chunk2"

    await supabase_datastore.delete(ids=["doc1"])
    results_after_delete = await supabase_datastore._query([query])

    assert len(results_after_delete[0].results) == 1
    assert results_after_delete[0].results[0].id == "chunk2"


@pytest.mark.asyncio
async def test_delete_all(supabase_datastore):
    await supabase_datastore.delete(delete_all=True)
    chunk = DocumentChunk(
        id="chunk",
        text="Another text",
        embedding=[1] * 1536,
        metadata=DocumentChunkMetadata(),
    )
    await supabase_datastore._upsert({"doc": [chunk]})

    query_embedding = [1] * 1536
    query = QueryWithEmbedding(
        query="Another query",
        embedding=query_embedding,
        top_k=1,
    )
    results = await supabase_datastore._query([query])

    assert len(results) == 1
    assert len(results[0].results) == 1
    assert results[0].results[0].id == "chunk"

    await supabase_datastore.delete(delete_all=True)
    results_after_delete = await supabase_datastore._query([query])

    assert len(results_after_delete[0].results) == 0



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/tests/datastore/providers/weaviate/docker-compose.yml
================================================
---
version: '3.4'
services:
  weaviate:
    command:
    - --host
    - 0.0.0.0
    - --port
    - '8080'
    - --scheme
    - http
    image: semitechnologies/weaviate:1.18.0
    ports:
    - 8080:8080
    restart: on-failure:0
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DEFAULT_VECTORIZER_MODULE: 'none'
      ENABLE_MODULES: ''
      CLUSTER_HOSTNAME: 'node1'
      LOG_LEVEL: debug
      AUTOSCHEMA_ENABLED: 'false'
...


================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/tests/datastore/providers/weaviate/test_weaviate_datastore.py
================================================
import logging
import os

import pytest
import weaviate
from _pytest.logging import LogCaptureFixture
from fastapi.testclient import TestClient
from loguru import logger
from weaviate import Client

from datastore.providers.weaviate_datastore import (
    SCHEMA,
    WeaviateDataStore,
    extract_schema_properties,
)
from models.models import DocumentMetadataFilter, Source
from server.main import app

BEARER_TOKEN = os.getenv("BEARER_TOKEN")

client = TestClient(app)
client.headers["Authorization"] = f"Bearer {BEARER_TOKEN}"


@pytest.fixture
def weaviate_client():
    host = os.getenv("WEAVIATE_HOST", "http://localhost")
    port = os.getenv("WEAVIATE_PORT", "8080")
    client = Client(f"{host}:{port}")

    yield client

    client.schema.delete_all()


@pytest.fixture
def test_db(weaviate_client, documents):
    weaviate_client.schema.delete_all()
    weaviate_client.schema.create_class(SCHEMA)

    response = client.post("/upsert", json={"documents": documents})

    if response.status_code != 200:
        raise Exception(
            f"Could not upsert to test client.\nStatus Code: {response.status_code}\nResponse:\n{response.json()}"
        )

    yield client


@pytest.fixture
def documents():
    documents = []

    authors = ["Max Mustermann", "John Doe", "Jane Doe"]
    texts = [
        "lorem ipsum dolor sit amet",
        "consectetur adipiscing elit",
        "sed do eiusmod tempor incididunt",
    ]
    ids = ["abc_123", "def_456", "ghi_789"]
    sources = ["chat", "email", "email"]
    created_at = [
        "1929-10-28T09:30:00-05:00",
        "2009-01-03T16:39:57-08:00",
        "2021-01-21T10:00:00-02:00",
    ]

    for i in range(3):
        documents.append(
            {
                "id": ids[i],
                "text": texts[i],
                "metadata": {
                    "source": sources[i],
                    "source_id": "5325",
                    "url": "http://example.com",
                    "created_at": created_at[i],
                    "author": authors[i],
                },
            }
        )

    no_metadata_doc = {
        "id": "jkl_012",
        "text": "no metadata",
    }

    documents.append(no_metadata_doc)

    partial_metadata_doc = {
        "id": "mno_345",
        "text": "partial metadata",
        "metadata": {
            "source": "file",
        },
    }

    documents.append(partial_metadata_doc)

    yield documents


@pytest.fixture
def caplog(caplog: LogCaptureFixture):
    handler_id = logger.add(caplog.handler, format="{message}")
    yield caplog
    logger.remove(handler_id)


@pytest.mark.parametrize(
    "document_id", [("abc_123"), ("9a253e0b-d2df-5c2e-be6d-8e9b1f4ae345")]
)
def test_upsert(weaviate_client, document_id):
    weaviate_client.schema.delete_all()
    weaviate_client.schema.create_class(SCHEMA)

    text = """
    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fusce in ipsum eget dolor malesuada fermentum at ac massa. 
    Aliquam erat volutpat. Sed eu velit est. Morbi semper quam id urna fringilla lacinia. Vivamus sit amet velit id lorem 
    pretium molestie. Nulla tincidunt sapien eu nulla consequat, a lacinia justo facilisis. Maecenas euismod urna sapien, 
    sit amet tincidunt est dapibus ac. Sed in lorem in nunc tincidunt bibendum. Nullam vel urna vitae nulla iaculis rutrum. 
    Suspendisse varius, massa a dignissim vehicula, urna ligula tincidunt orci, id fringilla velit tellus eu metus. Sed 
    vestibulum, nisl in malesuada tempor, nisi turpis facilisis nibh, nec dictum velit velit vel ex. Donec euismod, 
    leo ut sollicitudin tempor, dolor augue blandit nunc, eu lacinia ipsum turpis vitae nulla. Aenean bibendum 
    tincidunt magna in pulvinar. Sed tincidunt vel nisi ac maximus.
    """
    source = "email"
    source_id = "5325"
    url = "http://example.com"
    created_at = "2022-12-16T08:00:00+01:00"
    author = "Max Mustermann"

    documents = {
        "documents": [
            {
                "id": document_id,
                "text": text,
                "metadata": {
                    "source": source,
                    "source_id": source_id,
                    "url": url,
                    "created_at": created_at,
                    "author": author,
                },
            }
        ]
    }

    response = client.post("/upsert", json=documents)

    assert response.status_code == 200
    assert response.json() == {"ids": [document_id]}

    properties = [
        "chunk_id",
        "document_id",
        "source",
        "source_id",
        "url",
        "created_at",
        "author",
    ]

    where_filter = {
        "path": ["document_id"],
        "operator": "Equal",
        "valueString": document_id,
    }

    weaviate_doc = (
        weaviate_client.query.get("OpenAIDocument", properties)
        .with_additional("vector")
        .with_where(where_filter)
        .with_sort({"path": ["chunk_id"], "order": "asc"})
        .do()
    )

    weaviate_docs = weaviate_doc["data"]["Get"]["OpenAIDocument"]

    assert len(weaviate_docs) == 2

    for i, weaviate_doc in enumerate(weaviate_docs):
        assert weaviate_doc["chunk_id"] == f"{document_id}_{i}"

        assert weaviate_doc["document_id"] == document_id

        assert weaviate_doc["source"] == source
        assert weaviate_doc["source_id"] == source_id
        assert weaviate_doc["url"] == url
        assert weaviate_doc["created_at"] == created_at
        assert weaviate_doc["author"] == author

        assert weaviate_doc["_additional"]["vector"]


def test_upsert_no_metadata(weaviate_client):
    weaviate_client.schema.delete_all()
    weaviate_client.schema.create_class(SCHEMA)

    no_metadata_doc = {
        "id": "jkl_012",
        "text": "no metadata",
    }

    metadata_properties = [
        "source",
        "source_id",
        "url",
        "created_at",
        "author",
    ]

    response = client.post("/upsert", json={"documents": [no_metadata_doc]})

    assert response.status_code == 200

    weaviate_doc = weaviate_client.query.get("OpenAIDocument", metadata_properties).do()

    weaviate_doc = weaviate_doc["data"]["Get"]["OpenAIDocument"][0]

    for _, metadata_value in weaviate_doc.items():
        assert metadata_value is None


@pytest.mark.parametrize(
    "test_document, expected_status_code",
    [
        ({"id": "abc_123", "text": "some text"}, 200),
        ({"id": "abc_123"}, 422),
        ({"text": "some text"}, 200),
    ],
)
def test_upsert_invalid_documents(weaviate_client, test_document, expected_status_code):
    weaviate_client.schema.delete_all()
    weaviate_client.schema.create_class(SCHEMA)

    response = client.post("/upsert", json={"documents": [test_document]})

    assert response.status_code == expected_status_code


@pytest.mark.parametrize(
    "query, expected_num_results",
    [
        ({"query": "consectetur adipiscing", "top_k": 3}, 3),
        ({"query": "consectetur adipiscing elit", "filter": {"source": "email"}}, 2),
        (
            {
                "query": "sed do eiusmod tempor",
                "filter": {
                    "start_date": "2020-01-01T00:00:00Z",
                    "end_date": "2022-12-31T00:00:00Z",
                },
            },
            1,
        ),
        (
            {
                "query": "some random query",
                "filter": {"start_date": "2009-01-01T00:00:00Z"},
                "top_k": 3,
            },
            2,
        ),
        (
            {
                "query": "another random query",
                "filter": {"end_date": "1929-12-31T00:00:00Z"},
                "top_k": 3,
            },
            1,
        ),
    ],
)
def test_query(test_db, query, expected_num_results):
    queries = {"queries": [query]}

    response = client.post("/query", json=queries)
    assert response.status_code == 200

    num_docs = response.json()["results"][0]["results"]
    assert len(num_docs) == expected_num_results


def test_delete(test_db, weaviate_client, caplog):
    caplog.set_level(logging.DEBUG)

    delete_request = {"ids": ["def_456"]}

    response = client.request(method="delete", url="/delete", json=delete_request)
    assert response.status_code == 200
    assert response.json()["success"]
    assert weaviate_client.data_object.get()["totalResults"] == 4

    client.request(method="delete", url="/delete", json=delete_request)
    assert "Failed to delete" in caplog.text
    caplog.clear()

    delete_request = {"filter": {"source": "email"}}

    response = client.request(method="delete", url="/delete", json=delete_request)
    assert response.status_code == 200
    assert response.json()["success"]
    assert weaviate_client.data_object.get()["totalResults"] == 3

    client.request(method="delete", url="/delete", json=delete_request)
    assert "Failed to delete" in caplog.text

    delete_request = {"delete_all": True}

    response = client.request(method="delete", url="/delete", json=delete_request)
    assert response.status_code == 200
    assert response.json()["success"]
    assert not weaviate_client.data_object.get()["objects"]


def test_build_auth_credentials(monkeypatch):
    # Test when WEAVIATE_URL ends with weaviate.network and WEAVIATE_API_KEY is set
    with monkeypatch.context() as m:
        m.setenv("WEAVIATE_URL", "https://example.weaviate.network")
        m.setenv("WEAVIATE_API_KEY", "your_api_key")
        auth_credentials = WeaviateDataStore._build_auth_credentials()
        assert auth_credentials is not None
        assert isinstance(auth_credentials, weaviate.auth.AuthApiKey)
        assert auth_credentials.api_key == "your_api_key"

    # Test when WEAVIATE_URL ends with weaviate.network and WEAVIATE_API_KEY is not set
    with monkeypatch.context() as m:
        m.setenv("WEAVIATE_URL", "https://example.weaviate.network")
        m.delenv("WEAVIATE_API_KEY", raising=False)
        with pytest.raises(
            ValueError, match="WEAVIATE_API_KEY environment variable is not set"
        ):
            WeaviateDataStore._build_auth_credentials()

    # Test when WEAVIATE_URL does not end with weaviate.network
    with monkeypatch.context() as m:
        m.setenv("WEAVIATE_URL", "https://example.notweaviate.network")
        m.setenv("WEAVIATE_API_KEY", "your_api_key")
        auth_credentials = WeaviateDataStore._build_auth_credentials()
        assert auth_credentials is None

    # Test when WEAVIATE_URL is not set
    with monkeypatch.context() as m:
        m.delenv("WEAVIATE_URL", raising=False)
        m.setenv("WEAVIATE_API_KEY", "your_api_key")
        auth_credentials = WeaviateDataStore._build_auth_credentials()
        assert auth_credentials is None


def test_extract_schema_properties():
    class_schema = {
        "class": "Question",
        "description": "Information from a Jeopardy! question",
        "properties": [
            {
                "dataType": ["text"],
                "description": "The question",
                "name": "question",
            },
            {
                "dataType": ["text"],
                "description": "The answer",
                "name": "answer",
            },
            {
                "dataType": ["text"],
                "description": "The category",
                "name": "category",
            },
        ],
        "vectorizer": "text2vec-openai",
    }
    results = extract_schema_properties(class_schema)
    assert results == {"question", "answer", "category"}


def test_reuse_schema(weaviate_client, caplog):
    caplog.set_level(logging.DEBUG)

    weaviate_client.schema.delete_all()

    WeaviateDataStore()
    assert "Creating index" in caplog.text

    WeaviateDataStore()
    assert "Will reuse this schema" in caplog.text


def test_build_date_filters():
    filter = DocumentMetadataFilter(
        document_id=None,
        source=None,
        source_id=None,
        author=None,
        start_date="2020-01-01T00:00:00Z",
        end_date="2022-12-31T00:00:00Z",
    )
    actual_result = WeaviateDataStore.build_filters(filter)
    expected_result = {
        "operator": "And",
        "operands": [
            {
                "path": ["created_at"],
                "operator": "GreaterThanEqual",
                "valueDate": "2020-01-01T00:00:00Z",
            },
            {
                "path": ["created_at"],
                "operator": "LessThanEqual",
                "valueDate": "2022-12-31T00:00:00Z",
            },
        ],
    }

    assert actual_result == expected_result


@pytest.mark.parametrize(
    "test_input, expected_result",
    [
        ("abc_123", False),
        ("b2e4133c-c956-5684-bbf5-584e50ec3647", True),  # version 5
        ("f6179953-11d8-4ee0-9af8-e51e00dbf727", True),  # version 4
        ("16fe8165-3c08-348f-a015-a8bb31e26b5c", True),  # version 3
        ("bda85f97-be72-11ed-9291-00000000000a", False),  # version 1
    ],
)
def test_is_valid_weaviate_id(test_input, expected_result):
    actual_result = WeaviateDataStore._is_valid_weaviate_id(test_input)
    assert actual_result == expected_result


def test_upsert_same_docid(test_db, weaviate_client):
    def get_doc_by_document_id(document_id):
        properties = [
            "chunk_id",
            "document_id",
            "source",
            "source_id",
            "url",
            "created_at",
            "author",
        ]
        where_filter = {
            "path": ["document_id"],
            "operator": "Equal",
            "valueString": document_id,
        }

        results = (
            weaviate_client.query.get("OpenAIDocument", properties)
            .with_additional("id")
            .with_where(where_filter)
            .with_sort({"path": ["chunk_id"], "order": "asc"})
            .do()
        )

        return results["data"]["Get"]["OpenAIDocument"]

    def build_upsert_payload(document):
        return {"documents": [document]}

    # upsert a new document
    # this is a document that has 2 chunks and
    # the source is email
    doc_id = "abc_123"
    text = """
    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fusce in ipsum eget dolor malesuada fermentum at ac massa. 
    Aliquam erat volutpat. Sed eu velit est. Morbi semper quam id urna fringilla lacinia. Vivamus sit amet velit id lorem 
    pretium molestie. Nulla tincidunt sapien eu nulla consequat, a lacinia justo facilisis. Maecenas euismod urna sapien, 
    sit amet tincidunt est dapibus ac. Sed in lorem in nunc tincidunt bibendum. Nullam vel urna vitae nulla iaculis rutrum. 
    Suspendisse varius, massa a dignissim vehicula, urna ligula tincidunt orci, id fringilla velit tellus eu metus. Sed 
    vestibulum, nisl in malesuada tempor, nisi turpis facilisis nibh, nec dictum velit velit vel ex. Donec euismod, 
    leo ut sollicitudin tempor, dolor augue blandit nunc, eu lacinia ipsum turpis vitae nulla. Aenean bibendum 
    tincidunt magna in pulvinar. Sed tincidunt vel nisi ac maximus.
    """

    document = {
        "id": doc_id,
        "text": text,
        "metadata": {"source": Source.email},
    }

    response = client.post("/upsert", json=build_upsert_payload(document))
    assert response.status_code == 200

    weaviate_doc = get_doc_by_document_id(doc_id)
    assert len(weaviate_doc) == 2
    for chunk in weaviate_doc:
        assert chunk["source"] == Source.email

    # now update the source to file
    # user still has to specify the text
    # because test is a required field
    document["metadata"]["source"] = Source.file
    response = client.post("/upsert", json=build_upsert_payload(document))
    assert response.status_code == 200

    weaviate_doc = get_doc_by_document_id(doc_id)
    assert len(weaviate_doc) == 2
    for chunk in weaviate_doc:
        assert chunk["source"] == "file"

    # now update the text so that it is only 1 chunk
    # user does not need to specify metadata
    # since it is optional
    document["text"] = "This is a short text"
    document.pop("metadata")

    response = client.post("/upsert", json=build_upsert_payload(document))
    assert response.status_code == 200
    weaviate_doc = get_doc_by_document_id(doc_id)
    assert len(weaviate_doc) == 1

    # TODO: Implement update function
    # but the source should still be file
    # but it is None right now because an
    # update function is out of scope
    assert weaviate_doc[0]["source"] is None


@pytest.mark.parametrize(
    "url, expected_result",
    [
        ("https://example.weaviate.network", True),
        ("https://example.weaviate.network/", True),
        ("https://example.weaviate.cloud", True),
        ("https://example.weaviate.cloud/", True),
        ("https://example.notweaviate.network", False),
        ("https://weaviate.network.example.com", False),
        ("https://example.weaviate.network/somepage", False),
        ("", False),
    ],
)
def test_is_wcs_domain(url, expected_result):
    assert WeaviateDataStore._is_wcs_domain(url) == expected_result



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/tests/datastore/providers/zilliz/test_zilliz_datastore.py
================================================
# from pathlib import Path
# from dotenv import find_dotenv, load_dotenv
# env_path = Path(".") / "zilliz.env"
# load_dotenv(dotenv_path=env_path, verbose=True)

import pytest

from datastore.providers.zilliz_datastore import (
    ZillizDataStore,
)

from datastore.providers.milvus_datastore import (
    EMBEDDING_FIELD,
)

# Note: Only do basic test here, the ZillizDataStore is derived from MilvusDataStore.

@pytest.fixture
def zilliz_datastore():
    return ZillizDataStore()


@pytest.mark.asyncio
async def test_zilliz(zilliz_datastore):
    assert True == zilliz_datastore.col.has_index()
    index_list = [x.to_dict() for x in zilliz_datastore.col.indexes]
    for index in index_list:
        if index['index_name'] == EMBEDDING_FIELD:
            assert 'AUTOINDEX' == index['index_param']['index_type']


================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/.github/pull_request_template.md
================================================
## Pull Request (PR) Checklist
If you'd like to contribute, please follow the checklist below when submitting a PR. This will help us review and merge your changes faster! Thank you for contributing!

1. **Type of PR**: Indicate the type of PR by adding a label in square brackets at the beginning of the title, such as `[Bugfix]`, `[Feature]`, `[Enhancement]`, `[Refactor]`, or `[Documentation]`.

2. **Short Description**: Provide a brief, informative description of the PR that explains the changes made.

3. **Issue(s) Linked**: Mention any related issue(s) by using the keyword `Fixes` or `Closes` followed by the respective issue number(s) (e.g., Fixes #123, Closes #456).

4. **Branch**: Ensure that you have created a new branch for the changes, and it is based on the latest version of the `main` branch.

5. **Code Changes**: Make sure the code changes are minimal, focused, and relevant to the issue or feature being addressed.

6. **Commit Messages**: Write clear and concise commit messages that explain the purpose of each commit.

7. **Tests**: Include unit tests and/or integration tests for any new code or changes to existing code. Make sure all tests pass before submitting the PR.

8. **Documentation**: Update relevant documentation (e.g., README, inline comments, or external documentation) to reflect any changes made.

9. **Review Requested**: Request a review from at least one other contributor or maintainer of the repository.

10. **Video Submission** (For Complex/Large PRs): If your PR introduces significant changes, complexities, or a large number of lines of code, submit a brief video walkthrough along with the PR. The video should explain the purpose of the changes, the logic behind them, and how they address the issue or add the proposed feature. This will help reviewers to better understand your contribution and expedite the review process.

## Pull Request Naming Convention

Use the following naming convention for your PR branches:

```
<type>/<short-description>-<issue-number>
```

- `<type>`: The type of PR, such as `bugfix`, `feature`, `enhancement`, `refactor`, or `docs`. Multiple types are ok and should appear as <type>, <type2>
- `<short-description>`: A brief description of the changes made, using hyphens to separate words.
- `<issue-number>`: The issue number associated with the changes made (if applicable).

Example:

```
feature/advanced-chunking-strategy-123
```


================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/.well-known/ai-plugin.json
================================================
{
  "schema_version": "v1",
  "name_for_model": "retrieval",
  "name_for_human": "Retrieval Plugin",
  "description_for_model": "Plugin for searching through the user's documents (such as files, emails, and more) to find answers to questions and retrieve relevant information. Use it whenever a user asks something that might be found in their personal information.",
  "description_for_human": "Search through your documents.",
  "auth": {
    "type": "user_http",
    "authorization_type": "bearer"
  },
  "api": {
    "type": "openapi",
    "url": "https://your-app-url.com/.well-known/openapi.yaml",
    "has_user_authentication": false
  },
  "logo_url": "https://your-app-url.com/.well-known/logo.png",
  "contact_email": "hello@contact.com", 
  "legal_info_url": "http://example.com/legal-info"
}



================================================
File: vectordb/gpt-embeddings/chatgpt-retrieval-plugin/.well-known/openapi.yaml
================================================
openapi: 3.0.2
info:
  title: Retrieval Plugin API
  description: A retrieval API for querying and filtering documents based on natural language queries and metadata
  version: 1.0.0
  servers:
    - url: https://your-app-url.com
paths:
  /query:
    post:
      summary: Query
      description: Accepts search query objects array each with query and optional filter. Break down complex questions into sub-questions. Refine results by criteria, e.g. time / source, don't do this often. Split queries if ResponseTooLargeError occurs.
      operationId: query_query_post
      requestBody:
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/QueryRequest"
        required: true
      responses:
        "200":
          description: Successful Response
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/QueryResponse"
        "422":
          description: Validation Error
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/HTTPValidationError"
      security:
        - HTTPBearer: []
components:
  schemas:
    DocumentChunkMetadata:
      title: DocumentChunkMetadata
      type: object
      properties:
        source:
          $ref: "#/components/schemas/Source"
        source_id:
          title: Source Id
          type: string
        url:
          title: Url
          type: string
        created_at:
          title: Created At
          type: string
        author:
          title: Author
          type: string
        document_id:
          title: Document Id
          type: string
    DocumentChunkWithScore:
      title: DocumentChunkWithScore
      required:
        - text
        - metadata
        - score
      type: object
      properties:
        id:
          title: Id
          type: string
        text:
          title: Text
          type: string
        metadata:
          $ref: "#/components/schemas/DocumentChunkMetadata"
        embedding:
          title: Embedding
          type: array
          items:
            type: number
        score:
          title: Score
          type: number
    DocumentMetadataFilter:
      title: DocumentMetadataFilter
      type: object
      properties:
        document_id:
          title: Document Id
          type: string
        source:
          $ref: "#/components/schemas/Source"
        source_id:
          title: Source Id
          type: string
        author:
          title: Author
          type: string
        start_date:
          title: Start Date
          type: string
        end_date:
          title: End Date
          type: string
    HTTPValidationError:
      title: HTTPValidationError
      type: object
      properties:
        detail:
          title: Detail
          type: array
          items:
            $ref: "#/components/schemas/ValidationError"
    Query:
      title: Query
      required:
        - query
      type: object
      properties:
        query:
          title: Query
          type: string
        filter:
          $ref: "#/components/schemas/DocumentMetadataFilter"
        top_k:
          title: Top K
          type: integer
          default: 3
    QueryRequest:
      title: QueryRequest
      required:
        - queries
      type: object
      properties:
        queries:
          title: Queries
          type: array
          items:
            $ref: "#/components/schemas/Query"
    QueryResponse:
      title: QueryResponse
      required:
        - results
      type: object
      properties:
        results:
          title: Results
          type: array
          items:
            $ref: "#/components/schemas/QueryResult"
    QueryResult:
      title: QueryResult
      required:
        - query
        - results
      type: object
      properties:
        query:
          title: Query
          type: string
        results:
          title: Results
          type: array
          items:
            $ref: "#/components/schemas/DocumentChunkWithScore"
    Source:
      title: Source
      enum:
        - email
        - file
        - chat
      type: string
      description: An enumeration.
    ValidationError:
      title: ValidationError
      required:
        - loc
        - msg
        - type
      type: object
      properties:
        loc:
          title: Location
          type: array
          items:
            anyOf:
              - type: string
              - type: integer
        msg:
          title: Message
          type: string
        type:
          title: Error Type
          type: string
  securitySchemes:
    HTTPBearer:
      type: http
      scheme: bearer




================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_0.txt
================================================
Bank of England
Monetary Policy Report
Monetary Policy Committee 
November 2
023



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_1.txt
================================================
Monetary policy at the Bank of England  
The objectives of monetary policy  
The Bankâ€™s Monetary Policy Committee (MPC) sets monetary policy to keep inflation low and 
stable, which supports growth and jobs. Subject to maintaining price stability, the MPC is also 
required to support the Governmentâ€™s economic policy.  
The Government has set the MPC a target for the 12-month increase in the Consumer Prices 
Index of 2%.  
The 2% inflation target is symmetric and applies at all times.  
The MPCâ€™s remit  recognises, however, that the actual inflation rate will depart from its target 
as a result of shocks and disturba nces, and that attempts to keep inflation at target in these 
circumstances may cause undesirable volatility in output. In exceptional circumstances, the 
appropriate horizon for returning inflation to target can vary. The MPC will communicate how 
and when i t intends to return inflation to the target.  
The instruments of monetary policy  
The MPC currently uses two main monetary policy tools. First, we set the interest rate that 
banks and building societies earn on deposits, or â€˜reservesâ€™, placed with the Bank o f England 
â€“this is Bank  Rate. Second, we can buy government and corporate bonds, financed by the
issuance of central bank reserves â€“ this is asset purchases or quantitative easing.  
The Monetary Policy Report  
The MPC is committed to clear, transparent comm unication. The Monetary Policy Report 
(MPR) is a key part of that. It allows the MPC to share its thinking and explain the reasons for 
its decisions.  
The Report is produced quarterly by Bank staff under the guidance of the members of the 
MPC.  
This Report has been prepared and published by the Bank of England in accordance with 
section 18 of the Bank of England Act 1998.  
Bank of England  
Page 1



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_10.txt
================================================
Table 1.B: Conditioning assumptions (a) (b)
Average
1998â€“2007Average
2010â€“192022 2023 2024 2025 2026
Bank Rate
(c)5.0 0.5 2.8 5.3 (5.8) 5.1 (5.9) 4.5 (5) 4.2
Sterling
effective
exchange
rate (d)100 82 78 81 (82) 80 (81) 80 (81) 79
Oil prices
(e)39 77 89 90 (79) 81 (75) 77 (72) 74
Gas prices
(f)29 52 201 118 (113) 142 (139) 117 (114) 99
Nominal
government
expenditure
(g)7Â¼ 2Â¼ 4 6Â¼ (4) Â¾ (3) 1Â¾ (1Â½) 2Â½
1.2: Key judgements and risks
1.2: Key judgement 1
GDP is expected to be broadly flat in the first half of the forecast period and growth
is projected to remain well below historical averages in the medium term. That
reflects the significant increase in Bank Rate since the start of this tightening cycle,
subdued potential supply growth, and a waning boost from fiscal policy .Sources: Bank of England, Bloomberg Finance L.P., Office for Budget Responsibility (OBR), ONS, Refinitiv Eikon from
LSEG and Bank calculations.
(a) The table shows the projections for financial market prices, wholesale energy prices and government spending
projections that are used as conditioning assumptions for the MPCâ€™ s projections for CPI inflation, GDP growth and the
unemployment rate. Figures in parentheses show the corresponding projections in the August 2023 Monetary Policy
Report.
(b) Financial market data are based on averages in the 15 working days to 24 October 2023. Figures show the average
level in Q4 of each year, unless otherwise stated.
(c) Per cent. The path for Bank Rate implied by forward market interest rates. The curves are based on overnight index
swap rates.
(d) Index. January 2005 = 100. The convention is that the sterling exchange rate follows a path that is half way between
the starting level of the sterling ERI and a path implied by interest rate dif ferentials.
(e) Dollars per barrel. Projection based on monthly Brent futures prices.
(f) Pence per therm. Projection based on monthly natural gas futures prices.
(g) Annual average growth rate. Nominal general government consumption and investment. Projections are based on
the OBR's March 2023 Economic and Fiscal Outlook. Historical data based on NMRP+D7QK.
Bank of England  
Page 10



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_100.txt
================================================
Annex: Other forecasters â€™ expectations
This annex reports the results of the Bankâ€™s most recent survey of external forecasters.
Responses were submitted in the two weeks to 20 October and are summarised in Chart
A. These are compared with the MPCâ€™s modal projections, which are conditioned on a
range of assumptions (Section 1.1) that may differ from those made by external
forecasters.
On average, respondents expected GDP to rise by 0.6% in the four quarters to 2024 Q4
(left panel, Chart A). Responses ranged from -0.5% to 2.2%. Four-quarter GDP growth
was then expected to rise, on average, to 1.4% in 2025 Q4 and remain at 1.4% in 2026
Q4. These forecasts are higher than the MPCâ€™s modal projections for 2025 Q4 and 2026
Q4 of 0.4% and 1.1% respectively.
External forecasters expected an unemployment rate of 4.8% in 2024 Q4, higher than the
MPCâ€™s projection of 4.7% (middle panel, Chart A). The average external forecast then falls
to 4.6% for 2025 Q4 and 2026 Q4. By comparison, in the MPCâ€™s projection, the
unemployment rate increases to 5.0% in 2025 Q4 rising to 5.1% in 2026 Q4.
CPI inflation was expected to fall on average, to 2.4% in 2024 Q4, a slightly faster decline
than the MPCâ€™s projection of 3.1% (right panel, Chart A). The average forecasts for 2025
Q4 and 2026 Q4 were broadly in line with the 2% target at 2.1% for both periods, a little
above the MPCâ€™s modal projections at 1.9% and 1.5%.
Bank of England  
Page 100



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_101.txt
================================================
Chart A: At the three-year horizon, external forecasters expected four-quarter GDP
growth to be 1.4%, the unemployment rate to be 4.6%, and CPI inflation to be 2.1%
Projections for GDP, the unemployment rate and CPI inflation
Bank of England  
Page 101



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_102.txt
================================================
Glossary and other information
AWE â€“ average weekly earnings.
CPI â€“ consumer prices index.
CPI inflation â€“ inflation measured by the consumer prices index.
CPIH â€“ consumer prices index including owner-occupiersâ€™ housing costs.
DMP â€“ Decision Maker Panel.
ERI â€“ exchange rate index.
GDP â€“ gross domestic product.
HICP â€“ harmonised index of consumer prices.
LFS â€“ Labour Force Survey.
M4 â€“ UK non-bank, non-building society private sectorâ€™s holdings of sterling notes and
coin, and their sterling deposits (including certificates of deposit, holdings of commercial
paper and other short-term instruments and claims arising from repos) held at UK banks
and building societies.
MWSS â€“ Monthly Wages and Salaries Survey.
OIS â€“ overnight index swap.
PCE â€“ personal consumption expenditure.
PMI â€“ purchasing managersâ€™ index.
PPI â€“ producer price index.
RPI â€“ retail prices index.Glossary of selected data and instruments
Bank of England  
Page 102



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_103.txt
================================================
BTL â€“ buy-to-let.
CCS â€“ Credit Conditions Survey.
CFO â€“ chief financial officer.
CIPS â€“ Chartered Institute of Purchasing and Supply.
ECB â€“ European Central Bank.
EU â€“ European Union.
FCA â€“ Financial Conduct Authority.
FOMC â€“ Federal Open Market Committee.
FPC â€“ Financial Policy Committee.
G7 â€“ Canada, France, Germany, Italy, Japan, the United Kingdom and the United States.
GfK â€“ Gesellschaft fÃ¼r Konsumforschung, Great Britain Ltd.
HMRC â€“ His Majestyâ€™s Revenue and Customs.
HMT â€“ HM Treasury.
ILO â€“ International Labour Organization.
IMF â€“ International Monetary Fund.
IT â€“ information technology.
LTV â€“ loan to value.
MIDAS â€“ mixed-data sampling.
MPC â€“ Monetary Policy Committee.
MTIC â€“ missing trader intra-community.
OBR â€“ Office for Budget Responsibility.
OFC â€“ other financial corporation.Abbreviations
Bank of England  
Page 103



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_104.txt
================================================
Ofgem  â€“ Office of Gas and Electricity Markets.
ONS â€“ Office for National Statistics.
OPEC â€“ Organization of the Petroleum Exporting Countries.
PAYE â€“ Pay As You Earn.
PPI â€“ payment protection insurance.
PPP â€“ purchasing power parity.
REC â€“ Recruitment and Employment Confederation.
RTI â€“ Real Time Information.
S&P â€“ Standard & Poorâ€™s.
SME â€“ small and medium-sized enterprise.
WEO â€“ IMF World Economic Outlook.
Except where otherwise stated, the source of the data used in charts and tables is the
Bank of England or the Office for National Statistics (ONS) and all data, apart from
financial markets data and results from the Decision Maker Panel (DMP) Survey , are
seasonally adjusted.
n.a. = not available.
Because of rounding, the sum of the separate items may sometimes dif fer from the total
shown.
On the horizontal axes of graphs, larger ticks denote the first observation within the
relevant period, eg data for the first quarter of the year.Symbols and conventions
1.
The ONS has published estimates based on very early information from the developmental TLFS, covering the six
months to August 2023, but these are only indicative and are not seasonally adjusted.
Bank of England  
Page 104



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_11.txt
================================================
UK GDP is expected to have been flat in 2023 Q3, weaker than expected in the August
Monetary Policy Report (Section 2.2). Indicators of growth in Q4 are mixed. Some
business surveys are pointing to a slight contraction in GDP, but others are less
pessimistic. Many contacts of the Bankâ€™s Agents have continued to report weak activity
growth and a subdued outlook. On balance, Bank staff expect GDP to grow by 0.1% in the
fourth quarter, also weaker than projected in August.
Household consumption growth is now expected to be similarly weak during the second
half of this year, consistent with recent declines in retail sales volumes, consumer services
output and consumer confidence. Nonetheless, real labour income growth has been
slightly stronger than expected and workers do not appear to have become more
pessimistic about the security of their own jobs.
For a number of forecast rounds, the Committee has made a judgement to boost the
expected path of demand in light of the surprising resilience of economic activity . This
reflected a number of factors, including the possibility of lower precautionary saving by
households, in turn related to a lower risk of job loss given continued strength in labour
market activity. Developments since August, including in the labour market (Key
judgement 2), suggest that economic activity has been somewhat less resilient over
recent months. As a result, in its latest growth projection, the Committee has scaled back
somewhat the extent of this judgement to boost demand, but has not taken it out
completely.
Given the significant increase in Bank Rate since the start of this tightening cycle, the
current monetary policy stance is restrictive. There are increasing signs of some impact of
tighter policy on momentum in the real economy (Section 3.3). Based on the average
relationships over the past between Bank Rate, other financial instruments and economic
activity, Bank staff estimate that more than half of the impact of higher interest rates on
the level of GDP is still to come through, although there is significant uncertainty around
that estimate. The Committee will continue to monitor closely the impact of the significant
increase in Bank Rate. It will also continue to keep under review the relationship between
Bank Rate and economic activity, including how it may have changed during the current
tightening cycle.
Taken together, the pass-through of past rises in interest rates and the latest market-
implied interest rate path on which the forecast is conditioned (Section 1.1) continue to
push down on GDP over the forecast period. Nevertheless, the fall in the market path over
recent months pushes up on GDP in this forecast compared with the August projection, all
else equal.
Bank of England  
Page 11



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_12.txt
================================================
As in recent projections, and taking account of all announced government plans, the
positive impacts of past fiscal loosening measures related to the pandemic and the energy
price shock (Section 3.1) on the level of GDP unwind, which pulls down on GDP growth
throughout much of the forecast period. Real government consumption is, nevertheless,
expected to grow quite strongly in 2024, bouncing back from the negative impact on
spending associated with public sector strike activity in 2023.
International growth has remained subdued (Section 2.1) and the path of global GDP  is
expected to be broadly similar to that in the August Report. In the MPCâ€™s November
projection, annual UK-weighted world GDP growth is projected to rise from around 1Â½%
in 2023 to around 2% in the medium term (Table 1.D). That compares with average
annual growth of around 2Â½% in the decade prior to the pandemic.
Although aggregate global growth is evolving largely as expected, US GDP  growth has
been stronger than expected, while the euro area has seen weaker growth. As in the
United Kingdom, the impact of past monetary policy tightening is dragging on activity , with
fiscal policy and the rundown of excess household saving underpinning the resilience of
US demand. Growth in the euro area is expected to pick up over the forecast period, while
growth in the United States and China is projected to fall back somewhat.
Overall in the Committeeâ€™s November projection, UK GDP is projected to remain broadly
flat in the first half of the forecast period. As well as the impact of higher interest rates, this
reflects a waning boost from fiscal policy and relatively weak potential supply (Key
judgement 1 in the February 2023 Report and Key judgement 2 in this Report). GDP
growth recovers over the second half of the forecast period, although it remains well
below historical averages in the medium term. Calendar-year GDP growth is expected to
be marginally positive in 2024, and to increase by Â¼% in 2025 and by Â¾% in 2026 (T able
1.D). Four-quarter GDP growth picks up to slightly over 1% by 2026 Q4 (Chart 1.1).
Relative to the August Report projection, four-quarter GDP growth is expected to be
slightly weaker throughout the forecast period. Abstracting from the increase in the
historical level of GDP related to the Blue Book 2023 revisions (Box C), the level of GDP
has been revised down by around 1% by the end of the forecast period compared with the
August Report. That lower level of activity reflects recent weaker-than-expected GDP data
and the Committeeâ€™s related decision in this forecast to reduce somewhat its previous
judgement boosting expected demand. Those factors have been offset partially by the
projected boost to GDP from the lower path of market interest rates on which the forecast
is conditioned and, to a lesser degree, the estimated impact on growth of the recent
exchange rate depreciation (Section 1.1).
Bank of England  
Page 12



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_13.txt
================================================
In the GDP projection conditioned on the alternative assumption of constant interest rates
at 5.25% over the forecast period, growth is lower in the forecast period compared with
the MPCâ€™s projection conditioned on market rates, as the constant rate assumption is
above the market curve to an increasing degree beyond the end of next year .
Within the expenditure components underpinning the November GDP projection
conditioned on market interest rates, calendar-year household spending is expected to
grow by Â½% this year, to be flat in 2024, and to rise by Â½% in 2025 and by 1% in 2026
(Table 1.D). This projected path of consumption is weaker than in the August Report. Real
post-tax labour income is projected to grow by Â¾% this year and by 1% in 2024, but more
modestly in 2025 and 2026. Taking account of all components of household income, theChart 1.1: GDP growth projection based on market interest rate expectations, other
policy measures as announced
The fan chart depicts the probability of various outcomes for GDP  growth. It has been conditioned on Bank Rate
following a path implied by market yields, but allows the Committeeâ€™ s judgement on the risks around the other
conditioning assumptions set out in Section 1.1, including wholesale energy prices, to af fect the calibration of the fan
chart skew. To the left of the shaded area, the distribution reflects uncertainty around revisions to the data over the past.
To the right of the shaded area, the distribution reflects uncertainty over the evolution of GDP growth in the future. If
economic circumstances identical to todayâ€™s were to prevail on 100 occasions, the MPCâ€™s best collective judgement is
that the mature estimate of GDP growth would lie within the darkest central band on only 30 of those occasions. The fan
chart is constructed so that outturns are also expected to lie within each pair of the lighter aqua areas on 30 occasions.
In any particular quarter of the forecast period, GDP growth is therefore expected to lie somewhere within the fan on 90
out of 100 occasions. And on the remaining 10 out of 100 occasions GDP growth can fall anywhere outside the aqua
area of the fan chart. Over the forecast period, this has been depicted by the grey background. See the Box on page 39
of the November 2007 Inflation Report for a fuller description of the fan chart and what it represents.
Bank of England  
Page 13



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_14.txt
================================================
saving ratio is expected to be broadly unchanged over much of the forecast period, flatter
than the downward-sloping path projected in the August Report. That reflects a higher
degree of precautionary saving than expected previously, in part owing to the higher
profile for unemployment (Key judgement 2), and is broadly consistent with the
Committeeâ€™s decision in this forecast to reduce somewhat the scale of its judgement
boosting expected spending.
In large part reflecting the transmission of higher interest rates (Section 3.3), housing
investment is expected to fall significantly, by 5Â¾% in 2023, by 6Â¾% 2024 and by 2Â¾% in
2025. This profile is similar to that in the August Report, and is consistent with weakness
in housing transactions and forward-looking indicators of new housing construction.
Business investment is projected to increase by just under 7% this year , but to fall by 1%
in 2024, in part reflecting transport sector related volatility in capital expenditure. As set
out in Box D, respondents to a special survey by the Bankâ€™s Agents expect investment
growth to slow slightly next year but remain positive. Investment intentions are being held
back by economic uncertainty, and by the cost and availability of finance, but supported by
the need to invest for a number of reasons such as digitalisation, ef ficiency, sustainability
and maintenance. Further out in the latest projection, business investment is expected to
be broadly flat in 2025, before increasing by 2% in 2026.
There are risks in both directions around the central projections for household spending
and GDP, including related to the Committeeâ€™s decision in this forecast to scale back
somewhat the extent of its judgement to boost expected demand. Spending could be
stronger than expected if there is greater resilience in labour market activity (Key
judgement 2) and some households choose to save less or run down existing stocks of
savings to a greater extent. Conversely, demand could be weaker than expected if some
people become more worried about their job security and try to build up their savings to a
greater extent. The latest Bank/NMG survey suggests that, among those who are
planning to change their saving habits compared with the previous six months, the share
of households who are planning to save more than usual over the next six months is
expected to rise to around half (Chart 3.5).
There are also risks related to the transmission of monetary policy , including the response
of consumption to higher interest rates. Although there is a much greater proportion of
fixed-rate mortgages than in previous tightening cycles, some mortgagors who anticipate
needing to refinance in the future may take greater advance actions to prepare for facingThe risks around the projection for UK GDP growth are judged to be broadly
balanced.
Bank of England  
Page 14



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_15.txt
================================================
higher interest costs. There could also be non-linearities in the response of consumption
to weakness in the housing market via collateral and other channels (Section 3.3). These
could, in turn, relate to developments in unemployment and mortgage distress.
Internationally, the risk of higher commodity prices, including related to geopolitical
developments, could lead to weaker UK economic activity as well as upward pressure on
CPI inflation (Key judgement 3).
1.2: Key judgement 2
The margin of excess demand in the UK economy has diminished over recent
quarters and an increasing degree of economic slack is expected to emerge from
the start of next year. Unemployment is expected to rise further over the forecast
period and exceed the Committeeâ€™s upwardly revised estimate of the medium-term
equilibrium rate from the end of next year.
As set out in Box C, the upward revisions to GDP in Blue Book 2023 provide limited news
about the balance of demand and supply in the market sector economy , and hence
domestic inflationary pressures (Key judgement 3). In light of that, the MPC has judged it
appropriate to revise up potential supply in line with the revisions to measured GDP , such
that the output gap over the past is unchanged.
The Committee continues to judge that there has been a significant margin of excess
demand in the economy over the past two years, averaging just under 1% of potential
GDP, and in part reflecting the weakness of potential supply. That excess demand has
been accounted for by the tightness of the labour market and, prior to 2023, a higher than
normal degree of capacity utilisation within companies.
Since around the middle of last year, however, the margin of aggregate excess demand is
judged to have been diminishing, such that only a little remains currently . This is
consistent with the recent loss of momentum in activity (Key judgement 1) and with other
top-down cross-checks of the degree of slack in the economy. Compared with the August
Report, the decline in excess demand is judged to be occurring a little faster during the
second half of this year than expected previously.
The MPC is continuing to consider the collective steer from a wide range of data to inform
its view on labour market developments. As discussed in Box B, there are increased
uncertainties around the ONSâ€™s official labour market activity data that have previously
been based on the Labour Force Survey (LFS). A decline in response rates has resulted
Bank of England  
Page 15



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_16.txt
================================================
in the ONS temporarily ceasing to publish LFS estimates of employment, unemployment
and inactivity from the June data. The ONS has, however, published experimental
estimates of employment and unemployment that need to be interpreted with caution.
As discussed in Box B, the Committee is therefore continuing to consider the collective
steer from other indicators of labour market activity. Generally speaking, the Committee
has a wider set of indicators to draw on for its understanding of developments in
employment than it does for developments in the split of non-employment between
unemployment and inactivity.
Taken together, a range of indicators suggest that employment growth is likely to have
softened over the second half of this year to a greater extent than expected in the August
Report, but not turned negative (Section 2.2). Developments in indicators of recruitment
difficulties point to a loosening in the labour market. Contacts of the Bankâ€™s Agents have
also reported an easing in hiring constraints, although persistent skills shortages remain in
some sectors.
Recent LFS data issues notwithstanding, the unemployment rate is expected to be around
4Â¼% during the second half of 2023, slightly higher than expected in the August Report.
On this basis, the vacancies to unemployment ratio, an alternative measure of labour
market tightness, has continued to decline.
As discussed in the August Report, there is significant uncertainty about the rate of
unemployment consistent with meeting the 2% inflation target in the medium term. Higher-
than-expected wage growth after the recent terms of trade shock to the economy
suggests that the medium-term equilibrium rate is likely to be temporarily higher , as
employees and domestic firms have sought compensation in the form of higher nominal
pay and domestic selling prices for the reductions in real incomes that they have
experienced. There is also some evidence that the efficiency with which vacancies are
matched to those seeking work has decreased over recent years, which would be pushing
up more structurally on the equilibrium rate of unemployment. In its November forecast,
the Committee has made an additional judgement to increase its estimate of the medium-
term equilibrium rate of unemployment from the period since the energy shock started
and, to a lesser degree, over the forecast period. This equilibrium rate is judged to be
around 4Â½% currently. This change in judgement is consistent with a somewhat stronger
outlook for wage growth and domestic inflationary pressures all else equal (Key
judgement 3).
Bank of England  
Page 16



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_17.txt
================================================
The increase in the equilibrium rate of unemployment also pushes down aggregate supply
growth. Four-quarter supply growth is expected to slow from around 1Â½% currently to
around Â¾% next year, before rising to around 1Â¼% in the medium term, the latter of which
is similar to in previous Reports. The Committee will undertake a full review of the
determinants of the overall longer-term supply capacity of the economy in its next regular
stocktake ahead of the February 2024 Report.
Although aggregate supply is expected to be relatively subdued, particularly in the near
term, the outlook for demand is weaker, leading to an increasing degree of economic
slack emerging in the Committeeâ€™s latest projections from the start of next year. The
margin of aggregate excess supply is expected to widen to just over 1Â½% of potential
GDP by the end of the forecast period (Table 1.A), broadly similar to its path in the August
Report. Relative to August, the projection for excess demand/supply has been pushed up
by the lower market path of interest rates and by the Committeeâ€™ s judgement to raise the
equilibrium rate of unemployment, which has reduced potential supply . But the projection
has been pushed down by the Committeeâ€™s decision to scale back slightly the extent of its
judgement boosting expected demand.
In part reflecting indications of relatively optimistic longer-term expectations for output,
companies are expected to continue to respond to the weakness in demand by retaining
their existing inputs, while using them less intensively and hoarding labour for a prolonged
period. This limits to some extent the rise in unemployment that would otherwise be
expected to occur. Nevertheless, in the MPCâ€™s November projection, the unemployment
rate is projected to continue to rise gradually over the forecast period such that it exceeds
the Committeeâ€™s updated estimate of the medium-term equilibrium rate from the end of
next year, and it reaches just over 5% by the end of 2026 (Chart 1.2). This is slightly
higher than in the August Report, but is consistent with a similar degree of labour market
looseness, reflecting the Committeeâ€™s judgement to raise further the equilibrium rate in
this forecast.
Bank of England  
Page 17



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_18.txt
================================================
In projections conditioned on the alternative assumption of constant interest rates at
5.25% over the forecast period, the unemployment rate rises to a greater extent across
the forecast period compared with the MPCâ€™s projection conditioned on market rates, as
the constant rate assumption is above the market curve to an increasing degree beyond
the end of next year.
Reflecting the considerable uncertainties around interpreting estimates from the Labour
Force Survey, there are risks in both directions around the recent path of the
unemployment rate, and hence the outlook for unemployment and labour market
tightness. The labour market could remain tighter than assumed for a number of economicChart 1.2: Unemployment rate projection based on market interest rate
expectations, other policy measures as announced
The fan chart depicts the probability of various outcomes for the ILO definition of unemployment. It has been
conditioned on Bank Rate following a path implied by market yields, but allows the Committeeâ€™ s judgement on the risks
around the other conditioning assumptions set out in Section 1.1, including wholesale energy prices, to af fect the
calibration of the fan chart skew. The coloured bands have the same interpretation as in Chart 1.1, and portray 90% of
the probability distribution. Up to June 2023, this fan chart is based on LFS unemployment data. Beyond this point, the
Committee is drawing on the collective steer from other indicators of unemployment to inform its projection (see Box B).
The fan begins in 2023 Q3, a quarter earlier than for CPI inflation. A significant proportion of this distribution lies below
Bank staffâ€™s current estimate of the long-term equilibrium unemployment rate. There is therefore uncertainty about the
precise calibration of this fan chart.
The risks around the unemployment rate projection are judged to be broadly
balanced.
Bank of England  
Page 18



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_19.txt
================================================
reasons, including the upside risks around the outlook for demand (Key judgement 1).
Conversely, the labour market could loosen more rapidly than assumed, again including
because of any downside risks to demand.
There also remains significant uncertainty around the Committeeâ€™ s updated assumption
for the path of the equilibrium rate of unemployment, news in which would, holding
demand fixed, have implications for labour market tightness and inflationary pressures. In
particular, it is difficult to judge how quickly some of the factors pushing up the equilibrium
rate recently could fade over the forecast period, and the extent to which there are greater
structural factors, such as interactions with the benefits system, at play .
1.2: Key judgement 3
Second-round effects in domestic prices and wages are expected to take longer to
unwind than they did to emerge. In the modal forecast conditioned on the market-
implied path of market interest rates, an increasing degree of slack in the economy
and declining external cost pressures lead CPI inflation to return to the 2% target
by the end of 2025 and to fall below target thereafter. The Committee continues to
judge that the risks are skewed to the upside. Taking account of this skew, mean
CPI inflation is 2.2% and 1.9% at the two and three-year horizons respectively .
Twelve-month CPI inflation remains well above the MPCâ€™s 2% target, but has fallen back
to 6.7% in both September and in 2023 Q3 as a whole, below expectations in the August
Report. Most of the downside news since the previous Report reflects lower core goods
price inflation. Services inflation has been only slightly weaker than expected in August.
Twelve-month CPI inflation is expected to continue to fall quite sharply in the near term, to
below 5% in October, as the reduction in the Ofgem household energy price cap more
than offsets the impact on motor fuel costs of the recent rise in sterling oil prices (Section
2.3). CPI inflation is projected to decline to an average of 4.6% in 2023 Q4, slightly lower
than expected in the August Report, and then to 4.4% in 2024 Q1 and 3.6% in 2024 Q2
(Table 1.C). This decline is expected to be accounted for by lower energy, core goods and
food price inflation and, beyond January, by some fall in services price inflation.
Based on the latest paths of oil and gas futures prices, the direct energy contribution to
inflation is slightly higher throughout the forecast period than in the Committeeâ€™ s previous
forecast. In absolute terms, this direct energy contribution remains slightly negative over
the second half of the forecast period.
Bank of England  
Page 19



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_2.txt
================================================
The Monetary  Policy Committee  
â€¢Andrew Bailey, Chair
â€¢Sarah Breeden
â€¢Ben Broadbent
â€¢Swati Dhingra
â€¢Megan Greene
â€¢Jonathan Haskel
â€¢Catherine L Mann
â€¢Huw Pill
â€¢Dave Ramsden
PowerPoint â„¢ versions of the Monetary  Policy Report charts and Excel spreadsheets of the 
data underlying most of them are available at http://www.bankofengland.co.uk/monetary -
policy -report/2023/november -2023 . 
Â© Bank of England 202 3 
ISSN 2633 -7819  
Bank of England  
Page 2



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_20.txt
================================================
Four-quarter UK-weighted world export price inflation, excluding the direct ef fect of oil
prices, is estimated to have been slightly negative in 2023 Q2 and is expected to decline
more sharply over the next year, though by slightly less than anticipated in the August
Report. The weak absolute profile continues to reflect the clearing of global supply chain
bottlenecks and easing producer price pressures, particularly in China. W orld export price
inflation then turns slightly positive from the middle of 2025, broadly unchanged from the
August Report. The recent depreciation of the sterling exchange rate (Section 1.1) will put
upward pressure on UK import price inflation, and over time on CPI inflation, relative to
the August Report. Overall, import prices are projected to fall by 1Â¾% in 2023, a lesser fall
than expected in the August Report, and by 3% in 2024 (Table 1.D).
The MPC is continuing to monitor closely indications of persistent inflationary pressures
and resilience in the economy as a whole (Key judgement 1), including a range of
measures of the underlying tightness of labour market conditions (Key judgement 2),
wage growth and services price inflation.
Services CPI inflation has remained elevated and somewhat stronger than can be
explained by a simple empirical model based on developments in labour and non-labour
input costs. There has, however, been some signs of a turning point in a measure of
underlying inflationary pressures in consumer services prices (Chart 2.18).
Annual private sector regular average weekly earnings (AWE) growth has increased
further to 8.0% in the three months to August, materially above expectations in the August
Report. This most recent rise in growth is difficult to reconcile with other indicators of pay
growth (Section 2.3). Most of these have tended to be more stable at rates of growth that
are high but not quite as elevated as the AWE series. For example, a Bank staff proxy for
the private sector based on HMRC PAYE Real Time Information shows median pay
growth of around 7% currently.
Recent outturns in earnings growth have been stronger than standard models of wage
growth, based on productivity, short-term inflation expectations and a measure of
economic slack, would have predicted (Chart 1.3). The near-term outlook for pay growth is
also expected to be somewhat stronger than projected in the August Report. The annual
growth rate of private sector regular AWE is nonetheless still projected to decline in
coming quarters, to below 6% next spring and to just below 5% by the end of 2024. This is
broadly consistent with the forward-looking indications from the Decision Maker Panel and
early evidence from the Bankâ€™s Agents on private sector pay settlements next year.
Bank of England  
Page 20



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_21.txt
================================================
In the MPCâ€™s modal projection, private sector regular AWE growth falls to around 3% by
the end of the forecast period, as short-term inflation expectations are assumed to fall
back and a margin of spare capacity is expected to open up in the labour market in the
medium term (Key judgement 2). This is a slightly higher medium-term profile for AWE
growth compared with the August Report.
Private sector regular AWE growth is also expected to ease more slowly than the range of
forecasts from a suite of wage models would predict (Chart 1.3). These empirical models
illustrate what could happen if future pay setting is based on inflation expectations that
take account of the sharply downward near-term path of CPI inflation among other factors.
But there is insufficient evidence at present to be confident that wages will be set in this
way.
This difference between empirical model outputs and the November projection is therefore
one way of demonstrating the MPCâ€™s continuing judgement that second-round effects in
both domestic prices and wages are expected to take longer to unwind than they did toChart 1.3: Projections for private sector regular average weekly earnings four-
quarter growth ( a)
Sources: Bloomberg Finance L.P., Citigroup, ONS, YouGov and Bank calculations.
(a)
The shaded swathe represents a range of projections from three statistical models of nominal private sector regular
average weekly earnings growth, including a wage equation based on Yellen (2017) , a wage equation based on
Haldane (2018)  and a simple error-correction model based on productivity, inflation expectations and slack. The slack
measure for these models is based on the MPCâ€™s estimate of the unemployment gap. The projections are dynamic,
multi-step ahead forecasts beginning at a point within the modelsâ€™  estimation periods and are sensitive to data revisions,
which can lead to changes in the swathe over the past as well as over the forecast period.
Bank of England  
Page 21



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_22.txt
================================================
emerge. Reflecting the higher expected path of wage growth, the Committee has decided
in this forecast to increase slightly the scale of its judgement on the persistence of
domestic prices in its modal projection, crystallising further some of the upside risks from
second-round effects that have been incorporated into the mean projection previously.
This is in addition to the Committeeâ€™s decision in this forecast to increase the medium-
term equilibrium rate of unemployment (Key judgement 2), which has a similar ef fect on
increasing the persistence of wage growth and domestic inflationary pressures.
In the MPCâ€™s modal projection conditioned on the market-implied path of interest rates as
captured in the 15-working day average to 24 October, CPI inflation declines to below the
2% target from the end of 2025, as an increasing degree of slack in the economy reduces
domestic inflationary pressures alongside declining external cost pressures. CPI inflation
is projected to be 1.9% in two yearsâ€™ time and 1.5% in three years (Table 1.C and Chart
1.4). Compared with the August Report modal projection, CPI inflation is expected to
return to close to the 2% target slightly less rapidly in the middle of the forecast period,
reflecting higher energy and other import price inflation, with the latest profile relatively flat
around 2% over the four quarters from 2025 Q2.
Bank of England  
Page 22



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_23.txt
================================================
Chart 1.4: CPI inflation projection based on market interest rate expectations, other
policy measures as announced
The fan chart depicts the probability of various outcomes for CPI inflation in the future. It has been conditioned on Bank
Rate following a path implied by market yields, but allows the Committeeâ€™ s judgement on the risks around the other
conditioning assumptions set out in Section 1.1, including wholesale energy prices, to af fect the calibration of the fan
chart skew. If economic circumstances identical to todayâ€™s were to prevail on 100 occasions, the MPCâ€™s best collective
judgement is that inflation in any particular quarter would lie within the darkest central band on only 30 of those
occasions. The fan chart is constructed so that outturns of inflation are also expected to lie within each pair of the lighter
orange areas on 30 occasions. In any particular quarter of the forecast period, inflation is therefore expected to lie
somewhere within the fans on 90 out of 100 occasions. And on the remaining 10 out of 100 occasions inflation can fall
anywhere outside the orange area of the fan chart. Over the forecast period, this has been depicted by the grey
background. See the Box on pages 48â€“49 of the May 2002 Inflation Report for a fuller description of the fan chart and
what it represents.
Bank of England  
Page 23



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_24.txt
================================================
Table 1.C: The quarterly modal projection for CPI inflation based on market rate
expectations (a)
2023 Q4 2024 Q1 2024 Q2 2024 Q3
CPI inflation 4.6 4.4 3.6 3.3
2024 Q4 2025 Q1 2025 Q2 2025 Q3
CPI inflation 3.1 2.5 2.1 2.1
2025 Q4 2026 Q1 2026 Q2 2026 Q3 2026 Q4
CPI inflation 1.9 1.9 1.7 1.6 1.5
In the modal projection conditioned on the alternative assumption of constant interest
rates at 5.25% over the forecast period, CPI inflation is expected to be 1.7% and 1.2% in
two yearsâ€™ and three yearsâ€™ time respectively (Chart 1.5). These are lower than the
Committeeâ€™s modal forecasts at the same horizons conditioned on market rates, as the
constant rate assumption is above the market curve to an increasing degree beyond the
end of next year.(a) Four-quarter inflation rate.
Bank of England  
Page 24



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_25.txt
================================================
There remain considerable uncertainties around the pace at which CPI inflation will return
sustainably to the 2% target.
There are near-term risks in both directions around the paths of CPI inflation and pay
growth. On the downside, weakness in non-labour input cost growth, including recent
developments in producer price indices, could lead to a faster-than-expected decline in
consumer goods price inflation. On the upside, the recent rise in private sector regular
AWE growth could prove to be a better guide to near-term wage growth dynamics than
the steer from other indicators of pay.
In the medium term, there remain considerable risks around the Committeeâ€™ s judgement
that second-round effects in domestic prices and wages are expected to take longer to
unwind than they did to emerge. On the one hand, these risks may be more balanced
following the MPCâ€™s decision this round to increase further its assumption for the medium-
term equilibrium rate of unemployment, owing both to resistance to past losses in real
income and to more persistent labour market frictions (Key judgement 2), and the decisionChart 1.5: CPI inflation projection based on constant interest rates at 5.25%, other
policy measures as announced
This fan chart depicts the probability of various outcomes for CPI inflation in the future, conditioned on the assumptions
in Table 1.A footnote (b), apart from for Bank Rate, with this chart conditioned on constant interest rates at 5.25%. The
fan chart has the same interpretation as Chart 1.4.
The risks around the modal CPI inflation projection are judged to remain skewed
to the upside.
Bank of England  
Page 25



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_26.txt
================================================
to increase slightly its judgement on the persistence of domestic prices. On the other
hand, it is possible that the higher path of pay growth puts even greater upward pressure
on domestic price inflation over the forecast period. There could remain an inconsistency
between the wage and domestic price profiles in the modal projections, such that
companies seek to rebuild their squeezed profit margins to a greater extent than has been
assumed. Nevertheless, the Bankâ€™s Agents report that firms generally have limited scope
to rebuild margins significantly by raising their prices.
The pace at which CPI inflation falls back to the 2% target will also depend on inflation
expectations. An upside risk to the inflation outlook is that households and firms are less
confident that inflation will fall back quickly and do not factor such a decline into their
wage and price-setting behaviour. Since the August Report, indicators of household and
corporate short-term inflation expectations have tended to decline further , while medium-
term inflation compensation measures in financial markets have remained above their
long-term averages. The Committee will continue to monitor measures of inflation
expectations very closely and act to ensure that longer-term inflation expectations are
anchored at the 2% target.
There are upside risks around the modal projection for UK CPI inflation from international
factors. There remains the possibility of more persistence in consumer price inflation in
the UKâ€™s major trading partners, for similar reasons to the risks of stronger domestic
inflationary pressures at home including the tightness of labour markets, and wage and
services price inflation remaining elevated for longer than expected.
In addition, geopolitical risks have increased following events in the Middle East. Although
there has so far been only a relatively limited rise in energy prices, uncertainty around
future oil prices has increased and the balance of risks around future oil prices has shifted
from the downside to the upside, as indicated by implied volatilities and risk reversals in
financial markets. A larger shock to energy prices could mean that CPI inflation falls back
to the 2% target more slowly than currently expected, through both direct and second-
round effects, while also leading to weaker growth (Key judgement 1).
Overall, the Committee judge that the risks around the modal projection for CPI inflation
remain skewed to the upside, primarily reflecting the possibility of more persistence in
domestic wage and price-setting, but also the increasing upside risk to inflation from
energy prices now. This pushes up on the mean, relative to the modal, inflation projections
in the forecast. Conditioned on market interest rates, mean CPI inflation is 2.2% and 1.9%
at the two and three-year horizons respectively.
Bank of England  
Page 26



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_27.txt
================================================
In the mean projection conditioned on the alternative assumption of constant interest rates
at 5.25% over the forecast period, CPI inflation is expected to be 2.0% and 1.6% in two
yearsâ€™ and three yearsâ€™ time respectively. This constant rate projection also returns
inflation to the 2% target three quarters earlier than the mean market rate forecast.
Bank of England  
Page 27



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_28.txt
================================================
Table 1.D: Indicative projections consistent with the MPC's modal forecast (a) (b)
World GDP
(UK-
weighted) (c)3Â¼ 2Â½ 3 1Â½ (1Â½) 1Â¾ (1Â¾) 2 (2) 2
World GDP
(PPP-
weighted) (d)4Â¼ 3Â¾ 3Â¼ 3 (2Â¾) 2Â¾ (2Â¾) 3 (3Â¼) 3
Euro-area
GDP (e)2Â¼ 1Â½ 3Â½ Â½ (Â½) Â¾ (1) 1Â¾ (1Â½) 1Â¾
US GDP ( f) 3 2Â½ 2 2Â¼ (1Â½) 1Â½ (Â¾) 1Â¼ (1Â½) 1Â½
Emerging
market GDP
(PPP-
weighted) (g)5Â½ 5 4 4 (4) 3Â¾ (3Â¾) 4 (4Â¼) 4
  of which,
China GDP (h)10 7Â¾ 3 5Â¼ (5Â¼) 4Â¼ (4Â½) 4Â¼ (4Â¾) 4Â¼
UK GDP ( i) 2Â¾ 2 4Â¼ Â½ (Â½) 0 (Â½) Â¼ (Â¼) Â¾
Household
consumption
(j)3Â¼ 2 5Â¼ Â½ (Â½) 0 (Â¾) Â½ (Â¾) 1
Business
investment (k)3 4Â¼ 9Â½ 6Â¾ (1Â¾) -1 (-2) 0 (1Â¾) 2
Housing
investment (l)3Â¼ 4 9Â½ -5Â¾ (-5Â¾) -6Â¾ (-6Â¼) -2Â¾ (-3) Â¼
Exports (m) 4Â½ 3Â½ 8Â¾ -Â¾ (-2) Â¼ (Â¼) Â¾ (Â½) 1
Imports (n) 6 4 14Â¼ -Â¾ (-3Â½) 1Â½ (2) Â¾ (1Â½) 2
Contribution of
net trade to
GDP (o)-Â¼ -Â¼ -1Â¾ 0 (Â½) -Â½ (-Â½) 0 (-Â¼) -Â¼
Real post-tax
labour income
(p)3Â¼ 1Â½ -2Â¾ Â¾ (0) 1 (Â¾) Â½ (Â½) Â¼
Real post-tax
household
income (q)3 1Â½ 0 2 (1Â¼) Â¼ (-Â¼) Â¼ (Â¼) Â½Average
1998â€“2007Average
2010â€“192022 2023 2024 2025 2026
Bank of England  
Page 28



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_29.txt
================================================
Household
saving ratio (r)7Â¼ 8 8 9Â¼ (9Â¼) 9Â½ (8Â½) 9Â½ (8) 9
Credit spreads
(s)Â¾ 2Â½ 1 Â¾ (Â½) 1 (Â¾) 1Â¼ (1Â¼) 1Â¼
Excess
supply/Excess
demand (t)0 -1Â¾ 1Â¼ Â¼ (Â½) -Â½ (-Â¼) -1Â¼ (-1Â¼) -1Â½
Hourly labour
productivity (u)2Â¼ Â½ Â¾ Â¼ (-Â¼) 1Â¼ (Â¼) Â¾ (Â¾) Â¾
Employment
(v)1 1Â¼ Â¾ Â½ (1) -Â½ (-Â½) 0 (0) Â¼
Average
weekly hours
worked (w)32Â¼ 32 31Â½ 31Â½ (31Â¾) 31Â¼ (31Â¾) 31Â¼ (31Â¾) 31Â¼
Unemployment
rate (x)5Â¼ 6 3Â¾ 4Â¼ (4) 4Â¾ (4Â½) 5 (4Â¾) 5
Participation
rate (y)63 63Â½ 63Â¼ 63Â½ (63Â¾) 63 (63Â¼) 62Â¾ (63) 62Â½
CPI inflation
(z)1Â½ 2Â¼ 10Â¾ 4Â¾ (5) 3Â¼ (2Â½) 2 (1Â½) 1Â½
UK import
prices (aa)-Â¼ 1Â¼ 12Â¾ -1Â¾ (-4Â½) -3 (-3Â¼) Â¼ (Â½) 0
Energy prices
â€“ direct
contribution to
CPI inflation
(ab)Â¼ Â¼ 3Â¾ -1Â¼ (-1Â½) Â½ (Â½) -Â¼ (-Â¼) -Â¼
Average
weekly
earnings
(AWE) (ac)4Â¼ 2 6 6Â¾ (6) 4Â¼ (3Â½) 2Â¾ (2Â½) 2
Unit labour
costs (ad)3 1Â¼ 7 5 (4Â¾) 3Â¾ (3) 2Â¼ (1Â¾) 1Â¼
Private sector
regular pay
based unit
wage costs
(ae)2 1Â½ 6Â½ 6 (7Â½) 3Â¾ (4) 3 (2Â½) 2Â¼
Bank of England  
Page 29



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_3.txt
================================================
 Contents  
Monetary Policy Summary  4 
1: The economic outlook  7 
1.1: The conditioning assumptions underlying the MPCâ€™s projections  9 
1.2: Key judgements and risks  10 
Box A: Monetary policy since the August 2023 Report  32 
2: Current economic conditions  34 
2.1: Global developments and domestic credit conditions  36 
2.2: Domestic activity and the labour market  44 
2.3: Wage growth and inflation  52 
2.4: Inflation expectations  61 
Box B: Uncertainties around official estimate s of labour market activity  65 
Box C: How has Blue Book 2023 changed past estimates of UK GDP growth?  70 
Box D: Agentsâ€™ update on business conditions  72 
3: In focus â€“ The outlook for demand  78 
3.1: Recent developments in GDP  78 
3.2: Key factors supporting the resilience in demand  79 
3.3: The impact of higher interest rates on demand  87 
3.4: Conclusion  99 
Annex: Other forecastersâ€™ expectations  100 
Glossary and other information  102 
Bank of England  
Page 3



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_30.txt
================================================
Sources: Bank of England, Bloomberg Finance L.P., Department for Energy Security and Net Zero, Eurostat, IMF World
Economic Outlook (WEO), National Bureau of Statistics of China, ONS, US Bureau of Economic Analysis and Bank
calculations.
(a) The profiles in this table should be viewed as broadly consistent with the MPCâ€™s projections for GDP growth, CPI
inflation and unemployment (as presented in the fan charts).
(b) Figures show annual average growth rates unless otherwise stated. Figures in parentheses show the corresponding
projections in the August 2023 Monetary Policy Report. Calculations for back data based on ONS data are shown using
ONS series identifiers.
(c) Chained-volume measure. Constructed using real GDP growth rates of 188 countries weighted according to their
shares in UK exports.
(d) Chained-volume measure. Constructed using real GDP growth rates of 189 countries weighted according to their
shares in world GDP using the IMFâ€™s purchasing power parity (PPP) weights.
(e) Chained-volume measure. Forecast was finalised before the release of the preliminary flash estimate of euro-area
GDP for Q3, so that has not been incorporated.
(f) Chained-volume measure. Forecast was finalised before the release of the advance estimate of US GDP  for Q3, so
that has not been incorporated. Revisions since August have led to changes in the historical data.
(g) Chained-volume measure. Constructed using real GDP growth rates of 155 emerging market economies, weighted
according to their relative shares in world GDP using the IMFâ€™s PPP weights.
(h) Chained-volume measure.
(i) Excludes the backcast for GDP.
(j) Chained-volume measure. Includes non-profit institutions serving households. Based on ABJR+HAYO.
(k) Chained-volume measure. Based on GAN8.
(l) Chained-volume measure. Whole-economy measure. Includes new dwellings, improvements and spending on
services associated with the sale and purchase of property. Based on DFEG+L635+L637.
(m) Chained-volume measure. The historical data exclude the impact of missing trader intra â€‘community (MTIC) fraud.
Since 1998 based on IKBK-OFNN/(BOKH/BQKO). Prior to 1998 based on IKBK.
(n) Chained-volume measure. The historical data exclude the impact of MTIC fraud. Since 1998 based on IKBL-
OFNN/(BOKH/BQKO). Prior to 1998 based on IKBL.
(o) Chained-volume measure. Exports less imports.
(p) Wages and salaries plus mixed income and general government benefits less income taxes and employeesâ€™ National
Insurance contributions, deflated by the consumer expenditure deflator . Based on [ROYJ+ROYH-(RPHS+AIIV-
CUCT)+GZVX]/[(ABJQ+HAYE)/(ABJR+HAYO)]. The backdata for this series are available at Monetary Policy Report â€“
Download chart slides and data â€“ November 2023.
(q) Total available household resources, deflated by the consumer expenditure deflator. Based on
[RPQK/((ABJQ+HAYE)/(ABJR+HAYO))].
(r) Annual average. Percentage of total available household resources. Based on NRJS.
(s) Level in Q4. Percentage point spread over reference rates. Based on a weighted average of household and
corporate loan and deposit spreads over appropriate risk-free rates. Indexed to equal zero in 2007 Q3.
(t) Annual average. Per cent of potential GDP. A negative figure implies output is below potential and a positive figure
that it is above.
(u) GDP per hour worked. Hours worked based on YBUS.
(v) Four-quarter growth in the ILO definition of employment in Q4. Up to June 2023, this projection is based on LFS
employment data (MGRZ). Beyond this point, the Committee is drawing on the collective steer from other indicators of
employment to inform its projection.
(w) Level in Q4. Average weekly hours worked, in main job and second job. Based on YBUS/MGRZ up to June 2023.
(x) ILO definition of unemployment rate in Q4. Up to June 2023, this projection is based on LFS unemployment data
(MGSX). Beyond this point, the Committee is drawing on the collective steer from other indicators of unemployment to
inform its projection.
(y) ILO definition of labour force participation in Q4 as a percentage of the 16+ population. Up to June 2023, this
projection is based on LFS participation data (MGWG).
(z) Four-quarter inflation rate in Q4.
Bank of England  
Page 30



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_31.txt
================================================
(aa) Four-quarter inflation rate in Q4 excluding fuel and the impact of MTIC fraud.
(ab) Contribution of fuels and lubricants and gas and electricity prices to four-quarter CPI inflation in Q4.
(ac) Four-quarter growth in whole â€‘economy total pay in Q4. Growth rate since 2001 based on KAB9. Prior to 2001,
growth rates are based on historical estimates of AWE, with ONS series identifier MD9M.
(ad) Four-quarter growth in unit labour costs in Q4. Whole â€‘economy total labour costs divided by GDP at constant
prices. Total labour costs comprise compensation of employees and the labour share multiplied by mixed income.
(ae) Four-quarter growth in private sector regular pay based unit wage costs in Q4. Private sector wage costs divided by
private sector output at constant prices. Private sector wage costs are average weekly earnings (excluding bonuses)
multiplied by private sector employment.
Bank of England  
Page 31



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_32.txt
================================================
Box A: Monetary policy since the August 2023 Report
At its meeting ending on 20 September 2023, the MPC voted by a majority of 5â€“4 to
maintain Bank Rate at 5.25%. Four members preferred to increase Bank Rate by
0.25 percentage points, to 5.5%. The Committee also voted unanimously to reduce
the stock of UK government bond purchases held for monetary policy purposes,
and financed by the issuance of central bank reserves, by Â£100 billion over the next
12 months, to a total of Â£658 billion.
Since the MPCâ€™s previous meeting, global growth had evolved broadly in line with
the August Report projections, albeit with some differences across regions. Spot oil
prices had risen significantly, while underlying inflationary pressures had remained
elevated across advanced economies.
UK GDP was estimated to have declined by 0.5% in July and the S&P Global/CIPS
composite output PMI fell in August, although other business survey indicators
remained consistent with positive GDP growth. While some of this news could
prove erratic, Bank staff expected GDP to rise only slightly in 2023 Q3. Underlying
growth in the second half of 2023 was also likely to be weaker than expected.
There had been some further signs of a loosening in the labour market, although it
remained tight by historical standards. The vacancies to unemployment ratio had
continued to decline, reflecting both a steady fall in the number of vacancies and
rising unemployment. The Labour Force Survey unemployment rate had risen to
4.3% in the three months to July, higher than expected in the August Report.
Indicators of employment had generally softened against the backdrop of subdued
activity.
Annual private sector regular average weekly earnings (AWE) growth had
increased to 8.1% in the three months to July, 0.8 percentage points above the
August Report projection. The recent path of the AWE was, however, difficult to
reconcile with other indicators of pay growth. Most of these had tended to be more
stable at rates of growth that were elevated but not quite as high as the AWE series.
Twelve-month CPI inflation fell from 7.9% in June to 6.7% in August, 0.4 percentage
points below expectations at the time of the Committeeâ€™s previous meeting. Core
goods CPI inflation had fallen from 6.4% in June to 5.2% in August, much weaker
than expected in the August Report. Services CPI inflation rose from 7.2% in June
Bank of England  
Page 32



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_33.txt
================================================
to 7.4% in July but declined to 6.8% in August, 0.3 percentage points lower than
expected in the August Report. Some of those movements were linked to services
such as airfares and accommodation that tend to be volatile over the summer
holiday period. Excluding these travel-related components, services inflation had
been more stable at continued high rates, albeit slightly weaker than expected.
CPI inflation was expected to fall significantly further in the near term, reflecting
lower annual energy inflation, despite the renewed upward pressure from oil prices,
and further declines in food and core goods price inflation. Services price inflation,
however, was projected to remain elevated in the near term, with some potential
month-to-month volatility.
Developments in key indicators of inflation persistence had been mixed, with the
acceleration in the AWE not apparent in other measures of wages and with some
downside news on services inflation. There were increasing signs of some impact of
tighter monetary policy on the labour market and on momentum in the real
economy more generally. Given the significant increase in Bank Rate since the start
of this tightening cycle, the current monetary policy stance was restrictive.
The MPC would continue to monitor closely indications of persistent inflationary
pressures and resilience in the economy as a whole, including the tightness of
labour market conditions and the behaviour of wage growth and services price
inflation. Monetary policy would need to be sufficiently restrictive for sufficiently long
to return inflation to the 2% target sustainably in the medium term, in line with the
Committeeâ€™s remit. Further tightening in monetary policy would have been required
if there were evidence of more persistent inflationary pressures.
Bank of England  
Page 33



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_34.txt
================================================
2: Current economic conditions
Global growth continues to be subdued, with stronger growth in the US of fset by
weaker growth in the euro area. Consumer price inflation remains elevated in
advanced economies, but it has been falling this year. Global export prices are
declining, reflecting lower energy prices, the continued clearing of supply chain
bottlenecks and weak producer price inflation. The paths for policy rates implied by
financial markets suggest rates are at or near their peaks in the UK, US and euro
area.
UK economic growth is slowing. Based on the steer from a range of business
surveys, the level of GDP is expected to increase only slightly over 2023 H2,
weaker than the growth over 2023 H1, and the projection in the August Report.
Some of this slowing is likely to reflect the impact of the tightening in monetary
policy that has been needed to combat high inflation.
The labour market remains tight but there are clear signs of loosening, with the
slowdown in output growth feeding into a softening of labour demand and an easing
of recruitment difficulties. Despite that loosening, all indicators suggest that nominal
wage growth is still very high. But the recent rise in the official measure of pay
growth is not matched by other wage data, and forward-looking indicators suggest
that wage growth will fall back in 2024.
Since the August Report, CPI inflation has fallen, dropping from 7.9% in June to
6.7% in September, 0.3 percentage points lower than the projection in August. It
remains well above the MPCâ€™s 2% target, however. Inflation is expected to fall
further to 4.6% in 2023 Q4 and 4.4% in 2024 Q1. The majority of that near-term
decline is accounted for by a falling contribution from household gas and electricity
bills. Falling input price inflation is likely to reduce both consumer goods price
inflation and food price inflation, while services price inflation is projected to remain
elevated in the near term.
Bank of England  
Page 34



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_35.txt
================================================
Chart 2.1: GDP growth is expected to be flat in 2023 H2, the unemployment rate is
expected to continue to drift up and inflation is expected to have fallen in October
Near-term projections (a)
Sources: ONS and Bank calculations.
(a) The lighter diamonds show Bank staffâ€™s projections at the time of the August 2023 Monetary Policy Report. The
darker diamonds show Bank staffâ€™s current projections. Projections for GDP growth and the unemployment rate are
quarterly and show 2023 Q3 and Q4 (August projections show Q2 to Q4). Projections for CPI inflation are monthly and
Bank of England  
Page 35



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_36.txt
================================================
2.1: Global developments and domestic credit conditions
Global growth has remained subdued over the course of 2023. UK-weighted world GDP  is
expected to have grown by around 0.4% in 2023 Q3, similar to Q2 and broadly in line with
the projection in the August Report. Four-quarter growth in 2023 Q3 is expected to be
around 1.5%, below its 2010â€“19 average of 2.4%. The latest indicators, such as cross-
country purchasing managersâ€™ indices (PMIs), suggest that global GDP growth is likely to
remain weak in Q4.show October to December 2023 (August projections show July to September 2023). GDP  growth rate 2023 Q3
projections are based on official data to August, the CPI inflation figure is an outturn. For unemployment, up to 2023 Q2
the series is based on official LFS data. Beyond this point, the Committee is drawing on the collective steer from other
indicators of unemployment to inform its projection.
Global growth remains subduedâ€¦
Chart 2.2: Global GDP growth continues to be subdued
UK-weighted world four-quarter GDP growth ( a)
Sources: Refinitiv Eikon from LSEG and Bank calculations.
(a) See footnote (c) of Table 1.D for definition. Figures for 2023 Q3 to 2024 Q2 are Bank staff projections. These
projections do not include the advance estimate of US 2023 Q3 GDP  and the preliminary flash estimate of euro area
2023 Q3 GDP, which were released after the data cut-off.
Bank of England  
Page 36



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_37.txt
================================================
Although aggregate global growth has evolved largely as expected, growth in major
economies has been diverging. In particular, US GDP growth has been stronger, while the
euro area has seen weaker GDP growth. Chinese GDP grew more rapidly than expected
in Q3, but has been weaker than before the Covid pandemic for the last few years.
US GDP rose by 1.2% according to the advance estimate for 2023 Q3, faster than
projected in the August Report. US growth has been around its pre-pandemic average in
recent quarters despite the sharp tightening in monetary policy. Household consumption
has grown more quickly than household incomes over 2022â€“23. This suggests that US
households in aggregate have been more willing to run down the significant increase in
savings accrued during the Covid pandemic (de Soyres et al (2023)  provide an
international comparison). The US has also been less exposed to global energy price
shocks than Europe, as it is not reliant on gas supplies from Russia. When wholesale gas
prices peaked in August 2022, European prices were over 10 times higher than in North
America (Broadbent (2023) ).
In the euro area, quarterly GDP fell by 0.1% in the 2023 Q3 preliminary flash estimate,
following weak growth of around 0.1% in both Q1 and Q2. Tighter monetary policy has
reduced growth, and retail energy prices remain high, despite having fallen since late
2022, which has weighed on real incomes and spending. Euro-area households have also
tended to maintain savings built up over the pandemic period, rather than spending them.
Recent analysis  by staff at the ECB (Battistini et al (2023) ) shows that most of the
increase in household savings has been invested in financial assets such as equities and
bonds, rather than in more liquid deposits. This suggests that household savings are less
likely to be run down in the near term.
In China, GDP grew by 1.3% in 2023 Q3, up from 0.5% in Q2. Even including 2023 Q3,
Chinese GDP growth has slowed since the Covid pandemic. Quarterly GDP growth has
averaged around 1% since 2021, whereas 1.5% or more was typical before the pandemic.
Other indicators of activity such as PMIs and retail sales have softened on the quarter .
Activity in the property sector, which plays a significant role in Chinaâ€™s economy, continued
to be weak, with property starts falling by more than 15% over the 12 months to
September. While only around 5% of UK exports go to China, its key role in global trade
means that the indirect effect, via mutual trading partners, of changes in Chinese demand
on the UK can be more significant. And there are other channels, for example via financial
markets, through which developments in China can affect the UK (Gilhooly et al (2018)).â€¦with notable regional differences.
Bank of England  
Page 37



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_38.txt
================================================
Headline inflation rates in the UK, euro area and US have all fallen this year . This largely
reflects lower energy price inflation, although food and goods price inflation have also
declined, particularly in the US. Nonetheless, inflation remains above central bank targets.
In the US, the annual rate of PCE inflation was 3.4% in September , while in the euro area
the HICP inflation rate fell to 2.9% in the flash estimate for October. Underlying inflationary
pressures have remained elevated across the three regions. Services inflation remains
high, as does wage growth. Measures such as the vacancies to unemployment ratio
suggest that labour markets remain tighter than before the pandemic, even with the recent
loosening in the US and in the UK.
Global export price inflation has eased markedly over the past year , with prices falling
across regions in 2023 Q2 (Chart 2.3). This reflects the indirect effects of lower energy
prices, the continued clearing of supply chain bottlenecks and weak producer price
inflation. Measures of shipping costs have stabilised around their levels before the
pandemic, having been significantly elevated in 2021â€“22.Consumer price inflation in advanced economies remains elevated, but it is
decliningâ€¦
â€¦while global export prices have fallen...
Chart 2.3: Global export price inflation has fallen significantly
Quarterly change in export prices (a)
Sources: ECB, General Administration of Customs of the Peopleâ€™s Republic of China, US Bureau of Economic Analysis,
other national statistical agencies, and Bank calculations.
(a) â€˜Rest of worldâ€™ is a weighted average of 31 countries, including China and excluding major oil exporters. Countries
are weighted by their share of UK imports. The final data points refer to 2023 Q2.
Bank of England  
Page 38



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_39.txt
================================================
While global energy prices remain significantly below the levels seen in 2022, they have
increased since the August Report. Spot oil prices reached around Â£75 per barrel in
September, and November 2023 futures prices have risen by about 20% since the August
Report (Chart 2.4).
Given the subdued outlook for global activity, demand factors are unlikely to have put
much upward pressure on oil prices. But oil supply has been reduced by cuts in
production by OPEC and Russia, amounting to two million barrels per day over 2023.
Wholesale gas spot prices have also risen, but remain significantly lower and less volatile
than in 2022. Wholesale gas futures prices are little changed since August (Chart 2.4).
European gas storage levels are high compared with recent years, reducing the likelihood
of shortages over the winter.
The risk of energy prices rising further has increased following recent events in the Middle
East (Section 1).â€¦although global energy prices have risen since August, and the risk of them
rising further has increased.
Chart 2.4: Oil prices have increased since the August Report, while gas futures
prices are little changed
UK wholesale gas and oil prices (a)
Sources: Bloomberg Finance L.P. and Bank calculations.
(a) Oil prices are Brent crude, converted to sterling. Gas prices are Bloomberg UK NBP  Natural Gas Forward Day price.
Dashed lines refer to respective futures curves using one-month forward prices based on the 15-day average to 24
October, while dotted lines are based on the 15-day average to 25 July. The final data points refer to futures curves at
December 2026.
Bank of England  
Page 39



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_4.txt
================================================
Monetary Policy Summary
The Bank of Englandâ€™s Monetary Policy Committee (MPC) sets monetary policy to meet
the 2% inflation target, and in a way that helps to sustain growth and employment. At its
meeting ending on 1 November 2023, the MPC voted by a majority of 6â€“3 to maintain
Bank Rate at 5.25%. Three members preferred to increase Bank Rate by 0.25 percentage
points, to 5.5%.
The Committeeâ€™s updated projections for activity and inflation are set out in the
accompanying November Monetary Policy Report. These are conditioned on a market-
implied path for Bank Rate that remains around 5Â¼% until 2024 Q3 and then declines
gradually to 4Â¼% by the end of 2026, a lower profile than underpinned the August
projections.
Since the MPCâ€™s previous meeting, long-term government bond yields have increased
across advanced economies. GDP growth has been stronger than expected in the United
States. Underlying inflationary pressures in advanced economies remain elevated.
Following events in the Middle East, the oil futures curve has risen somewhat while gas
futures prices are little changed.
UK GDP is expected to have been flat in 2023 Q3, weaker than projected in the August
Report. Some business surveys are pointing to a slight contraction of output in Q4 but
others are less pessimistic. GDP is expected to grow by 0.1% in Q4, also weaker than
projected previously.
The MPC continues to consider a wide range of data to inform its view on developments
in labour market activity, rather than focusing on a single indicator. The increasing
uncertainties surrounding the Labour Force Survey underline the importance of this
approach. Against a backdrop of subdued economic activity, employment growth is likely
to have softened over the second half of 2023, and to a greater extent than projected in
the August Report. Falling vacancies and surveys indicating an easing of recruitment
difficulties also point to a loosening in the labour market. Contacts of the Bankâ€™s Agents
have similarly reported an easing in hiring constraints, although persistent skills shortages
remain in some sectors.
Pay growth has remained high across a range of indicators, although the recent rise in the
annual rate of growth of private sector regular average weekly earnings has not been
apparent in other series. There remains uncertainty about the near-term path of pay, but
Bank of England  
Page 4



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_40.txt
================================================
Over the past two years, central banks in the UK, US and euro area have tightened policy
(Chart 2.5). Since August, the ECB Governing Council raised its key policy rates by 25
basis points in September and held rates on 26 October, leaving the deposit facility rate at
4%. In the US, the FOMC has kept the target range for the federal funds rate unchanged
at 5.25%â€“5.5%.
In both the US and euro area, market-implied paths for policy rates are consistent with no
further increases in this tightening cycle. Since the August Report, the market-implied path
for policy rates in the euro area is little changed, while the path for US rates is, on
average, around 50 basis points higher (Chart 2.5).
Since the August Report, market expectations of UK policy rates have fallen by about 60
basis points over the next three years, on average (Chart 2.5). The market curve has also
flattened. The UK curve remains broadly above the US and euro area, although the gap
has narrowed.Market expectations suggest policy rates are at or near their peaks.
Chart 2.5: Interest rates are at or close to the peak of their market-implied paths in
the US, euro area and UK
Policy rates and forward curves for the US, euro area and UK (a)
Sources: Bloomberg Finance L.P. and Bank calculations.
(a) All data as of 24 October 2023. The August curves are estimated based on the 15 UK working days to 25 July 2023.
The November curves are estimated using the 15 working days to 24 October 2023. Federal funds rate is the upper
bound of the announced target range. ECB deposit rate is based on the date from which changes in policy rates are
effective. The final data points refer to futures curves at September 2026.
UK policy rate expectations have fallen since August...
Bank of England  
Page 40



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_41.txt
================================================
Partly reflecting stronger demand in the US leading the dollar to appreciate, and a smaller
differential between expected policy rates in the UK and the US and euro area, the
sterling effective exchange rate has depreciated by about 2.3% since the August Report.
Sterling has fallen by 5.5% against the dollar, and by 1% against the euro.
Yields on 10-year government bonds have risen since August across advanced
economies. This partly reflects market expectations that policy rates will remain higher for
longer. But models used by Bank staff to analyse movements in long-term bond yields
suggest it also reflects a rise in term premia â€“ the compensation that investors require for
the risks associated with holding government bonds of longer duration.
Fixed-rate mortgages make up around 80% of the stock of UK mortgages, and the rates
on new fixed-rate mortgages are influenced by expectations of Bank Rate (Box C of theFebruary 2022 Report ). Since August, quoted fixed-rate mortgage rates have fallen,
largely reflecting pass-through of lower reference rates, but they remain substantially
higher than at the start of the Bank Rate tightening cycle (Chart 2.6). Analysis by Bank
staff has found that the pass-through from reference rates to mortgage rates has occurred
largely as expected during this period as a whole (Box B of the May 2023 Report ).
Interest rates on new fixed-rate savings bonds, which, like new fixed-rate mortgage rates,
are influenced by expectations of Bank Rate, have ticked down slightly in the most recent
data. Instant access savings accounts tend to be priced relative to current Bank Rate, and
pass-through from previous increases in Bank Rate has been slow. However, the recent
data suggest this has accelerated, with average quoted rates increasing by around 90
basis points between June and October, although the spread between instant access
deposit rates and Bank Rate remains wide (Chart 2.6). Some of this acceleration may be
driven by banks and building societies responding to the FCAâ€™s action plan on cash
savings , which aims to ensure firms appropriately reflect changes in Bank Rate in the
rates they offer to depositors.â€¦and UK fixed-rate mortgage rates have declined.
Bank of England  
Page 41



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_42.txt
================================================
Interest rates on bank loans to UK businesses have been rising, reflecting the increase in
Bank Rate. In the latest data for August, the interest rate on new bank loans stood at
nearly 7%, compared to around 2% at the start of the tightening cycle.
Larger companies are able to borrow from capital markets, for example by issuing bonds.
While spreads over risk-free rates on both high-yield and investment-grade corporate
bonds have narrowed by around 90 basis points since late last year , higher risk-free rates
have increased the overall cost of issuing bonds.
The availability of mortgage credit declined in 2023 Q3, continuing the trend of the last
two years, according to the latest Credit Conditions Survey (CCS). Tighter wholesale
funding conditions and expectations for house prices were the most significant factors
reported to be reducing credit availability. The availability of unsecured credit fell slightly
on the quarter, although lenders expected little change over the next three months.Chart 2.6: Rises in reference rates have fed through to mortgage rates, and
increasingly deposit rates
Average quoted interest rates on two-year fixed-rate mortgages, fixed-rate savings bonds,
instant access accounts, and the respective reference rates (a)
Sources: Bank of England, Bloomberg Finance L.P. and Bank calculations.
(a) The reference rate for mortgages and fixed-rate savings bonds is the two-year overnight index swap (OIS) rate. The
Bankâ€™s quoted rates series are weighted monthly average rates advertised by all UK banks and building societies with
products meeting the specific criteria. In February 2019 the method used to calculate these data was changed. For
more information, see Introduction of new Quoted Rates data â€“ Bankstats article. Diamonds for mortgage and
saving products represent averages of daily quoted rates using data to 24 October and were provisional. OIS rate and
Bank Rate show monthly averages and the respective diamonds show the average of daily rates to 24 October .
Credit availability is tightening, reflecting increased credit risk.
Bank of England  
Page 42



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_43.txt
================================================
Businesses are reporting a decline in credit availability. The credit availability balance in
the Deloitte CFO Survey, which covers large companies, has fallen, although it remains
significantly above the levels seen during the global financial crisis. Data from the latest
Federation of Small Businesses Index suggest that credit availability is tightening for
SMEs, although fewer than half of UK SMEs use any external financing, according to the
2023 Q2 BVA BDRC SME Finance Monitor. In the CCS, lenders reported that credit
availability to the corporate sector was largely unchanged in 2023 Q3.
The FPC has judged that the tightening of lending standards seen over recent quarters
reflects increased credit risk, rather than defensive actions by banks to protect their
capital positions (Financial Policy Summary and Record â€“ October 2023).
Growth in mortgage lending has been falling, and is approaching the low rates seen
immediately following the global financial crisis. This lower growth has been driven by a
reduction in gross mortgage lending, with repayments by mortgagors broadly flat.
Mortgage approvals have been somewhat lower than the 2010â€“19 average, suggesting
subdued lending volumes will continue in the near term. Section 3.3 discusses the impact
of higher interest rates on the housing market, and how that affects the outlook for
demand.
Net finance raised by companies continues to be weak, driven in part by subdued demand
for credit. A majority of lenders in the CCS reported that demand for corporate lending had
contracted in 2023 Q3, continuing a broad trend over the previous four quarters. In the
year to September, UK non-financial companies made net repayments of around Â£17
billion of financing, well below the 2010â€“19 average for the same point in the year of a
roughly Â£10 billion increase in borrowing. Weakness in net bank lending is particularly
pronounced among SMEs. Much of this is due to continued repayment of borrowing under
Covid loan schemes.
Broad money growth has continued to slow. The annual growth rate fell to -4.2% in
September. The sharp decline in the latest month largely reflects base effects related to a
large increase in broad money one year ago, associated with developments at liability-
driven investment funds. The growth rate of M4 excluding other financial corporations fell
by less, to -0.7% (Chart 2.7). More generally, money growth is well below the 2010â€“19
average of 3.8% and has decelerated significantly since 2020â€“21. The slowing in moneyCredit volumes are subdued for both households and businesses, in part
reflecting weak demand for credit.
Money growth has continued to slow.
Bank of England  
Page 43



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_44.txt
================================================
growth has been driven by a reduction in net lending by banks, particularly in net lending
to other financial institutions, as well as sales from the Bankâ€™ s asset purchase facility,
which tend to reduce the level of bank deposits.
2.2: Domestic activity and the labour market
Latest estimates suggest GDP increased by 0.2% in 2023 Q2, down a little from 0.3%
growth in Q1 but marginally stronger than expected in the August Report. Household
consumption and business investment boosted headline GDP, while housing investment
dragged on growth (Chart 2.8). Although Q2 GDP grew only slightly on the previous
quarter, the latest annual ONS Blue Book revisions shifted up the level of GDP by around
2% (Box C).Chart 2.7: Aggregate money and lending growth have slowed
Twelve-month growth rate of aggregate money and lending (a)
(a) Aggregate money, excluding other financial corporations refers to M4 excluding other financial corporations (OFCs).
Aggregate money and aggregate lending refer to M4 and M4 lending respectively , both excluding intermediate other
financial corporations (IOFCs). OFCs are corporations engaged in financial services that are not banks nor building
societies, for example insurance companies and pension funds. IOFCs are specialised entities that mainly provide
intermediation services to banks and building societies. Only quarterly data are available for the aggregate money and
aggregate lending series from 1998 Q4 to 2010 Q2. All other data are at a monthly frequency. The final data points refer
to September 2023. For more information on these data, see Further details about M4 data, Further details about
M4 excluding intermediate other financial corporations (OFCs) data  and Further details about M4 lending
excluding lending to intermediate other financial corporations data .
UK GDP growth averaged around Â¼% per quarter over 2023 H1.
Bank of England  
Page 44



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_45.txt
================================================
Based on ONS data to August, UK GDP is expected to have been flat in 2023 Q3, weaker
than expected in August. Output growth was volatile within the quarter, in part reflecting
the impact of strikes and poor weather. Monthly output was estimated to have fallen by
0.6% in July before rebounding slightly in August, increasing by 0.2% on the month.
Breaking down GDP by expenditure category (Chart 2.8), housing investment is expected
to have continued to contribute negatively to growth in 2023 Q3, as higher interest rates
weighed on house building and housing transactions. Business investment is also
expected to have dragged on growth: strong business investment in the first half of the
year was partly driven by volatile components such as aircraft investment, and that is
expected to unwind. Household consumption is expected to have continued to contribute
positively to GDP growth, though to a lesser extent than in 2023 H1. Positive real income
growth is expected to have provided some support to household spending, while higher
interest rates have reduced consumption through a range of channels (Section 3).Chart 2.8: GDP is expected to be broadly flat in 2023 H2
Contributions to quarterly GDP growth (a)
Sources: ONS and Bank calculations.
(a) Diamonds show quarterly headline GDP growth. Figures for 2023 Q3 and Q4 are Bank staff projections.
Economic activity is expected to be flat in Q3.
Bank of England  
Page 45



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_46.txt
================================================
The S&P Global/CIPS UK composite current output PMI has been below the 50 â€˜no
changeâ€™ mark for several months, consistent with a fall in Q4 GDP based on historical
relationships (Chart 2.9). The S&P Global/CIPS UK composite new orders and new export
orders have also been weak.
Contacts of the Bankâ€™s Agentsâ€™ report subdued demand and growing concerns about the
economic outlook, most notably from contacts in consumer-facing sectors.
Some more forward-looking business survey indicators  suggest GDP growth could be
more resilient. The S&P Global/CIPS UK future output PMI series, which asks firms about
their expectations for output in 12 monthsâ€™ time, is only a little below its long-run averageSome business surveys point to a fall in output in Q4â€¦
Chart 2.9: Some survey measures suggest economic activity growth will weaken in
2023 H2, while forward-looking measures are less pessimistic
Survey indicators of UK output growth (a)
Sources: S&P Global/CIPS and Bank calculations.
(a) A reading of above 50 indicates an increase on the previous month while a reading below 50 indicates a fall. Dashed
lines represent the long-run series averages, calculated from January 1998 for the current output and new orders series
and July 2012 for the output expectations series. Latest data are flash estimates for October 2023.
â€¦but some more forward-looking business survey indicators are less pessimistic
about growth prospects.
Bank of England  
Page 46



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_47.txt
================================================
(Chart 2.9). Other similarly forward-looking business activity indicators, such as the latest
Lloyds Business Barometer and the British Chambers of Commerceâ€™s Quarterly
Economics Survey, have also remained consistent with positive GDP growth.
Combining the steers from a range of business surveys, among which the near-term
output balances get the greatest weight, Bank staff expect GDP growth of 0.1% in 2023
Q4. The projection for broadly flat output over the second half of 2023 is weaker than the
August projection of around Â¼% growth per quarter. The anticipated softening in growth
chimes with the weakening observed across several other near-term indicators, such as
those for employment growth, the housing market and global activity (Section 2.1).
Timely measures of labour demand have softened against a backdrop of subdued activity.
Vacancies edged down further from their 2022 peaks in the three months to September.
The S&P Global/CIPS UK composite employment PMI has also been in contractionary
territory for a couple of months (Chart 2.10). The permanent staff placements index from
the KPMG/REC UK Report on Jobs was below its historical average in September .
Official estimates of employment growth have also weakened. The most recent ONS
Workforce Jobs and HMRC payrolls data point to a modest but positive quarterly growth in
employment in Q3, and a small contraction in employment growth in Q4. There have been
notable uncertainties surrounding recent Labour Force Survey (LFS) estimates. These
have made the data harder to interpret and have resulted in the ONS temporarily pausing
its publication of LFS estimates following the June data (Box B). Alternative experimental
statistics published by the ONS, which take the LFS employment estimate in the three
months to June and project it forward in line with the HMRC payrolls data thereafter ,
suggest that employment fell by 0.2% in the three months to August. As experimental
statistics, these estimates need to be interpreted with caution.Overall, Bank staff project a 0.1% increase in GDP in Q4. The 2023 H2 projection is
weaker than expected in the August Report.
The softening in activity is affecting labour demand.
Bank of England  
Page 47



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_48.txt
================================================
Bank staff take a combined steer from a wide range of indicators to inform their view
about underlying employment growth. Daniell and Moreira (2023) describes how these
indicators feed into the near-term forecast and the evolution of this indicator-based model
forecast over time is shown in the aqua line in Chart 2.11. The indicator-based model
suggests that employment growth has slowed gradually since end-2021. The latest staff
projections suggest employment will be broadly flat over the second half of 2023.Chart 2.10: Most indicators of employment growth are softening
Indicators of employment growth (a) (b)
Sources: Bank of England Agents, HMRC, KPMG/REC/S&P Global UK Report on Jobs, Lloyds Business Barometer,
ONS, S&P Global/CIPS and Bank calculations.
(a) ONS employment growth is the change in headline employment level for people aged 16+ over the value three
months earlier. Employment indicators include data from: ONS/HMRC Pay As You Earn (the three-month change in the
monthly number of PAYE employees), the Bankâ€™s Agents (employment intentions over the next six months);
KPMG/REC/S&P Global (weighting together the temporary and permanent staff placements series); Lloyds Business
Barometer (balance of higher staffing levels over next 12 months); and S&P Global/CIPS (PMI composite employment
index). The surveys and Agentsâ€™ scores have varying samples and questions but have been mean and variance
adjusted to match the ONS employment growth series between 2000 and 2019, and are therefore shown consistent
with the three-month on three-month growth rate. The final data point for ONS employment growth is the three months
to August 2023.
(b) The LFS employment growth series shown here uses official LFS estimates to June 2023 and thereafter uses the
ONSâ€™s experimental alternative labour market statistics (based on HMRC payrolls data). See Box B for further
information.
Bank of England  
Page 48



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_49.txt
================================================
The ONSâ€™s estimate of the proportion of working-age people in the labour force is also
affected by the uncertainties surrounding the LFS estimates, with the participation rate
probably overestimated (Box B). These issues notwithstanding, the rise in participation
since mid-2022 has been partly driven by a return of students to the labour force. Those
reporting they are outside the labour force due to looking after family or the home and
retirement has also declined. The number of people saying they are unable to participate
in the workforce due to sickness remains elevated, however, and is just over half a million
higher than before the pandemic.
A range of evidence points to the labour market having loosened, consistent with a
restrictive stance of monetary policy. The ONS vacancies to unemployment ratio, a key
measure of labour market tightness, has been falling since August 2022. This reflects both
a steady fall in the number of vacancies and rising unemployment (right panel of ChartChart 2.11: An indicator-based model points to flat employment in 2023 Q4
Measures of quarterly employment growth (a) (b)
Sources: Bank of England Agents, HMRC, KPMG/REC/S&P Global UK Report on Jobs, Lloyds Business Barometer,
ONS, S&P Global/CIPS and Bank calculations.
(a) LFS employment growth is the change in headline employment level for people aged 16+ over the value in the
previous quarter. Latest data point is for 2023 Q2.
(b) Bank staffâ€™s indicator-based model of near-term employment growth uses mixed-data sampling (or MIDAS)
techniques (see Daniell and Moreira (2023)  for more detail). A range of indicators inform the model, including series
from the Bank of England Agents, the Lloyds Business Barometer, ONS/HMRC PAYE payrolls, S&P Global/CIPS
purchasing managersâ€™ index and the KPMG/REC UK Report on Jobs. Indicators are weighted together according to
their relative forecast performance in the recent past. Diamonds represent projections for 2023 Q3 and Q4.
Labour force participation has increased from its mid-2022 trough.
The labour market is loosening, and by a little more than projected in the August
Reportâ€¦
Bank of England  
Page 49



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_5.txt
================================================
wage growth is nonetheless projected to decline in coming quarters from these elevated
levels.
Twelve-month CPI inflation fell to 6.7% both in September and 2023 Q3, below
expectations in the August Report. This downside news largely reflects lower-than-
expected core goods price inflation. At close to 7%, services inflation has been only
slightly weaker than expected in August. CPI inflation remains well above the 2% target,
but is expected to continue to fall sharply, to 4Â¾% in 2023 Q4, 4Â½% in 2024 Q1 and 3Â¾%
in 2024 Q2. This decline is expected to be accounted for by lower energy, core goods and
food price inflation and, beyond January, by some fall in services inflation.
In the MPCâ€™s latest most likely, or modal, projection conditioned on the market-implied
path for Bank Rate, CPI inflation returns to the 2% target by the end of 2025. It then falls
below the target thereafter, as an increasing degree of economic slack reduces domestic
inflationary pressures.
The Committee continues to judge that the risks to its modal inflation projection are
skewed to the upside. Second-round effects in domestic prices and wages are expected
to take longer to unwind than they did to emerge. There are also upside risks to inflation
from energy prices given events in the Middle East. Taking account of this skew, the mean
projection for CPI inflation is 2.2% and 1.9% at the two and three-year horizons
respectively. Conditioned on the alternative assumption of constant interest rates at
5.25%, which is a higher profile than the market curve beyond the second half of 2024,
mean CPI inflation returns to target in two yearsâ€™ time and falls to 1.6% at the three-year
horizon.
The MPCâ€™s remit is clear that the inflation target applies at all times, reflecting the primacy
of price stability in the UK monetary policy framework. The framework recognises that
there will be occasions when inflation will depart from the target as a result of shocks and
disturbances. Monetary policy will ensure that CPI inflation returns to the 2% target
sustainably in the medium term.
Since the MPCâ€™s previous decision, there has been little news in key indicators of UK
inflation persistence. There have continued to be signs of some impact of tighter monetary
policy on the labour market and on momentum in the real economy more generally . Given
the significant increase in Bank Rate since the start of this tightening cycle, the current
monetary policy stance is restrictive. At this meeting, the Committee voted to maintain
Bank Rate at 5.25%.
Bank of England  
Page 5



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_50.txt
================================================
2.12). The ONSâ€™s alternative experimental statistics, which take the LFS unemployment
estimate in the three months to June and project it forward in line with the claimant count
data thereafter, suggest that the unemployment rate may have increased to 4.2% in the
three months to August. This would be higher than expected in the August Report. The
unemployment rate is projected to rise a little further in 2023 Q4.
While there are significant uncertainties around the LFS data at present (Box B), other
indicators are also indicative of a loosening in the labour market. Recent KPMG/REC
Report on Jobs surveys have pointed to some easing of recruitment dif ficulties and a pick-
up in staff availability. This is consistent with evidence from the ONSâ€™s Business Insights
and Conditions Survey, in which the number of firms reporting difficulties recruiting has
trended down in recent months. The latest recruitment difficulties score from the Bankâ€™s
Agents has also fallen back from its peak in mid-2022, though it remains above its
historical average. Despite the reported easing in hiring constraints, Agentsâ€™ contacts
reported that persistent skills shortages remain in some sectors.
Despite evidence that the labour market is loosening, it remains tight in a historical
context. The vacancies to unemployment ratio is still elevated, and above its 2019 Q4
level (left panel of Chart 2.12). And it is noteworthy that the period immediately before the
pandemic was one where there was considered to be little spare capacity in the labour
market (see November 2019 Report ).
Another measure of labour market tightness is the gap between the current
unemployment rate and the equilibrium rate of unemployment. The equilibrium rate of
unemployment â€“ which is not observable and has to be estimated â€“ is defined as the rate
consistent with meeting the inflation target in the medium term (see Box 4 of the February
2018 Report ). If the unemployment rate is below this equilibrium rate that tends to put
upwards pressure on wage growth and inflation, as companies need to pay more to
recruit suitably skilled staff.
The long-term equilibrium unemployment rate changes only slowly over time, determined
by structural features of the economy that affect the time it takes for people to find the
right jobs. The MPCâ€™s latest estimate suggests that the long-term equilibrium
unemployment rate is just above 4% (see Section 3 of the February 2023 Report ). The
Committee will revisit this estimate as part of its forthcoming supply stocktake.
The medium-term equilibrium unemployment rate is more relevant for determining the
degree of slack in the labour market and hence wage pressures, however . It can be
affected by cyclical factors, such as changes in the mix of jobs and job seekers. Wageâ€¦but it remains tight by historical standards.
Bank of England  
Page 50



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_51.txt
================================================
growth has been higher than standard models would have predicted (Chart 2.14), which
could be explained by a rise in the medium-term equilibrium unemployment rate. For
example, the rate may have increased if employees and domestic firms have sought
compensation in the form of higher nominal pay and domestic selling prices for the
reductions in real incomes that they have experienced after the terms of trade shock.
There is also some evidence that the efficiency with which vacancies are matched to
those seeking work has decreased in recent years. In its November forecast, the MPC
has made a judgement to further increase the medium-term equilibrium unemployment
rate since the sharp rise in energy prices â€“ it is judged to be around 4Â½% currently (Key
judgement 2 in Section 1).Chart 2.12: The labour market remains tight, although it has loosened since mid-
2022
Vacancies to unemployment ratio and contributions to changes in vacancies to unemployment
ratio since 2019 Q4 (a)
Sources: ONS and Bank calculations.
(a) Latest data points are for the three months to August 2023. The LFS unemployment series shown in these charts
use the official LFS estimates to June 2023 and thereafter use the ONSâ€™s experimental alternative labour market
statistics (based on claimant count data). See Box B for further information.
Bank of England  
Page 51



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_52.txt
================================================
2.3: Wage growth and inï¬‚ation
Alongside labour market tightness and services CPI inflation, the MPC monitors various
measures of wage growth as they could indicate independent evidence of more persistent
domestic inflationary pressures. The ONS measure of annual private sector regular
average weekly earnings (AWE) growth was 8.0% in August. This was 0.8 percentage
points higher than expected in the August Report, largely accounted for by upward
revisions to previous monthsâ€™ data. This measure had been on an upward trend between
February and June, before falling back slightly in July and August.
Other pay indicators have been more stable at rates of growth that are also high, but do
not show a further rise in recent months (Chart 2.13). According to the DMP, annual pay
increases have been steady at around 7% between April and October. HMRCDespite a loosening labour market, nominal wage growth is still high.
Chart 2.13: Annual private sector regular pay growth stood at 8.0% in August,
higher than other indicators
Measures of annual private sector wage growth (a)
Sources: DMP Survey, Indeed Hiring Lab, ONS and Bank calculations.
(a) The adjusted HMRC Real Time Information (RTI) measure strips out pay in sectors with a high share of public
workers, such as public administration and defence, social security , education, health and social work, to proxy private
sector wage developments. In contrast to the AWE measure of private sector regular pay, RTI data include bonus
payments. The latest data are for August 2023 (ONS private sector regular pay), September 2023 (HMRC RTI and
Indeed Wage Tracker) and October 2023 (Bank DMP) respectively.
While all measures of pay growth are elevated, the recent rise in the annual rate of
growth of private sector regular AWE is not matched by other indicators.
Bank of England  
Page 52



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_53.txt
================================================
administrative data on payrolls suggest that median private sector pay growth was broadly
flat at around 7% in the months leading up to September. The Indeed Wage Tracker,
which measures the average annual change in the wages stated in job adverts, points to
wage growth falling back slightly to around 7% between April and September. Meanwhile,
contacts of the Bankâ€™s Agents continue to report that average annual pay settlements
were in the region of 6% to 6.5%.
The AWE data are based on the Monthly Wages and Salaries Survey (MWSS), covering a
representative sample of 9,000 firms with more than 20 employees. The MWSS is
separate from the LFS and is not subject to falling response rates because firms are
legally obliged to respond. Some of the recent divergence between the AWE measure and
the other indicators could reflect sampling variability or differences in methodology and
data coverage. For example, the MWSS and other surveys can be af fected by differences
in the characteristics of surveyed firms and the full population of firms, while the HMRC
RTI estimates are based on the PAYE system that collects income tax and national
insurance from employment and therefore covers all firms. In addition, in contrast to
HMRC RTI median pay growth, the AWE measure of average pay growth will be sensitive
to changes in pay across the whole wage distribution.
The MPC will continue to monitor all data on pay growth. While the recent rise in the
annual rate of growth of private sector regular AWE is not mirrored in other indicators, all
pay measures continue to signal that wage growth is high and, if sustained, not consistent
with inflation returning to target in the medium term.
Labour market tightness has played a role in elevated nominal wage growth to date, and
the August 2023 Report  highlighted tentative evidence that it explains some of the wide
variation in pay growth across sectors. The unexplained strength in pay growth recently
may reflect a rise in the medium-term equilibrium unemployment rate (Section 2.2), as
well as stronger second-round effects of external cost shocks on inflation in wages and
domestic prices. With the labour market now loosening, and inflation set to moderate,
some of the upward pressures on wage growth should ease in 2024.
Wage growth tends to rise when headline inflation and inflation expectations increase.
Near-term inflation expectations in particular tend to be strongly correlated with current
headline inflation (Chart 2.19). Rising headline inflation, and the associated increase inA looser labour market and falling inflation are expected to contribute to a
moderation in wage growthâ€¦
Bank of England  
Page 53



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_54.txt
================================================
near-term inflation expectations, appear to have played a role in supporting wage growth
since 2021 (Chart 2.14). As headline inflation and near-term inflation expectations have
started to fall, they are likely to exert less upward pressure on pay growth in 2024.
A number of forward-looking indicators suggest that nominal wage growth is likely to
moderate in 2023 Q4 and in 2024. Contacts of the Bankâ€™s Agents expect settlements to
fall to around 4% to 5% next year, and for there to be fewer additional payments provided
to compensate for a higher cost of living. Respondents to the DMP  Survey expect wage
growth of 5.1% next year.Chart 2.14: Easing labour market tightness and falling inflation expectations
should reduce pay growth in the near term
Contributions to annual private sector regular pay growth (a)
Sources: ONS and Bank calculations.
(a)  Wage equation based on Yellen (2017) . Private sector regular pay growth is Bank staffâ€™s estimate of underlying pay
growth between January 2020 and March 2022 and ONS private sector regular pay growth otherwise. Short-term
inflation expectations are based on the Barclays Basix Index and the YouGov/Citigroup one year ahead measure of
household inflation expectations and projected forward based on a Bayesian V AR estimation. Slack is based on the
MPCâ€™s estimate of the vacancies to unemployment ratio. Productivity growth is based on long-run market sector
productivity growth per head. The unexplained component is the residual. Data are to 2023 Q2, projections are for 2023
Q3 to 2024 Q1.
â€¦and forward-looking indicators suggest that wage growth will fall back.
Bank of England  
Page 54



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_55.txt
================================================
Measures of staff salaries for new hires within the KPMG/REC Report, which have in the
past been a strong predictor of aggregate private sector pay growth, returned to levels
close to their historical averages this year and continued to fall in September (Chart 2.15).
This might indicate that aggregate pay growth is likely to return to levels consistent with
the inflation target next year. However, the predictive power of these data has been less
strong over recent months. Labour hoarding following a period of acute recruitment
difficulties could explain the strength of pay growth for current employees relative to new
hires. Changes in how frequently people move jobs might also affect the relationship
between the salaries of new hires in the REC survey and the official wage data, although
staff analysis suggests that the impact of this on the current outlook is small.Chart 2.15: Forward-looking indicators suggest pay growth could slow markedly in
2024
Measures of annual wage growth (a)
Sources: DMP Survey, KPMG/REC/S&P Global UK Report on Jobs, ONS and Bank calculations.
(a) Definitions of wage growth vary between each of the measures. Private sector regular pay growth is Bank staf fâ€™s
estimate of underlying pay growth between January 2020 and November 2022 and ONS private sector regular pay
growth otherwise. REC shows average starting salaries for permanent staf f compared to the previous month. The REC
index is mean-variance adjusted to ONS private sector regular pay growth over March 2001â€“19 and is advanced by 12
months, which coincides with the greatest correlation with private sector regular pay growth. The Agentsâ€™ contacts
expected range is based on early indications on pay settlements in 2024. Latest data points are September 2023 for the
REC index, and the three months to August 2023 for private sector regular pay. Pay growth projections are for 2023 Q4
and 2024 Q1.
Bank of England  
Page 55



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_56.txt
================================================
Accumulating evidence of loosening labour market conditions and the signals from
leading indicators of pay growth underpin the central projection for wage growth, which
falls slightly to 7Â¼% in Q4, before declining more substantially to around 5% by the end of
2024, predicated on the expected path for inflation. The recent pattern of upside surprises
in the official wage data may point to some upside risk to this projection, whereas the REC
survey continues to suggest risks to the downside.
Twelve-month consumer price inflation fell to 6.8% in July before edging down further to
6.7% in August and remaining at 6.7% in September (Chart 2.16). This was 0.3
percentage points below the August Report forecast. Declines in inflation up to July had
been driven largely by lower energy prices. But in August, core CPI inflation, which
excludes energy, food, beverages and tobacco, fell to 6.2% and further to 6.1% in
September, having been relatively stable at just under 7% in preceding months. The
decline in core inflation was driven by core goods inflation, which stood at 4.7% in
September, 1 percentage point below the August forecast.Overall, annual private sector regular pay growth is projected to fall to around
7Â¼% in Q4 before declining quite markedly in 2024, but the outlook remains highly
uncertain.
Consumer price inflation is falling, but remains well above the 2% inflation target.
Bank of England  
Page 56



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_57.txt
================================================
Inflation is expected to fall to 4.8% in October and remain around that level for the rest of
the year. The main driver of the expected fall in Q4 is a reduction in the Ofgem energy
price cap, reflecting the decline in wholesale gas prices over the course of 2023. The
typical household energy bill is going down to Â£1,834 annually at that point. Because the
typical bill rose to Â£2,500 over the same period a year ago, the lower level of the cap this
year will have a material impact on the annual inflation rate in 2023 Q4. Based on
wholesale gas prices up to 24 October 2023, the energy price cap is projected to rise
again by around Â£130 in 2024 Q1. This would remain well below the typical bill in the
same quarter this year, so energy prices would continue to contribute negatively to the
annual inflation rate.
Sterling oil prices have risen by 10% since the August Report (Section 2.1). Higher
sterling oil prices feed through to petrol prices relatively quickly . Fuel prices are still
expected to contribute negatively to CPI inflation in 2023 Q4, but that contribution is
smaller than in the August forecast.Chart 2.16: Consumer price inflation has fallen since last yearâ€™s peak and is
projected to fall further
Contributions to CPI inflation (a)
Sources: Bloomberg Finance L.P., Department for Energy Security and Net Zero, ONS and Bank calculations.
(a) Figures in parentheses are CPI basket weights in 2023. Data to September 2023. Bank staf f projections from
October 2023 to March 2024. Fuels and lubricants estimates use Department for Energy Security and Net Zero petrol
price data for October 2023 and then are based on the sterling oil futures curve.
Inflation is expected to fall markedly in October to 4.8%, largely reflecting a
reduction in the Ofgem price cap.
Bank of England  
Page 57



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_58.txt
================================================
Food price inflation, which has a large impact on the living costs of lower-income families
because it makes up a larger share of these familiesâ€™ budgets, remains high. The annual
rate peaked at 19.1% in March and has since fallen back a little faster than expected in
the August Report, to 12.1% in September. Input prices continue to ease, but will take
time to transmit through the supply chain. Food price inflation is expected to fall to around
9% in 2023 Q4 and to around 5% in 2024 Q1, which remains broadly in line with the
intelligence gathered from contacts in the food sector reported in Box D of the August
Report.
Goods price inflation has moderated more quickly than expected in the August Report.
Some of the recent fall in goods price inflation has reflected developments in used car
prices, which tend to be driven by idiosyncratic factors. But there was also broader
downside news across a number of goods categories, with core goods inflation falling to
4.7% in September. Core goods price inflation is projected to fall further to 2.4% by March
2024, contributing to the expected reduction in headline inflation (Chart 2.16).
Easing input cost pressures are expected to continue to reduce consumer goods price
inflation in the coming months. Changes in producer price inflation tend to lead changes in
consumer goods price inflation by a few months. Output producer price inflation, which
measures the change in the price of goods sold by UK manufacturers, has slowed
significantly since its peak in mid-2022 (Chart 2.17).CPI inflation is expected to fall further to 4.4% in 2024 Q1. That mainly reflects
lower goods price inflation, as firms are expected to pass on lower producer price
inflationâ€¦
Bank of England  
Page 58



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_59.txt
================================================
Alongside labour market tightness and wage growth, services CPI inflation is one of the
MPCâ€™s key indicators of domestic inflationary pressure. Services inflation stood at 6.9% in
September, 0.1 percentage points below the August forecast. There was some volatility in
services inflation over the summer due to travel-related components such as airfares and
accommodation. Excluding these components, services inflation has been more stable
since April. An underlying measure of services inflation produced by Bank staff, which is
determined by the comovement of price changes across services components to strip out
idiosyncratic fluctuations, has begun to fall back slightly in recent months but remains high
(Chart 2.18).
Services inflation is expected to remain broadly stable throughout 2023 Q4, before
increasing temporarily in January 2024. Large and unusual falls in a number of services
prices at the beginning of 2023 are unlikely to be repeated, resulting in a positive base
effect.Chart 2.17: Producer price inflation suggests cost pressures for goods are easing
Annual output producer price and CPI goods excluding energy inflation (a)
Sources: ONS and Bank calculations.
(a) The output PPI series is the headline ONS measure for manufacturing output producer prices. The PPI series has
been mean and variance adjusted to match the corresponding CPI series between 2012 and 2019. The latest data point
is September 2023 and the projection is to March 2024.
â€¦and to a lesser extent services inflation, which, after a projected spike in
January, is expected to moderate as pay growth and other input costs fall.
Bank of England  
Page 59



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_6.txt
================================================
The MPC will continue to monitor closely indications of persistent inflationary pressures
and resilience in the economy as a whole, including a range of measures of the
underlying tightness of labour market conditions, wage growth and services price inflation.
Monetary policy will need to be sufficiently restrictive for sufficiently long to return inflation
to the 2% target sustainably in the medium term, in line with the Committeeâ€™ s remit. The
MPCâ€™s latest projections indicate that monetary policy is likely to need to be restrictive for
an extended period of time. Further tightening in monetary policy would be required if
there were evidence of more persistent inflationary pressures.
Bank of England  
Page 6



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_60.txt
================================================
Starting in February 2024, services inflation is expected to fall back gradually . Labour
costs make up the bulk of services firmsâ€™ production costs, so price pressures for services
are likely to decline as wage growth is expected to moderate. Non-labour input costs have
also played a role in pushing up services inflation over the past two years. In recent
months, the S&P Global/CIPS UK services input and output price PMIs have continued to
fall, signalling that the non-labour elements of services firmsâ€™  costs are already
moderating. Consistent with these data, the Bankâ€™s Agentsâ€™ contacts report that, although
pay pressures are still significant for consumer-facing services companies, other cost
pressures are mostly easing.
Overall, services CPI inflation is projected to fall back to 6.4% by March 2024 (Chart
2.18), consistent with the expected decline in pay pressures and broader input price
inflation.Chart 2.18: A measure of underlying services inflation has started to fall back
slightly
Twelve-month services inflation (a)
Sources: ONS and Bank calculations.
(a) The methodology for an aggregate underlying inflation measure is set out in Potjagailo et al (2022). The underlying
services inflation measure shown here focuses on comovement in the prices of services items rather than all items. The
inflation rate of each item is disentangled into a common component and idiosyncratic fluctuations using a dynamic
factor model. In a second step, the common components of individual services price items are aggregated into the
underlying services inflation measure using the CPI item weights. The latest data are for September 2023, the projection
is to March 2024.
Bank of England  
Page 60



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_61.txt
================================================
Both domestic workers and firms have suffered real income losses following the
deterioration in the UKâ€™s terms of trade, which has been driven largely by the increase in
imported energy costs (Martin and Reynolds (2023)). Chart 3.2 shows the squeeze on
real labour incomes. Meanwhile, the Bankâ€™s DMP Survey suggests that firmsâ€™ margins
have been more likely to fall than rise over the past year. These results are in line with
recent work by Piton et al (2023): UK firmsâ€™ earnings in excess of all production costs
have been declining since the start of 2022, as they did following sharp rises in energy
prices in the past. However, the decline in profits has not been uniform across firms. While
many firms have experienced declining profits, some firms with greater market power
have been able to increase their margins.
Against the backdrop of past reductions in aggregate margins, there is some evidence to
suggest that firms may attempt to rebuild margins as external cost pressures moderate. In
the DMP Survey, more firms expect their profit margins to increase than to decrease in the
coming year. But the Bankâ€™s Agents report that firms currently see limited scope for margin
rebuilding through further price increases (Box D). The extent to which firms are able to
improve their margins is likely to depend on the outlook for demand (Section 3).
If firms and employees seek to recoup lost incomes by pushing for higher prices and
wages, the second-round effects from the increase in global energy and goods prices may
take longer to unwind. These risks continue to underpin the upside risks to the MPCâ€™s
inflation projection (Key judgement 3 in Section 1).2.4: Inï¬‚ation expectations
In the latest YouGov/Citigroup survey, short-term household inflation expectations
remained broadly unchanged at still elevated levels, after falling back significantly from
their peaks in 2022 (Chart 2.19). Medium-term inflation expectations in this survey stand
close to their 2010â€“19 average. The distribution of inflation expectations within surveys
may contain information. The share of respondents expecting annual inflation of 6% or
more on average over the medium term remained broadly stable at around 16% between
August and October, well below the peak of 31% in August 2022 and close to the average
level over 2019.In aggregate, firmsâ€™ margins appear to have been squeezed in the past couple of
years. There is some evidence to suggest that firms plan to rebuild margins.
Household inflation expectations have continued to ease.
Bank of England  
Page 61



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_62.txt
================================================
In the DMP Survey, firmsâ€™ CPI expectations for the year ahead have been declining and
stood at 4.6% in October, still well above the inflation target (Chart 2.20). Firmsâ€™ three year
ahead CPI expectations also continued to edge down to 3.1% in October , compared to a
peak of 4.8% in September 2022.Chart 2.19: Household inflation expectations have fallen back from their 2022 peak
Household inflation expectations (a)
Sources: Citigroup, YouGov and Bank calculations.
(a) Data are not seasonally adjusted. â€˜Short-term expectationsâ€™ refers to expectations in the next 12 months and
â€˜medium-term expectationsâ€™ refers to expectations five to ten years ahead. The household survey asks about expected
changes in prices but does not reference a specific price index. The latest data points are for October 2023.
Firms are also expecting lower inflation over the coming year, though still higher
than the 2% inflation target.
Bank of England  
Page 62



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_63.txt
================================================
A measure of medium-term inflation compensation in financial markets has risen over the
course of the year and stands well above its average level over the previous decade,
though still below its peak in the first half of 2022 (Chart 2.21). Interpreting these data is
challenging because they can move for reasons unrelated to inflation expectations, for
example due to illiquidity in markets and the use of these instruments in hedging pension
liabilities. As this is a measure of RPI inflation compensation, any changes in the outlook
for the wedge between RPI and CPI can also affect these data.
The median respondent in the November Market Participants Survey expected CPI
inflation of 2.1% three years ahead, down slightly from 2.2% in August. The distribution of
survey responses remained skewed to the upside.Chart 2.20: Firmsâ€™ CPI inflation expectations have fallen back
Firm inflation expectations (a)
Source: DMP Survey.
(a) Data are based on responses to the question: â€˜What do you think the annual CPI inflation rate will be in the UK, one
year from now and three years from now?â€™. The latest data points are for October 2023.
Market-based measures of inflation compensation have increased further since
August.
Bank of England  
Page 63



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_64.txt
================================================
Chart 2.21: A measure of medium-term inflation expectations in financial markets
has been drifting up since the start of the year
RPI-reform adjusted measure of five-year, five-year forward inflation compensation (a)
Sources: Bloomberg Finance L.P. and Bank calculations.
(a) Market-derived inflation compensation rates for average UK RPI inflation over a five-year period starting five years
into the future. It is derived by adjusting the five-year, five-year rate to account for UK RPI reform. From 2030, UK RPI
will be aligned with the CPIH measure of consumer prices. At present, the wedge between the current definition of RPI
and CPIH affects the unadjusted series. This measure is calculated by adding a scaled market-derived estimate of the
impact of RPI reform onto the unadjusted rate. That is calculated as the difference between the closest one-year forward
rates before and after the planned RPI reform date (currently the five-year , one-year rate and the seven-year, one-year
rate) on a three-month daily rolling average basis, and the adjustment is applied to the five-year forward period
impacted by the reform. The latest data point is 24 October 2023.
Bank of England  
Page 64



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_65.txt
================================================
[Non-text file]


================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_66.txt
================================================
Response rates have deteriorated further since mid-2020. The continued
deterioration might be an extension of the pre-Covid downward trend, which could
reflect a shift in societal attitudes towards responding to household surveys. Similar
downward trends in household survey response rates have been observed across
many other advanced economies (see, for example, Leeuw et al (2018) ).
The decline in response rates increases the risk that the LFS estimates may be
statistically biased. If the fall in responses is concentrated in households that have
different labour market characteristics than the average respondent, the survey will
be less representative as a result. In addition, the decline in response rates has
contributed to a reduction in the achieved sample size of the LFS. This means the
survey is experiencing higher sampling variability than in the past, which can result
in more volatile estimates from quarter to quarter.
In an effort to address some of these challenges the ONS is developing atransformed Labour Force Survey (TLFS). The new estimates will be based on
improved methods for collecting data, aimed at increasing the response rate, and aChart A: LFS response rates and achieved sample sizes have declined
notably since the pandemic
LFS response rate and achieved sample size (a)
Sources: ONS and Bank calculations.
(a) Dashed lines represent the 2013â€“19 trend in sample size and response rate if projected forward linearly .
Data are to 2023 Q2, prior to the phasing out of the boost to the LFS sample size introduced by the ONS during
the pandemic.
Bank of England  
Page 66



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_67.txt
================================================
larger sample size. The ONS plans to transition to the TLFS estimates in March
2024. [1]
Due to concerns around the impact of the fall in the response rates on the quality of
the data, the ONS has temporarily stopped publishing LFS estimates of
employment, unemployment and inactivity following the June 2023 data. The ONS
has announced that it plans to resume publication of its LFS estimates in
December, but in the meantime it has replaced them with experimental estimates.
These figures take the LFS estimates for unemployment and employment in the
three months to June, and thereafter project them forward in line with the claimant
count measure of unemployment and HMRC payrolls data respectively . As
experimental statistics, these estimates need to be interpreted with caution
(especially given both the HMRC payrolls data and the claimant count measure are
themselves experimental statistics). The Office for Statistics Regulation is
undertaking a review of these data.
The population weights that the ONS uses to produce the latest LFS estimates
assume that the demographics of the population have not changed since June
2021. Updating for recent demographic changes will affect estimates of the rates of
participation, employment and unemployment in the labour market. For example,
the current LFS population weights do not account for the ageing in the population
since mid-2021, and as a result older people have been progressively
underweighted in the LFS estimates. As older people are more likely to be out of
the labour force, this means that the estimates of the participation rate and
employment rate are probably too high.
The existing population weights used in the LFS are also based on a mid-2021
assumption about the size and growth of the population. They do not therefore
capture more recent information about the population, such as greater than
assumed inward migration. Because the UK population is now estimated to have
increased since mid-2021 by more than assumed in the LFS, the total number of
people estimated to be in employment, unemployment and outside of the workforce
in the UK is expected to be revised up, all else equal.The ONS has temporarily paused the publication of LFS estimates from the
June data, and replaced them with experimental estimates.
LFS estimates are also based on mid-2021 population estimates, and will be
revised to reflect more up to date estimates later this year.
Bank of England  
Page 67



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_68.txt
================================================
The ONS plans to incorporate updated population estimates in December , and this
is expected to result in revisions to the LFS back data.
Bank staff estimates of the impact of using more up to date population weights are
shown in Chart B. Those estimates suggest that the population reweighting would
reduce the employment and participation rates in mid-2022 by around 0.4
percentage points. These estimates use the latest publicly available population
estimates from January 2023, but the ONS plans to publish a further update later
this month.
The indicative staff estimates point to a limited impact on the unemployment rate in
mid-2022 from the population reweighting. As vacancies are measured through a
different survey, this also implies little impact on the vacancies to unemployment
ratio â€“ one of the MPCâ€™s key measures of labour market tightness.Bank staff estimates using more up to date population weights point to
notable downward revisions to the employment and participation rates in
mid-2022, but a smaller effect on the unemployment rate.
Chart B: Forthcoming revisions from updating population weights are
expected to reduce estimates of the participation and employment rates in
mid-2022
Indicative staff estimates of the impact of updating the LFS population weights (a)
Sources: ONS and Bank calculations.
(a) Indicative staff estimates are based on the ONSâ€™s January 2023 population projections. Bars represent the
change between 2019 Q4 and the three months to July 2022 in the current LFS estimates versus the indicative
post-revision estimates calculated by Bank staff.
Bank of England  
Page 68



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_69.txt
================================================
These indicative estimates only give a snapshot of the possible impact of the
reweighting in mid-2022 on the participation, employment and unemployment rates.
In addition to the uncertainty over these impacts, it is possible that any future
revised estimates will also reveal a change to the recent path of these labour
market variables.
While there are increased uncertainties about the LFS data, the Committeeâ€™ s views
on labour market developments are informed by a wide range of data. The MPC
looks at other official data, such as the ONS Workforce Jobs and HMRC payrolls
data, and wider labour market indicators, such as private sector surveys and
intelligence from the Bankâ€™s Agents. In addition to the vacancies to unemployment
ratio, surveys of recruitment difficulties inform the MPCâ€™s assessment of labour
market tightness. The collective steer from these data sources is discussed in
Sections 2.2 and 2.3.These increased uncertainties in the official LFS data reaffirm the
importance of taking a steer from a wide range of data on labour market
developments.
Bank of England  
Page 69



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_7.txt
================================================
1: The economic outlook
Twelve-month CPI inflation remains well above the MPCâ€™s 2% target, but has fallen
back to 6.7% both in September and in 2023 Q3 as a whole, below expectations in
the August Monetary Policy Report. Most of the downside news since the previous
Report reflects lower core goods price inflation. Services inflation has been only
slightly weaker than expected in August and remains elevated. CPI inflation is
expected to continue to fall quite sharply in the near term, to an average of around
4Â¾% in 2023 Q4, 4Â½% in 2024 Q1 and 3Â¾% in 2024 Q2. Most indicators of pay
growth have tended to be stable at rates of growth that are high. But they have not
shown the recent rise in the annual rate of growth of the private sector regular AWE
series. Earnings growth is expected to be somewhat stronger than in the August
Report, but is still projected to decline in coming quarters from these elevated
levels.
Second-round effects in domestic prices and wages are expected to take longer to
unwind than they did to emerge (Key judgement 3). In the most likely , or modal,
forecast conditioned on the market-implied path of interest rates, an increasing
degree of slack in the economy and declining external cost pressures lead CPI
inflation to return to the 2% target by the end of 2025 and to fall below target
thereafter. Compared with the August Report modal projection, inflation is expected
to return to close to the 2% target slightly less rapidly in the middle of the forecast
period, reflecting higher energy and other import price inflation.
The Committee continues to judge that the risks to its modal projection are skewed
to the upside. Taking account of this skew, and conditioned on market interest rates,
mean CPI inflation is 2.2% and 1.9% at the two and three-year horizons
respectively. In the mean projection conditioned on the alternative assumption of
constant interest rates at 5.25% over the forecast period, CPI inflation is expected
to be 2.0% and 1.6% in two yearsâ€™ and three yearsâ€™ time respectively.
Given the significant increase in Bank Rate since the start of this tightening cycle,
the current monetary policy stance is restrictive. GDP is expected to be broadly flat
in the first half of the forecast period and growth is projected to remain well below
historical averages in the medium term, also reflecting a waning boost from fiscal
policy and subdued potential supply growth (Key judgement 1). GDP  is lower
Bank of England  
Page 7



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_70.txt
================================================
Box C: How has Blue Book 2023 changed past estimates of
UK GDP growth ?
Each year the ONS updates its estimates of GDP growth in past years to reflect
improvements in its methodologies and additional data. The Blue Book 2023
revisions included large changes to estimates of GDP growth over 2020 and 2021,
a time when measuring the size of the economy was particularly challenging (ONS
(2023) ). Annual GDP growth is now estimated to have been 0.7 percentage points
higher in 2020 and 1.1 percentage points higher in 2021. Growth rates since then
are little changed.
Following the revisions, UK GDP is now estimated to have exceeded its pre-
pandemic level by 2021 Q4, rather than having remained slightly below it (Chart A).
In 2023 Q2, GDP is estimated to have been 1.8% above its pre-Covid level. On
current estimates, the UKâ€™s post-Covid recovery in output is now more in line with
other G7 countries.The ONS raised its estimates of GDP growth over 2020 and 2021 in its
annual Blue Book revisions.
The latest estimates imply that UK output returned to its pre-Covid level by
the end of 2021.
Chart A: The level of UK GDP was revised higher in Blue Book 2023
Change in level of GDP since 2019 Q4 (a)
Sources: ONS and Bank calculations.
(a) Data are to 2023 Q2.
Bank of England  
Page 70



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_71.txt
================================================
The revisions to GDP were concentrated in the public sector and, specifically, the
healthcare industry where output had been particularly hard to measure during the
pandemic. They reflected new data sources and measurement improvements.
Revisions to overall market sector output were small. Within the market sector ,
upward revisions to the wholesale and retail trade industry were largely of fset by
downward revisions to industrial production.
Revisions to historic estimates of GDP are not typically judged to have implications
for the balance between demand and supply and hence inflationary pressures,
since inflation outturns remain the same. In addition, these particular Blue Book
revisions are almost entirely accounted for by changes to public sector output and
so contain little news about the balance of supply and demand in the market sector
economy, which is more relevant for assessing inflationary pressures. In light of
that, the MPC has judged it appropriate to revise up potential supply in line with the
revisions to measured GDP, such that the balance between them over the past is
unchanged.Upward revisions to public sector output were sizable; estimates of total
market sector output were little changed.
These revisions do not have implications for the MPCâ€™s assessment of the
balance between demand and supply over the past and hence underlying
inflationary pressures.
Bank of England  
Page 71



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_72.txt
================================================
Box D : Agents â€™ update on business conditions
The key information from Agentsâ€™ contacts considered by the Monetary Policy
Committee at its November meeting is presented in this box, which summarises
intelligence gathered in the six weeks to mid-October.
In line with the September Agentsâ€™ update, many areas of the economy continued to
report weak activity. In some areas such as housing, commercial real estate,
consumer goods, business services and manufacturing, contacts suggested there
had been a further softening in activity.
Employment intentions have weakened a little further, but overall remained
consistent with broadly stable headcount in the coming year. Recruitment difficulties
have continued to ease, although skill shortages were still a concern for some.
Goods inflation is slowing more quickly than for services as input cost pressures
continue to ease.
For consumer goods, volumes have mostly been flat, including for clothing and
beauty items. But they have fallen sharply over the past year for furniture,
technology, and home improvement products, likely a consequence of increased
spending on these during the pandemic.
Price inflation has sustained revenue growth at pubs and restaurants as the number
of customers has fallen. Following a generally good summer, contacts in the
hospitality sector were worried about demand falling more over the coming months
than is usual for the time of year. Bookings for hotels and other tourist venues were
being made later than usual reflecting greater consumer caution.
Contacts reported that households have also been cutting back the size of their
entertainment and data packages and spending less on telecoms, such as ending
landline contracts and upgrading mobile phones less frequently.
Overall, contacts remained pessimistic about the outlook, with most expecting weak
volume growth over the coming year.While still positive, nominal growth in consumer spending is weakening, in
line with lower inflation. Growth in consumer spending volumes remained
subdued. Spending on services appeared to be outperforming goods.
Bank of England  
Page 72



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_73.txt
================================================
Higher finance costs and greater economic uncertainty have led some contacts â€“
especially those already facing margin or cash-flow pressure, or those dependent
on borrowing to invest â€“ to lower their investment intentions. Those with strong cash
positions plan to continue to invest: business services firms continue to spend on
information and other technology; professional service firms are most likely to refer
to meaningful investment in AI technologies aimed at creating efficiencies rather
than reducing headcount; consumer services are investing to maintain their service
offering; while many production firms are focusing on green energy generation and
other efficiency measures.
These considerations continue to shape investment intentions. Chart A summarises
responses to a recent Agentsâ€™ special survey on this topic. The main factors
supporting investment intentions are digitalisation, efficiency, and sustainability,
while the cost of external finance is the largest reported drag.Investment growth is expected to slow slightly but remain positive next
year, driven by the need to maintain and upgrade information and other
technology, the pursuit of efficiencies and investment to improve
sustainability.
Bank of England  
Page 73



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_74.txt
================================================
Manufactured goods exports volumes growth has slowed, with exports now at the
same level as a year ago. Consumer goods exports volumes fell, with EU sales
notably weaker. This reflected softer demand but also Brexit-related trade frictions.
While demand from the US and Middle East remained robust, it has softened from
China. Services exports values growth has slowed but remained positive, mostly
driven by fee increases. There are near-term downside risks from weaker consumer
demand and the continued impact of Brexit-related trade frictions, although contacts
expected demand for aerospace, defence, and pharmaceuticals to remain robust.Chart A: Structural factors are motivating higher investment spending next
year
Factors affecting investment over the next 12 months (a)
Source: Bank calculations.
(a) Taken from responses to the Agentsâ€™ survey on investment intentions. Question: â€˜How are the following
factors affecting your UK investment spending plans over the next 12 months compared to the past 12
months?â€™. Reports of â€˜slightâ€™ reduction/boost were given a 50% weight relative to reports of â€˜substantialâ€™
reduction/boost when calculating these net balances.
Goods export volume and services export value growth have slowed over
the past year. Contacts cited the ongoing impact of Brexit-related trade
frictions.
Revenue growth in business services, while slightly weaker, continues to be
sustained by inflation as volumes fell slightly on a year ago. Manufacturing
volumes have reduced a little, and construction output continues to decline.
Bank of England  
Page 74



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_75.txt
================================================
Revenue growth in many areas of business services has continued to be subdued.
Mergers and acquisitions activity has remained weak, demand for logistics has
softened with container imports down compared with last year and property service
revenues have fallen, in line with declining commercial real estate transactions.
Even accountancy, law and consultancy services, where demand had been holding
up, were now reporting a softening as clients become more circumspect on
discretionary spending. Contacts were concerned that turnover growth could soften
further over the winter, although some expected demand to pick up in 2024 H1 as
funding costs stabilised.
Manufacturing volumes fell slightly. This is weaker than in the Agentsâ€™ September
update on business conditions, when contacts reported that volumes were flat.
Domestic demand has eased, with demand falling for homewares and construction
products, although some sectors such as aerospace, renewables and defence,
chemicals, pharmaceuticals, and energy were still seeing growing demand
supported by exports. Consumer-facing manufacturers cited weakening in order
books as a sign that volumes would decline over the year ahead.
Weakening demand, high costs and lower returns have led to a continued decline in
construction output. Private sector activity has slowed sharply, while public sector
activity has moderated. The pace of decline in construction output could increase
over the winter, given uncertainty about private sector project rates of return and
increasing pressure on public sector budgets.
Contacts reported that higher mortgage rates and expectations of further house
price falls were increasingly weighing on housing demand, with reductions of
around 20% in transactions and 5% in prices having occurred relative to a year ago.
Most expected broadly similar falls in prices over the year ahead. Mortgage
approval numbers have declined further and were expected to continue to do so
over the coming year. There was no pickup in mortgage arrears, but the credit
profile of applicants has worsened owing to weaker disposable income.
Rental demand has remained strong, well in excess of supply, pushing new rents
up by double digits on a year ago. One contributing factor was the higher cost of
mortgage borrowing, which was making rents relatively less expensive, further
stimulating rental demand.The declining trends in housing demand and housing starts have
intensified. Rental demand has remained strong.
Bank of England  
Page 75



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_76.txt
================================================
Investorsâ€™ demand for commercial real estate has softened given their perception of
an increased likelihood of â€˜higher-for-longerâ€™ interest rates. There were isolated
reports of forced sales due to increased outflows from portfolio funds held by
institutional investors. Banks have started responding more firmly to breaches in
loan covenants, leading to an expectation of more forced sales next year .
Larger corporates typically reported continued access to bank or non-bank finance
through 2023. Contacts say banks have tightened lending supply for small
businesses, with some reports of banks rejecting new loans, not rolling over debt,
or imposing more onerous terms and conditions.
Demand for credit has remained weak across firms of all sizes due to higher
interest rates and uncertainty about the economic outlook. This was consistent with
declining bank loan books. Large corporates were issuing fewer bonds in the hope
that yields would decline, and private equity firms were much less active.
Bad debts were still at normal levels, although the failure rate of the smallest
companies was higher than recent years. Companies in the construction sector
were faring the worst, with trade credit tightening and even some large companies
getting into difficulty.
Many contacts were looking to maintain employment at current levels, if they judged
they were the right size to meet demand or if they were prepared to hoard labour
after experience of recruiting difficulties in the past. There was a slight uptick in
reports of planned headcount reductions in property and construction, although
such reports remain in the minority. Some sectors such as audit, insurance,
pharmaceuticals, and aerospace have continued to expand their workforces on the
back of growth.
Recruitment difficulties have continued to ease for many contacts, particularly for
lower-skilled roles. But recruitment remained a serious concern for contacts in
particular locations and for contacts demanding skills in the finance, accountancy ,
IT, and engineering sectors.Credit supply has tightened for small firms, less so for large corporates.
Demand for credit remains weak given high interest rates.
Employment intentions have softened a little further since mid-August but
overall remained consistent with broadly stable headcount in the coming
year. Recruitment difficulties have continued to ease, although skill
shortages were still a concern for some.
Bank of England  
Page 76



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_77.txt
================================================
Wage settlements were expected to trend down slightly over the rest of this year.
Early indications for 2024 suggested that average pay increases were expected to
be lower than in 2023. But those businesses that rely heavily on lower-paid workers
were concerned about another sizable increase in the National Living W age next
April.
Contacts reported price falls for a range of key inputs, despite increases in the oil
price and the recent depreciation of the pound. Manufacturersâ€™ domestic price
inflation has continued to ease, with an increasing number of contacts expecting to
return to a single small annual price increase in 2024 and some hoping to avoid
price increases all together.
Business services price inflation has remained high, but contacts judged that it had
peaked amid more caution about price rises, even in sectors such as law ,
accountancy, and IT where demand had been strong.
Aggregate profit margins have remained squeezed as many companies were
unable to fully pass through higher costs into output prices while sales volumes
were weakening. Firms saw limited scope for margin rebuilding through further
price increases, with many instead focused on operational efficiency improvements.
Consumer goods price inflation has continued to ease for a broad range of
categories. Food producers reported ingredient costs either stabilising or falling.
Price inflation for new cars has passed its peak and was expected to moderate
further. Contacts expected little change in prices for electronics, white goods, and
furniture. Clothing retailers reported low single-digit inflation for their autumn and
winter ranges.
Inflationary pressures for consumer services were weakening more slowly than for
goods. But restaurants, pubs and hotels saw somewhat limited scope for further
price increases without adverse consequences for sales volumes. Health services
inflation appeared likely to remain high, with continuing cost pressures and robust
demand.Goods inflation is slowing more quickly than for services inflation as input
cost pressures continue to ease.
Bank of England  
Page 77



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_78.txt
================================================
3: In focus â€“ The outlook for demand
In the face of the series of significant shocks that have hit the UK economy in
recent years, demand growth has proved relatively resilient (Section 3.1). Several
factors have been important in supporting that resilience, including a strong labour
market, falling energy prices and fiscal support from the Government, all of which
have boosted household real income (Section 3.2). There are increasing signs that
the restrictive stance of monetary policy needed to combat the elevated inflation
caused by the original economic shocks is now reducing demand through a range
of channels (Section 3.3). The evolution of those factors which have been
supporting the economy will be the key influence on how demand, and hence
inflation, develops. In the MPCâ€™s central projection, demand growth remains below
historical averages as higher interest rates weigh on activity, a margin of slack
opens up, and inflation is brought back to target (Section 3.4 and Section 1).
3.1: Recent developments in GDP
Between the end of 2022 and August 2023, the UK economy grew by 0.6%. That
represents slow growth compared to historical norms. However, in the context of the
sequence of supply shocks that have hit the UK economy ((Bailey (2023 )), including the
energy price spike following Russiaâ€™s invasion of Ukraine, the economy has shown more
resilience than many expected. Chart 3.1 shows the evolution of forecasts for 2023
calendar year growth, with the Bankâ€™s forecasts in aqua and the average forecast from the
HM Treasury survey of independent forecasters in orange. In 2022 and early 2023, the
consensus was that the economy would be in recession in 2023. Expectations are now
that the economy will grow slightly across 2023. Much of this improvement reflects a
recovery from the supply shocks, and in particular the fall in energy prices. However , over
this time, the MPC has also adjusted the conditioning assumptions upon which its
forecasts are constructed and made a series of judgements to increase its projections of
demand. These judgements include the possibility of lower precautionary saving by
households than previously assumed, in turn related to a lower risk of job loss given the
continued strength in labour market activity (Section 1).GDP growth has been stronger than previously anticipated during the first half of
2023 but recent data suggest GDP growth has started to slow.
Bank of England  
Page 78



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_79.txt
================================================
However, recent data releases suggest that the gradual improvement in the near-term
outlook for GDP has stalled (Section 2.2). In assessing the likely evolution of the
economy, a key question for the MPC is the extent to which supportive factors will remain
in the coming quarters. Another key question is the extent to which higher interest rates
will slow demand growth. Given the significant increase in Bank Rate since the start of
this tightening cycle, the current monetary policy stance is restrictive.
3.2: Key factors supporting the resilience in demand
The labour market has remained relatively resilient during the course of the energy price
shock. Although the unemployment rate has risen over recent months (Section 2.2), it
remains low in a historical context.Chart 3.1: Forecasters have consistently revised up expectations for 2023 GDP
growth over the past year
Evolution of forecasts for 2023 calendar year GDP growth (a)
Sources: HM Treasury and Bank calculations.
(a) The independent forecasters series shows the mean of HMTâ€™s survey of independent forecasters. This includes the
most recent forecast for each institution included in the survey and therefore can include forecasts made in earlier
months than each survey period. Differences between independent forecasters and the Bankâ€™s projections will in part
relate to different conditioning assumptions for their forecasts. The evolution of the Bankâ€™s MPR forecasts reflects a
sequence of judgements made by the MPC about the likely outlook for demand (see Key judgement 1 in Section 1). The
final data points refer to the October 2023 average forecast from HMTâ€™ s survey of independent forecasters and
projections from this Report respectively.
A resilient labour market has helped support aggregate nominal household
incomes...
Bank of England  
Page 79



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_8.txt
================================================
compared with August, reflecting recent weaker-than-expected activity data and the
Committeeâ€™s related decision in this forecast to reduce somewhat the scale of, but
not to remove completely, its previous judgement boosting expected demand.
The margin of excess demand in the UK economy has diminished over recent
quarters and an increasing degree of economic slack is expected to emerge from
the start of next year (Key judgement 2). Unemployment is expected to rise further
over the forecast period and exceed the Committeeâ€™s upwardly revised estimate of
the medium-term equilibrium rate from the end of next year. There are increased
uncertainties around the ONSâ€™s official labour market activity data that have
previously been based on the Labour Force Survey, and the Committee is therefore
continuing to consider the collective steer from a range of indicators.
Table 1.A: Forecast summary (a) (b)
2023 Q4 2024 Q4 2025 Q4 2026 Q4
GDP (c) 0.6 (0.9) 0 (0.1) 0.4 (0.5) 1.1
Modal CPI inflation (d) 4.6 (4.9) 3.1 (2.5) 1.9 (1.6) 1.5
Mean CPI inflation (d) 4.6 (4.9) 3.4 (2.8) 2.2 (1.9) 1.9
Unemployment rate (e) 4.3 (4.1) 4.7 (4.5) 5 (4.8) 5.1
Excess supply/ Excess demand  (f) 0 (Â¼) -Â¾ (-Â¾) -1Â½ (-1Â½) -1Â½
Bank Rate (g) 5.3 (5.8) 5.1 (5.9) 4.5 (5) 4.2
(a) Figures in parentheses show the corresponding projections in the August 2023 Monetary Policy Report.
(b) Unless otherwise stated, the numbers shown in this table are modal projections and are conditioned on the
assumptions described in Section 1.1. The main assumptions are set out in Monetary Policy Report â€“ Download
chart slides and data â€“ November 2023 .
(c) Four-quarter growth in real GDP.
(d) Four-quarter inflation rate. The modal projection is the single most likely outcome. If the risks are symmetrically
distributed around this central view, this will also provide a view of the average outcome or mean forecast. But when the
risks are skewed, as in the current forecast, the mean projection will dif fer from the mode.
(e) ILO definition of unemployment. Up to June 2023, this projection is based on LFS unemployment data. Beyond this
point, the Committee is drawing on the collective steer from other indicators of unemployment to inform its projection
(see Box B).
(f) Per cent of potential GDP. A negative figure implies output is below potential and a positive that it is above.
(g) Per cent. The path for Bank Rate implied by forward market interest rates. The curves are based on overnight index
swap rates.
Bank of England  
Page 8



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_80.txt
================================================
A stronger labour market supports household demand directly, through higher household
labour incomes, and indirectly as a result of greater consumer confidence, partly from
lower worries about job security. As shown in Chart 3.2, employment growth has
contributed positively to annual real labour income growth throughout 2022 and 2023, with
a peak contribution in 2023 of 1.2 percentage points in April. The tightness of the labour
market is also estimated to have pushed up nominal wage growth (Chart 2.14). Although
these factors have been outweighed in 2023 by the fall in energy prices and the
commensurate easing in inflation, they remain an important determinant of labour
incomes.
There is a range of potential explanations for this resilience in the labour market. One is
â€˜labour hoardingâ€™: the Bankâ€™s Agents report that, in response to past heightened
recruitment difficulties, some businesses have kept employment levels higher than they
would have done otherwise.
There are some early signs that the resilience in the labour market has started to wane,
suggesting the support for income growth might also lessen at some point. Some
indicators of employment growth have weakened over time and the unemployment rate
has risen to 4.2%, based on the ONSâ€™ experimental estimate of unemployment (Section
2.2). Since the peak in the vacancies to unemployment ratio in 2022, much of the
reduction in labour demand has been reflected in falling vacancies. But the recent rise in
unemployment may indicate that more of the adjustment in the future will be through job
losses, or weaker employment growth, which would reduce aggregate household income.
In contrast, nominal wage growth is projected to remain high and above inflation in the
near term, supporting real labour income growth.â€¦but unemployment has started to pick up.
Bank of England  
Page 80



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_81.txt
================================================
There has been a marked fall in wholesale gas prices since 2022 (Chart 2.4), now feeding
through into retail gas and electricity bills. A falling contribution from energy prices has
been the primary driver of the decline in CPI inflation this year (Section 2.3), and the drag
on real incomes from rising prices (purple bars in Chart 3.2) has waned. The improvement
in measures of consumer confidence, which have shown a marked rebound from near-
historic lows in 2022, aligns closely with the fall in wholesale gas prices, probably
reflecting households expecting lower energy bills in the future. In the most recent data,
the headline GfK consumer confidence measure fell back somewhat but remains well
above its low point in 2022.Chart 3.2: A resilient labour market and a fall in energy prices have supported an
improvement in household real income growth
Annual real labour income growth (a)
Sources: ONS and Bank calculations.
(a) Contributions to real labour income growth are approximations calculated as the growth rate of the individual series
and so do not exactly sum to the total. Prices are measured using the three-month average seasonally adjusted CPI
index. Wages are defined as seasonally adjusted whole economy total average weekly earnings. The recent
contributions from employment growth may be affected by additional uncertainties around interpreting official estimates
of labour market activity (see Box B for more detail). The final data points refer to July 2023.
Real incomes have been supported by the fall in energy prices.
Incomes have also been supported by the Governmentâ€™s measures to help
households weather the pandemic and energy price shock. These are now being
withdrawnâ€¦
Bank of England  
Page 81



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_82.txt
================================================
The Government provided significant fiscal support to households through the pandemic
and also in response to the energy price shock. Chart 3.3 shows two ways of quantifying
the level of fiscal support for the economy: the cyclically adjusted primary deficit and the
cumulative borrowing impact of government policies announced since Budget 2020. Both
these measures show record levels of fiscal policy support, particularly at the height of the
pandemic. Some of the more recent key policies have taken the form of transfers to
households, including the Â£400 Energy Bills Support Scheme and the Cost of Living
Payments for lower-income households. These will have directly supported household
demand.
Given the waning impacts of the pandemic and lower energy prices, fiscal support is
being progressively withdrawn. As shown in Chart 3.3, the level of support from fiscal
policy is expected to fall slightly in 2023â€“24 before falling more materially in subsequent
years.Chart 3.3: Support from fiscal policy is expected to fall in the coming years
Measures of fiscal support (a)
Sources: OBR and Bank calculations.
(a) Forecast as of March 2023. The cyclically adjusted primary deficit measures government expenditure excluding
interest costs net of government revenue, adjusted for the economic cycle by the OBR. It is presented as a share of
GDP consistent with the latest available ONS GDP data to the OBR at the time of its publication (Quarterly National
Accounts published 30 June 2023). Cumulative policy measures since March 2020 is the total of the OBR estimate of
the impact of government policies announced at successive fiscal events on public sector net borrowing, measured in
nominal terms. The final data points refer to the 2027â€“28 financial year.
Bank of England  
Page 82



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_83.txt
================================================
Despite the headwinds to income growth, households are relatively optimistic about their
future finances. In the Bankâ€™s NMG survey, the measure of householdsâ€™ expectations for
their own financial situation over the next year has improved substantially since 2022 and
is now in line with results prior to the pandemic. Survey responses also suggest that
householdsâ€™ perceived risk of job loss has been falling and is now at its lowest level since
2015, although expectations for the level of economy-wide unemployment have increased
slightly over the past six months. The NMGâ€™s measure of household income expectations
has also risen, although this largely reflects the expectation that nominal incomes will
grow given high inflation.
During the pandemic, household consumption fell by more than income as households
were less able to spend on services, which meant that in aggregate households built up
additional savings. Much of these additional savings took the form of bank deposits. As
shown on the left of Chart 3.4, the total stock of household deposits rose materially ,
peaking around 10% higher than its previous trend in 2022 Q1. A similar pattern has been
observed across advanced economies (IMF (2023) ).â€¦although households in the NMG survey remain optimistic about their future
finances.
Household saving decisions will also affect the demand outlook; there is some
evidence to suggest that households have been saving less recently .
Bank of England  
Page 83



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_84.txt
================================================
There is some evidence that households are now saving less out of their current income,
potentially partly reflecting some households having built up an additional savings buf fer.
The stock of deposits, relative to trend, has fallen back a little since its peak. And as Chart
3.5 shows, two thirds of households who have reported changed saving habits in the
NMG survey reported saving less than usual over the past six months.Chart 3.4: In aggregate, households built up additional savings during the
pandemic
Household deposits growth since 2010 (a)
Sources: ONS and Bank calculations.
(a) The dashed orange line shows a simple trend growth path based on average growth between 2010 and 2019.
Household income is defined as nominal post-tax household and non-profit institutions serving households disposable
income. The final data point refers to 2023 Q2.
Bank of England  
Page 84



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_85.txt
================================================
The ONS measure of the aggregate saving rate still remains higher than before the
pandemic, matched by a similar trend in the cash-based saving rate (Chart 3.6). The
difference between the NMG survey, which suggests households are saving less, and the
aggregate ONS data may reflect differences in behaviour across the savings and income
distribution. The distribution in household savings in the UK is highly skewed (see, for
example, Broome and Leslie (2022)). This means that behaviour at the top of the
distribution has a much larger impact on the aggregate saving rate.
Evidence from the NMG survey shows that, on average, the highest income households
have been much more likely to increase their savings levels compared to a year ago.
Within the highest income decile, the share of respondents who increased their savings
over the past year is 25 percentage points higher than those who have reduced their
savings. In contrast, for the bottom half of the income distribution, respondents on
average report having fewer savings than a year ago.Chart 3.5: Fewer households are expecting to save less than usual in the next six
months than was the case over the past six months
Share of households reporting changes in their rate of saving (a)
Sources: NMG Consulting and Bank calculations.
(a) Results show responses to the survey question: â€˜As a result of any changes in income or spending, would you say
that your household has saved more, less, or the same over the last six months compared with how much you usually
save?â€™. Respondents who indicated having a different saving pattern to normal were asked a follow-up question. If they
saved less than usual, they were asked if that would continue. And if they saved more than usual, they were asked if
that would continue. Respondents saving around their usual rate over the past six months are excluded from the chart.
The survey results were collected between 30 August and 19 September.
Bank of England  
Page 85



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_86.txt
================================================
This distributional pattern is important because household behaviour is likely to vary
across the distribution, which can shape the aggregate demand outlook. For example,
households at the top of the savings distribution are likely to be less responsive to
changes in broader economic trends, such as the weakening in the labour market. And
higher wealth and income households also make up a disproportionate share of
consumption: in 2021â€“22, the tenth of households with the highest income made up
around a fifth of household expenditure.
There are uncertainties around whether households can support consumption by running
down savings or saving less out of their current income in the future. The right panel in
Chart 3.4 shows that when savings are considered relative to income, the above-trend
savings built up during the pandemic have been fully eroded. If households aim to
maintain a stock of savings in line with incomes, they may not want to draw down savings
any further. In terms of saving out of current income, the aggregate saving rate is
projected to remain fairly flat over the MPCâ€™s forecast period, as unemployment increases.Chart 3.6: The aggregate saving rate is expected to be broadly unchanged over the
next three years
Aggregate saving ratio (a)
Sources: ONS and Bank calculations.
(a) The cash-based savings ratio excludes transactions which are imputed and unobserved by households, for example
the imputed value gained by owner-occupiers from living in their home. The final outturn data is for 2023 Q2 and the
final forecast value is for 2026 Q4.
Bank of England  
Page 86



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_87.txt
================================================
Indeed, Chart 3.5 shows that among households who have changed their saving habits
over the past six months, the share of households who are planning to save less than
usual falls from two thirds to a half or under.
Other ways in which households can maintain their spending include borrowing and
raising hours worked. And there is some evidence more households are using credit to
support consumption. Data from the ONS Opinions and Lifestyle Survey show that the
share of people using credit to cope with high inflation has risen from 1 1% in early 2022 to
16% in the most recent data. However, overall growth in lending in the economy has
slowed significantly; for households this has been driven by a slowdown in mortgage
lending (Section 2.1) outweighing continued growth in consumer credit. There is also
evidence from the latest NMG survey that some households will support consumption by
changing their working arrangements: of those households who are expecting to see their
real incomes fall but plan to cut consumption by less than the fall in their income, 17%
plan to fund that gap by working overtime and 8% plan to move to a better paying job.
Real post-tax household labour income is projected to grow modestly over the forecast
period. There is uncertainty around how household saving behaviour will evolve over the
coming years. Despite a still elevated stock of nominal household deposits, evidence from
both the NMG survey and the likely gradual rise in the unemployment rate point towards
households avoiding running down savings where possible. Therefore, the household
saving ratio is expected to remain broadly unchanged over the MPCâ€™ s forecast period
(Chart 3.6). The relatively slow growth in income means that consumption growth is
expected to be weak throughout the forecast period: calendar-year household spending is
expected to be flat in 2024, and to rise by Â½% in 2025 and by just over 1% in 2026
(Section 1).
3.3: The impact of higher interest rates on demand
In order to bring inflation back to target, the MPC has raised Bank Rate significantly since
the start of this tightening cycle and now judges that the current monetary policy stance is
restrictive (Section 1). There are increasing signs of the impact of tighter monetary policy
in the labour market and in momentum in the real economy more generally .Overall, real labour income is projected to grow modestly over the forecast
period, the aggregate saving rate is projected to remain broadly unchanged, and
consumption growth is projected to be weak.
Higher interest rates are expected to reduce demand to an increasing extent.
Bank of England  
Page 87



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_88.txt
================================================
The largest component of lower demand from higher interest rates is estimated to come
from household consumption, in part because consumption makes up around 60% of total
GDP. The effects of higher rates on housing investment, business investment and the
exchange rate are all also important for the overall impact on demand. The following
sections present evidence of the impact that rate rises are having on the economy
through each of these channels so far.
Overall, the impact of higher interest rates on GDP is expected to materialise with a
significant lag: in the November projections, it takes until 2025 for the GDP  impact to be
close to fully felt. Based on the average relationships over the past between Bank Rate,
other financial instruments and economic activity, Bank staff estimate that more than half
of the impact on the level of GDP is still to come through, although there is significant
uncertainty around that estimate. The impact is likely to be felt more quickly on housing
investment and more slowly on consumption.
There are a wide range of factors, some of which change over time, that are likely to af fect
the impact that interest rates have on the economy. For example, as set out in Mann
(2023) , financial market conditions, the level of interest rates prior to initial rate rises, and
the evolution of real rates have the potential to affect the pass-through of rate rises to the
economy. The structure of the economy may itself also affect monetary policy
transmission, such as the value and distribution of assets and loans.
Consumption
Chart 3.7 presents an estimate of the impact of higher interest rates since August 2021 on
the level of household consumption, both so far and into the future. The total effect is split
into broad categories that show the relative size of some of the channels of monetary
policy transmission. The mortgage cash-flow channel captures the direct impact of
changes in household mortgage costs. The broader housing channel represents the
impact of changes in the value of housing. This includes, among other effects, changes in
the available collateral against which households can borrow and ef fects on householdsâ€™
saving behaviour. Other channels are captured in the purple bars. This includes a range
of mechanisms such as the impact of interest rates on financial wealth, and â€˜second-
roundâ€™ effects in which a reduction in demand then leads to households cutting
consumption as the wider economy weakens.Higher interest rates reduce household consumption through a range of channels.
Bank of England  
Page 88



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_89.txt
================================================
Changes in Bank Rate and future interest rate expectations pass-through to changes in
household deposit and loan rates over time. As set out in Section 2.1, pass-through to
rates on new loans has been broadly in line with developments in the relevant reference
rates. In contrast, pass-through to some savings rates has tended to be somewhat slower .
Nevertheless, the direct cash-flow effects of changes in interest rates have increased
average household incomes. This is accounted for by two key factors. First, the stock of
household savings exceeds the stock of mortgages: the outstanding value of mortgages is
a little over Â£1.5 trillion compared to close to Â£1.7 trillion in household deposits. That
means for an equivalent change in interest rates the impact on interest income is greater
than the impact on mortgage costs. And second, as set out in the May 2023 Report , overChart 3.7: Increases in interest rates are expected to continue to reduce
consumption
Estimated impact of changes in OIS yield curve since August 2021 on the level of consumption
(a)
(a) OIS rates are the overnight index swap rates and represent market expectations for Bank Rate. Data up to 24
October 2023 are included. Estimates show the output from the standard treatment of the impact of changes in Bank
Rate and Bank Rate expectations in the Bankâ€™s forecasting models. The consumption effects of the estimated impact of
changes in the OIS curve on sterling exchange rates have been excluded. Both the overall total impact and the
individual channel estimates are uncertain and could be higher or lower than presented here. The â€˜Mortgage cash flowâ€™
bars include some effect from intertemporal substitution where higher interest rates shift incentives to consume later
than otherwise would be the case. The â€˜Otherâ€™ bars show the net effect of changes in Bank Rate excluding the direct
mortgage cash-flow channel, intertemporal substitution and broader housing ef fects.
Higher interest rates reduce household consumption via higher mortgage costs,
even though aggregate household incomes have increased.
Bank of England  
Page 89



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_9.txt
================================================
1.1: The conditioning assumptions underlying the MPC â€™s
projections
As set out in Table 1.B, the MPCâ€™s projections are conditioned on:
The paths for policy rates in advanced economies implied by financial markets, as
captured in the 15-working day average of forward interest rates to 24 October (Chart
2.5). The market-implied path for Bank Rate in the United Kingdom has fallen by just
over Â½ percentage point on average over the next three years compared with the
equivalent period at the time of the August Report. The path for Bank Rate
underpinning the November projections remains around 5Â¼% until 2024 Q3 and then
declines gradually to 4Â¼% by the end of 2026. There has been a significant increase in
longer-term government bond yields globally since August (Section 2.1).A path for the sterling effective exchange rate index that is around 1Â½% lower on
average than in the August Report, and is depreciating gradually over the forecast
period given the role for expected interest rate differentials in the Committeeâ€™s
conditioning assumption.Wholesale energy prices that follow their respective futures curves over the forecast
period. Since August, spot oil prices and the oil futures curve have risen, while gas
futures prices are little changed. Significant uncertainty remains around the outlook for
wholesale energy prices, including related to recent geopolitical developments (Key
judgement 3).UK household energy prices that move in line with Bank staff estimates of the Ofgem
price cap implied by the path of wholesale energy prices (Section 2.3).
Fiscal policy that evolves in line with announced UK government policies to date, and
so does not include the contents of the Autumn Statement on 22 November.
Bank of England  
Page 9



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_90.txt
================================================
four fifths of mortgages are on fixed rates. This means changes in interest rates do not
immediately affect payments on the majority of mortgages, but rather only when those
mortgages are refinanced.
Despite this, the reduction in consumption from rising mortgage costs is expected to
outweigh the boost to consumption from higher savings income. In general, changes in
income do not affect consumption one-for-one. Instead, they depend on householdsâ€™
marginal propensity to consume. This measures the responsiveness of consumption to a
given change in income. Estimates of marginal propensities to consume vary significantly
over a number of different dimensions. Analysis suggests that a householdâ€™s marginal
propensity to consume will be lower, meaning consumption changes less in response to a
given income change, if the income change is positive, the household has a high income
and if the household is not liquidity constrained (see, for example, Christelis et al (2017)
and Albuquerque and Green (2022)). Households with positive net savings are more
likely to meet all of these conditions compared to households with large loans. This means
that households with large savings are likely to increase consumption relatively little in
response to rising savings incomes, but those with mortgages and other loans will reduce
consumption materially in response to higher loan costs. The net result is for demand to
fall despite the net aggregate increase in household income.
The NMG survey also suggests that the overall direct effects of interest rates on
household cash flows will reduce consumption. As Chart 3.8 shows, mortgagor
households are far more likely to report a negative effect on household finances from
interest rates than the equivalent share of saver households who report a positive impact
from higher interest rates.
The consumption effects from higher mortgage payments are expected to build over time
(aqua bars in Chart 3.7). This reflects the large number of people on fixed-rate mortgages
who have yet to experience an increase in mortgage costs (Box B in the May 2023
Report). However, the aggregate reduction in consumption is likely to materialise
somewhat faster than the realised increase in mortgage costs. This is because it is likely
that those with fixed-rate mortgages coming to an end will know that their mortgage costs
are going to increase and adjust consumption in advance. The Bankâ€™s NMG survey shows
that just over 30% of mortgagors who are yet to reach the end of their fixed-rate loan have
already spent less as a result. And only around two fifths of these mortgagors expect to
take no action in the next year as a result of the future increase in mortgage costs.
Bank of England  
Page 90



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_91.txt
================================================
Higher interest rates are typically associated with lower house prices, which can have
knock-on effects on consumption, largely via collateral effects.
Nominal house price falls have been relatively muted over the past year . Chart 3.9 shows
a range of measures of house prices since February 2020. It shows that during the
pandemic period and its immediate aftermath, house price inflation was rapid: the of ficial
ONS measure of house prices indicates that prices rose by around 25% between
February 2020 and the end of 2022. Since then, the official measure has indicated
broadly flat prices, although other more forward-looking measures suggest prices might
have fallen somewhat. There is some evidence that the divergence between these series
is partly due to cash-buyer transactions, which are captured in the ONS data but not in
data from mortgage lenders. The proportion of cash-buyer transactions has been elevated
but has now fallen back, so the divergence in price indices may shrink in coming months.Chart 3.8: More borrower households report a negative impact on their finances
from higher interest rates than saver households
Net balance of households reporting a positive versus negative impact of higher interest rates
on their finances (a)
Sources: NMG Consulting and Bank calculations.
(a) Results are based on responses to the following questions: â€˜What impact has this rise in interest rates had on your
household finances in the last twelve months?â€™ and â€˜What impact do you expect the rise in interest rates to have on your
household finances in the next twelve months?â€™. Mortgagor and renter households are restricted to those who do not
have positive net savings. The net savers group includes all housing tenures. The survey results were collected
between 30 August and 19 September.
Higher interest rates weigh on consumption via the housing market, for example
from collateral and precautionary saving effectsâ€¦
Bank of England  
Page 91



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_92.txt
================================================
In real terms, after taking account of inflation, house prices have fallen by much more than
the nominal change. Based on the official ONS measure, real house prices have fallen by
around 7% since they peaked in January 2022.
Lower house prices could affect household consumption in two main ways: through direct
wealth effects and through the availability of collateral for loans. Evidence suggests that,
on average, direct effects are not large (Barrell et al (2015) ). This is because, while
property-owning households cut consumption when they experience a fall in their housing
wealth, these effects on aggregate consumption are relatively small. Indeed, they may be
reduced somewhat by an opposite effect in which lower house prices represent an
effective boost in wealth for future potential buyers. To the extent to which there are small
direct effects from changes in wealth values, evidence suggests this relates to an increase
in precautionary saving. The latest NMG survey indicates that some households are
reporting planning to save more in the coming months in response to lower house prices,
although the numbers were small. Bank analysis suggests that the collateral channel, in
which falling house prices reduce householdsâ€™ available assets for borrowing, is more
important.
Together, the impact of interest rates via broader housing market effects is estimated to
grow over time. The reduction in the level of consumption from these effects by 2023 Q3
is estimated to be just under Â½%. However, as asset prices and households continue to
adjust, that impact is likely to increase. Bank staff project that the overall reduction in
consumption from these effects will grow to over 2% by the end of 2026. These estimates
are uncertain and evidence suggests that the relationship between house prices and
consumption can vary over time (Benito et al (2006)).
Bank of England  
Page 92



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_93.txt
================================================
Interest rate rises will also reduce non-housing asset prices. Indeed, net financial wealth
relative to household incomes fell materially in 2022 despite no reduction in nominal
household deposits (Broadbent (2022) ). However, changes in household financial wealth
tend to have less overall impact on demand because most households do not own
significant non-housing and non-pension wealth. The effect can be important for those
households at the top of the wealth distribution where, prior to the pandemic, the richest
tenth of households had 17% of their total net wealth in financial assets. The consumption
effect of reductions in the value of financial wealth are captured within the purple bars in
Chart 3.7.
Although not directly affected by rising interest rates, households in the rental sector may
also face increased housing costs, leading to further reductions in consumer demand.
Specifically, rising interest rates increase costs for buy-to-let (BTL) landlords with a
mortgage and reduce their returns through lower house prices. Landlords may try to pass
on their costs through rent increases. In the long term, rent rises will be driven byChart 3.9: Some measures of nominal house prices have fallen somewhat in recent
months
House price indices (a)
Sources: Nationwide, ONS, Refinitiv Eikon from LSEG, Rightmove.co.uk, S&P  Global/Halifax and Bank calculations.
(a) The final data point for the ONS UK House Price Index is August 2023. Halifax, Nationwide and Rightmove data are
advanced to reflect the respective timing of each data source in the house-buying process.
â€¦and also reduce financial wealth, although the impact on consumption is
estimated to be smaller.
Renters are also affected by the impact of interest rates on the housing sector .
Bank of England  
Page 93



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_94.txt
================================================
household income and housing supply growth. However, given information asymmetries,
moving costs, and other frictions within the rental market, landlords may temporarily have
market power to raise rents. Landlords may also choose to sell their property , and there is
some evidence that recent market exit by landlords has caused a degree of shrinkage of
the private rented sector. Evidence from the Bankâ€™s Agents suggests some smaller BTL
investors have left the market due to regulatory changes (the scale of selling in the BTL
sector is discussed in Bank of England (2023)).
Together, these effects appear to be contributing to rapid increases in rents. The CPI
measure of rents rose by 6.4% over the year to 2023 Q3 â€“ the fastest pace since 1994.
This measure is also a lagging indicator of the potential impact of interest rate rises on
rents as it measures rent increases across all rental properties rather than the increases
faced by those moving home. Estimates from Rightmove suggest that average new rents
rose by 10% over the year to 2023 Q3. Most of this rapid increase in rents will reflect high
nominal income growth during the period of high inflation. But higher rents from other
factors may cause households to cut back on other forms of consumption, reducing
demand in the wider economy. More broadly, renter households who are net borrowers
report similar impacts of higher interest rates on their finances as mortgagor households
(Chart 3.8).
Housing investment
Housing investment is likely to be reduced by higher interest rates. This form of
investment makes up only around 5% of overall GDP but is one of the most variable
components, meaning that it can have an outsized effect on aggregate growth over the
business cycle. For example, during the 2008 financial crisis, falls in housing investment
accounted for close to a quarter of the overall fall in GDP. This means that the impact of
interest rates on housing investment is particularly important for the transmission of
monetary policy.
As shown in Chart 3.10, the ONS splits housing investment into three main categories:
investment in new dwellings; improvements, repair and maintenance of existing dwellings;
and transfer costs, which include many of the costs of moving home such as legal fees.
Each of these can be affected by interest rates.
Higher interest rates will result in a lower real return on building new homes because it will
reduce the expected future selling price. It will therefore encourage housing developers to
reduce investment, and indeed this is what has happened over the past year (green barsThe impacts of interest rates on the housing market also reduce housing
investment, for example by reducing the number of housing transactions.
Bank of England  
Page 94



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_95.txt
================================================
in Chart 3.10). Most indicators of new construction are consistent with a continued drag
from interest rates on this part of housing investment.
Higher interest rates have contributed to a reduction in housing transactions by reducing
housing demand. The reduction in housing transactions directly reduces total transfer
costs (orange bars in Chart 3.10), which make up around a fifth of total housing
investment. Mortgage approvals for house purchases, a key part of housing transactions,
have averaged below 50,000 per month in 2023. This is the lowest sustained level since
the aftermath of the financial crisis in 2008â€“09.
The connection between higher interest rates, lower housing transactions and investment
in existing dwellings is less clear. Prior to the pandemic, investment in existing dwellings
had a positive correlation with housing transactions. Part of that may reflect people
choosing to make investments in homes when they move. However, the relationship
between transactions and investment in existing dwellings has weakened since then,
potentially reflecting changes in household preferences after the pandemic. Investment in
existing dwellings (gold bars in Chart 3.10) is yet to fall.
Overall, housing investment has fallen by 6.4% in the year to 2023 Q2, reflecting the falls
in new dwellings investment and transfer costs. Housing investment is projected to fall by
a further 9.4% by the end of 2026.
Bank of England  
Page 95



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_96.txt
================================================
Business investment
Interest rate rises impact businesses through many of the same mechanisms laid out
above, including increased loan costs for those with debt and reduced assets available for
collateral. There is evidence that the increase in interest rates so far is becoming more
salient for businesses. Chart 3.11 shows the share of businesses reporting various issues
as the primary concern for their business. High inflation and energy prices have
consistently ranked as the main concerns. The share of businesses most concerned
about interest rates is small but it has risen, up from close to zero in much of 2022 to 7%
of businesses in October 2023.Chart 3.10: Housing investment has fallen since early 2022, with lower investment
in new dwellings and lower transfer costs the key drivers
Real housing investment (a)
Sources: ONS and Bank calculations.
(a) Dashed line shows the latest Bank forecast for total real housing investment. Private existing dwellings investment
captures improvements, repair and maintenance of existing dwellings. The other category mostly comprises dwelling
investment in existing and new dwellings for non-financial public corporations. The total is calculated as the sum of the
individual components. The final data outturns refer to 2023 Q2 and the final forecast period is 2026 Q4.
Businesses report that interest rates are an increasingly important concernâ€¦
Bank of England  
Page 96



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_97.txt
================================================
Rising interest rates are likely to contribute to a reduction in business investment growth.
Evidence from the Decision Maker Panel (DMP) from January 2023 suggested that
interest rates were expected to reduce business investment by 8% on average over the
following year. This effect was driven by only a third of businesses, as many businesses
do not have debt and may not be affected by interest rate changes directly, or do not
make investments. Despite this, some firms without debt expected to cut their investment
as a result of higher interest rates. This suggests some of the estimated impact on
investment is potentially capturing second-round effects of reduced demand in the
economy limiting the need for additional investment.
Headline business investment growth in 2023 H1 has been strong: rising 4.1% over 2023
Q2 alone and 9.2% compared to 2022 Q2. However, this may not provide a clear steer on
the effect of interest rates due to erratic factors affecting the data. The ONS (2023)  report
that strong investment growth in 2023 Q1 was likely affected by the end of theChart 3.11: Concerns around interest rates are rising but remain below other
factors
Proportion of businesses reporting select issues as the main concern for the business (a)
Sources: ONS and Bank calculations.
(a) Results are based on responses to the question: â€˜Which of the following, if any , will be the main concern for your
business?â€™ for the following month. Some monthsâ€™ data have two results, where the survey has asked for that month in
more than one wave. Where that is the case, the results have been averaged. Results are weighted to match the count
of all businesses with 10 or more employees. The final data points refer to October 2023.
â€¦and that concern is reflected in expected business investment growth.
Bank of England  
Page 97



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_98.txt
================================================
Governmentâ€™s temporary additional tax relief on business investment. And the strong
growth in Q2 reflected transport investment which tends to be volatile. Indeed, the ONS
(2023)  report that acquisition of new aircraft was a large contributor to the quarterly figure.
In the latest DMP, businesses in 2023 Q2 expected nominal capital expenditure to grow
by an average rate of 2.7% over the coming year, down from a 6.1% expected increase in
2023 Q1. A wide range of factors influences businessesâ€™ investment decisions; see Box D
for evidence on these from the Bankâ€™s Agents. Real business investment is projected to
fall by just over 1% in 2024 and to be broadly flat in 2025, before increasing by 2% in
2026 (Section 1).
The exchange rate channel
Increases in UK interest rates relative to other countriesâ€™ rates would, all else equal, cause
the pound to appreciate. An exchange rate appreciation makes imports cheaper and
exports more expensive. Cheaper imports reduce UK inflation directly , because UK
households consume imported products, and indirectly, as imported goods and services
are used by firms in supply chains. There are also effects on UK activity through falling net
trade as UK exports become less competitive globally.
The exchange rate channel of monetary policy is particularly important for the UK
compared to other advanced economies because a high share of UK economic activity
involves trade with countries using different currencies.
Given most advanced economies have been raising rates at the same time as the UK, the
value of the pound has not appreciated since rate rises began. That suggests that, other
things equal, the impact on demand growth from monetary policy induced changes in the
exchange rate has been relatively limited to date. That said, had Bank Rate been kept at
the post-pandemic low, sterling might well have depreciated materially, leading to
additional inflation. Moreover, rate rises abroad have reduced demand in those countries
with a consequent reduction in demand for UK exports to those countries.Increasing interest rates across countries have limited the impact of the exchange
rate channel on UK demand growth.
Bank of England  
Page 98



================================================
File: vectordb/gpt-embeddings/destFilesDir/extracted_pages_99.txt
================================================
3.4: Conclusion
As set out above, income growth, saving behaviour, and the impact of higher Bank Rate
are key factors affecting demand in the economy at the moment. In the MPCâ€™s central
projection, GDP growth stays below historical averages over the forecast period. This
reflects the restrictive stance of monetary policy, which weighs to an increasing extent on
the level of demand, although the impact on quarterly GDP growth is currently around its
peak. A margin of slack opens up in the economy, which helps to bring inflation back to
the 2% target. The projection for demand, and the risks around it, are discussed in more
detail in Key judgement 1 in Section 1.In the MPCâ€™s central projection, demand growth remains weak by historical
standards. A margin of slack opens up which helps to bring inflation back to the
2% target.
Bank of England  
Page 99


